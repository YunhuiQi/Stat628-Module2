{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the official jupyter notebook for goal 1, our aim is to develop a shiny app which can give owners of gym business information on their position in gym business, suggestions on how to improve their ratings in yelp from both aspects of reviews and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We import python modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## import modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import FreqDist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import multiprocessing\n",
    "import time\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer #该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## define stopwords\n",
    "sr = stopwords.words('english')\n",
    "p = inflect.engine()\n",
    "wnl = WordNetLemmatizer() \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "porter = nltk.PorterStemmer()\n",
    "nlp = spacy.load(\"en\")\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the reviews data for gym business as pandas dataframe and let the name be data_review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read review data\n",
    "data_review=pd.read_csv(\"review_train.csv\")\n",
    "data_review[\"review_number\"] = range(len(data_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The main mathod we use is LDA topic model. Here are summary of our analysis.\n",
    "1. Divide each review into sentences, create a pd dataframe named sentence_train with columns are review number and sentence. The review number will help us merge information from data_sentence and data_review.\n",
    "2. For each sentence, do general data cleaning steps and get a list of words as column3 in data_sentence.\n",
    "3. For each sentence, filter nouns as a list  to become column 4 in data_sentence\n",
    "4. Create frequency matrix and tfidf matrix for nouns \n",
    "5. Apply LDA topic model on TFIDF matrix.\n",
    "6. Use some method to pick unique topic for each feature words and get our topic-term list.\n",
    "7. For each sentence and each topic, pick all the adjectives describing this topic's terms,  Search for the pre defined dictionary of positive and negative words, get this sentence's score at this topic equal to (number of positive words - number of negative words) / (number of positive words + number of negative words) \n",
    "8. Define a function of business_id, first get corresponding reviews' numbers, and in data_sentece, get relevant rows. Compute average of sentence_score at each topic as this business_score at each topic\n",
    "9. After getting all scores on each topic for all business, we can get distribution of scores on different topics. So we can output the position of given business.\n",
    "10. However, in order to give quantative and actionable suggestions on improving ratings, we want to do some statistical analysis with respect to reviews. For each business, we use weighted average stars of reviews as our response variable with weight equal to the number of feature words in each review. And T topic scores as our predictors to do linear regression （or grouped lasso????). As a result, we can get quantative interpretation on which topic is more important, and help owners to pick the most efficient way to achieve their goal.\n",
    "11. About the attributes, due to the large amount of missing values, we use GUIDE to build decision trees. This method can give us interpretable results and can deal with missing values. After GUIDE, we may use some statistical methods like ANOVA to find the reasons to interpret the relation between attributes  and stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create data_sentence dictionary and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess(dataset):\n",
    "    data_sentence = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        ## dealing with \\n and \\n\\n things\n",
    "        xx = dataset.iloc[i]['text']\n",
    "        xx = re.sub(r' \\n\\n','.',xx)\n",
    "        xx = re.sub(r'\\n','',xx)\n",
    "        xx = re.sub(r'\\.\\.','. ',xx)\n",
    "        xx = re.sub(r'(\\.)(\\S)',r'\\1 \\2',xx)\n",
    "        \n",
    "        ## for each review, tokenize it into sentences, saved in sent_set\n",
    "        sent_set = sent_tokenize(xx)\n",
    "    \n",
    "        ## for each sentence in sent_set, tokenized and cleaning into tokenized_sentence\n",
    "        for j in range(len(sent_set)):\n",
    "        \n",
    "            sent = sent_set[j]\n",
    "            if len(sent) > 3:\n",
    "                ## get \"review_number\" for this sentence\n",
    "                data_sentence.setdefault('review_number', []).append(dataset.iloc[i]['review_number'])\n",
    "        \n",
    "                ## assign this sentence into column \"sentence\" and sentence_list\n",
    "                data_sentence.setdefault('sentence', []).append(sent)\n",
    "\n",
    "        \n",
    "                ## clean this sentence into \"tokenized_sentence\"\n",
    "                x = re.sub(r'n\\'t',' not',sent)\n",
    "                ## change not adj into not_adj\n",
    "                x = re.sub(r'not ','not_',x)\n",
    "                ## split into words\n",
    "                x = word_tokenize(x)\n",
    "                ## remove punctuation\n",
    "                x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "                ## change numbers into words\n",
    "                x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "                ## remove not alphabetic\n",
    "                x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "                ## convert to lower case\n",
    "                x = [w.lower() for w in x]\n",
    "                ## remove stop words\n",
    "                x = [w for w in x if not w in sr]\n",
    "                ## lemmatization\n",
    "                x = [wnl.lemmatize(w) for w in x]\n",
    "            \n",
    "                ## assign cleaned sentence words to \"tokenized_sentence\" and tokenized_sentence_list\n",
    "                data_sentence.setdefault('tokenized_sentence', []).append(x)\n",
    "\n",
    "\n",
    "                \n",
    "                ## POS\n",
    "                ## change cleaned words into nlp format\n",
    "                sent_nlp = nlp(\" \".join(x))\n",
    "                ## get nouns for each sentence and saved into nouns_list\n",
    "                nolis = [token.lemma_ for token in sent_nlp if token.pos_ == \"NOUN\"]\n",
    "                ## assign nouns to \"nouns\"\n",
    "                data_sentence.setdefault('nouns', []).append(nolis)\n",
    "            \n",
    "    return data_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/35033 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 2/35033 [00:00<31:33, 18.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 3/35033 [00:00<50:30, 11.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 5/35033 [00:00<47:08, 12.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 6/35033 [00:00<55:19, 10.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 7/35033 [00:00<1:03:19,  9.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 9/35033 [00:00<54:00, 10.81it/s]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 11/35033 [00:00<48:56, 11.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 14/35033 [00:01<49:09, 11.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 16/35033 [00:01<46:58, 12.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 20/35033 [00:01<40:45, 14.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 22/35033 [00:01<42:48, 13.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 24/35033 [00:01<43:03, 13.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 26/35033 [00:01<40:01, 14.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 29/35033 [00:02<38:04, 15.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 31/35033 [00:02<52:51, 11.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 33/35033 [00:02<47:27, 12.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 36/35033 [00:02<42:21, 13.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 39/35033 [00:02<37:06, 15.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 41/35033 [00:02<36:47, 15.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 43/35033 [00:03<38:31, 15.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 47/35033 [00:03<32:02, 18.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 50/35033 [00:03<37:35, 15.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 53/35033 [00:03<36:57, 15.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-70acd8e61372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-911b1773f7ce>\u001b[0m in \u001b[0;36mdataprocess\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m## POS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m## change cleaned words into nlp format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0msent_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0;31m## get nouns for each sentence and saved into nouns_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mnolis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_nlp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NOUN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mXhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_rescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin_update_scale_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(ops, X, mu, var)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_sentence = dataprocess(data_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e95207aaba79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save data into data_sentence.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_sentence.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# save data into data_sentence.txt\n",
    "fw = open(\"data_sentence.txt\",'w+')\n",
    "fw.write(str(data_sentence))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data_sentence.txt\n",
    "fr = open(\"data_sentence.txt\",'r+')\n",
    "data_sentence_dic = eval(fr.read()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentence_df = pd.DataFrame(data_sentence_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create frequency and tfidf matrix for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcorpus(datadict):\n",
    "    \n",
    "    nouns_list = datadict[\"nouns\"]\n",
    "    \n",
    "    ## define dictionary for \"nouns\"\n",
    "    dictionary = corpora.Dictionary(nouns_list)\n",
    "\n",
    "    ## create frequency matrix\n",
    "    frequency_matrix = [dictionary.doc2bow(n) for n in nouns_list]\n",
    "                    \n",
    "    ## create tfidf matrix\n",
    "    tfidf = gensim.models.TfidfModel(frequency_matrix)\n",
    "    corpus_tfidf = tfidf[frequency_matrix]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf, dictionary = getcorpus(data_sentence_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply LDA topic model to tfidf matrix and visualization and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply LDA model on tfidf matrix                                                        \n",
    "lda_model = LDA(corpus_tfidf, id2word=dictionary, num_topics=7, random_state=100,chunksize=1000, passes=50)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el59891213061850647167236889\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el59891213061850647167236889_data = {\"mdsDat\": {\"x\": [-0.17561770112729314, 0.3658608475835439, -0.16884896515583936, -0.02139418130041141], \"y\": [-0.017836373788229103, -0.08839740977174206, -0.21349970685739406, 0.31973349041736515], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [39.47782897949219, 21.670103073120117, 20.29613494873047, 18.555936813354492]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [71.0, 71.0, 67.0, 48.0, 48.0, 78.0, 36.0, 31.0, 31.0, 31.0, 24.0, 25.0, 26.0, 24.0, 24.0, 46.0, 46.0, 46.0, 40.0, 31.0, 40.0, 28.0, 15.0, 15.0, 15.0, 28.0, 14.0, 10.0, 18.0, 22.0, 45.50297927856445, 45.50297927856445, 45.50297927856445, 39.520992279052734, 27.41403579711914, 16.78737449645996, 16.398805618286133, 16.3988037109375, 16.3988037109375, 14.162718772888184, 14.162718772888184, 14.16272258758545, 14.162718772888184, 14.16272258758545, 14.162718772888184, 8.314483642578125, 8.314483642578125, 8.314483642578125, 8.314483642578125, 8.314483642578125, 8.314481735229492, 8.314483642578125, 8.314483642578125, 8.314481735229492, 8.314481735229492, 8.314483642578125, 8.314483642578125, 8.314483642578125, 8.314483642578125, 8.314481735229492, 22.97891616821289, 8.314483642578125, 31.918609619140625, 11.932365417480469, 11.510354042053223, 47.72547149658203, 47.72547149658203, 35.45412063598633, 30.458187103271484, 30.458179473876953, 30.458179473876953, 25.54694366455078, 13.449143409729004, 8.037031173706055, 8.037031173706055, 7.590242385864258, 3.771735668182373, 2.1078860759735107, 2.1078860759735107, 2.1078860759735107, 2.1078860759735107, 2.1078860759735107, 2.1078860759735107, 11.654430389404297, 1.9744391441345215, 0.23020020127296448, 0.23020020127296448, 0.23020020127296448, 0.23023048043251038, 0.23032982647418976, 0.23017534613609314, 0.23017534613609314, 0.23017534613609314, 0.23054435849189758, 0.2302914261817932, 0.23045887053012848, 0.23033255338668823, 0.23033228516578674, 0.2303316295146942, 0.2302725613117218, 0.23024052381515503, 0.23023894429206848, 66.2939453125, 24.523719787597656, 24.215150833129883, 24.215150833129883, 15.21658992767334, 15.21658992767334, 15.21658992767334, 6.924533843994141, 6.924533843994141, 3.649264335632324, 3.649264335632324, 3.5826010704040527, 3.132678508758545, 1.6785337924957275, 1.3538423776626587, 1.3538423776626587, 1.3538423776626587, 1.1616712808609009, 46.133487701416016, 0.9578145146369934, 0.7971089482307434, 0.7971089482307434, 0.7971089482307434, 10.499430656433105, 12.807097434997559, 2.2171988487243652, 0.24163708090782166, 0.24115295708179474, 0.24115295708179474, 0.24115295708179474, 0.24115295708179474, 0.24260181188583374, 0.24205279350280762, 0.24145153164863586, 71.26560974121094, 71.26560974121094, 23.494674682617188, 10.294180870056152, 8.271595001220703, 7.841285228729248, 6.824107646942139, 2.7138009071350098, 2.7138009071350098, 16.65328025817871, 17.990772247314453, 16.602346420288086, 6.146613121032715, 2.305640459060669, 0.25131040811538696, 0.25131040811538696, 0.25131040811538696, 0.251352459192276, 0.25149133801460266, 0.25127533078193665, 0.25127533078193665, 0.25127533078193665, 0.28283777832984924, 0.2588309347629547, 0.2512153089046478, 0.2512153089046478, 0.2512153089046478, 0.2512153089046478, 0.25121527910232544, 0.25121527910232544, 0.2515721321105957, 0.25145331025123596, 0.2514320909976959, 0.2514219582080841, 0.25141075253486633, 0.2513958215713501, 0.251364529132843, 0.2513643503189087, 0.2513643205165863, 0.2513403594493866, 0.25131216645240784, 0.2513120472431183], \"Term\": [\"bill\", \"service\", \"cent\", \"hospital\", \"cost\", \"pill\", \"gym\", \"facility\", \"review\", \"product\", \"spit\", \"way\", \"time\", \"month\", \"sticker\", \"nerve\", \"charge\", \"crook\", \"weight\", \"people\", \"cage\", \"power\", \"splotch\", \"squat\", \"platform\", \"machine\", \"area\", \"nothing\", \"job\", \"lifting\", \"charge\", \"crook\", \"nerve\", \"weight\", \"machine\", \"equipment\", \"membership\", \"rate\", \"pad\", \"smith\", \"combo\", \"cardio\", \"plate\", \"bumper\", \"array\", \"lock\", \"friend\", \"employee\", \"hour\", \"nocontract\", \"crack\", \"number\", \"shower\", \"space\", \"dumbbell\", \"sweat\", \"con\", \"wear\", \"towel\", \"access\", \"cage\", \"option\", \"pill\", \"lifting\", \"power\", \"cost\", \"hospital\", \"gym\", \"review\", \"facility\", \"product\", \"time\", \"area\", \"price\", \"star\", \"rule\", \"day\", \"side\", \"idea\", \"interaction\", \"patron\", \"line\", \"town\", \"job\", \"locker\", \"stair\", \"climber\", \"story\", \"treadmill\", \"thing\", \"basketball\", \"court\", \"anything\", \"stick\", \"bike\", \"pill\", \"crook\", \"charge\", \"nerve\", \"swam\", \"lifting\", \"power\", \"cent\", \"way\", \"month\", \"sticker\", \"splotch\", \"platform\", \"squat\", \"parking\", \"lot\", \"desk\", \"help\", \"general\", \"swam\", \"bike\", \"anything\", \"court\", \"basketball\", \"thing\", \"pill\", \"treadmill\", \"story\", \"stair\", \"climber\", \"lifting\", \"people\", \"sign\", \"stick\", \"line\", \"interaction\", \"idea\", \"patron\", \"dowd\", \"weight\", \"locker\", \"bill\", \"service\", \"spit\", \"nothing\", \"dowd\", \"davidson\", \"room\", \"steam\", \"sauna\", \"power\", \"people\", \"cage\", \"job\", \"locker\", \"climber\", \"stair\", \"story\", \"treadmill\", \"thing\", \"anything\", \"basketball\", \"court\", \"stick\", \"bike\", \"interaction\", \"line\", \"town\", \"idea\", \"patron\", \"side\", \"pill\", \"weight\", \"rule\", \"time\", \"swam\", \"lifting\", \"nerve\", \"charge\", \"crook\", \"sign\", \"price\", \"star\"], \"Total\": [71.0, 71.0, 67.0, 48.0, 48.0, 78.0, 36.0, 31.0, 31.0, 31.0, 24.0, 25.0, 26.0, 24.0, 24.0, 46.0, 46.0, 46.0, 40.0, 31.0, 40.0, 28.0, 15.0, 15.0, 15.0, 28.0, 14.0, 10.0, 18.0, 22.0, 46.22600173950195, 46.22600173950195, 46.22600173950195, 40.244651794433594, 28.1365909576416, 17.509906768798828, 17.12124252319336, 17.121240615844727, 17.121240615844727, 14.885307312011719, 14.885307312011719, 14.885311126708984, 14.885307312011719, 14.885311126708984, 14.885307312011719, 9.036922454833984, 9.036922454833984, 9.036922454833984, 9.036922454833984, 9.036922454833984, 9.036920547485352, 9.036922454833984, 9.036922454833984, 9.036920547485352, 9.036920547485352, 9.036922454833984, 9.036922454833984, 9.036922454833984, 9.036922454833984, 9.036920547485352, 40.05278778076172, 9.0369234085083, 78.53412628173828, 22.913433074951172, 28.635160446166992, 48.448638916015625, 48.44864273071289, 36.1771354675293, 31.181182861328125, 31.181175231933594, 31.181175231933594, 26.27035140991211, 14.172224998474121, 8.760205268859863, 8.760205268859863, 8.313684463500977, 4.494879722595215, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 18.273006439208984, 12.41508960723877, 1.5092449188232422, 1.5092449188232422, 1.5092449188232422, 1.6700396537780762, 1.8759562969207764, 2.0659050941467285, 2.0659050941467285, 2.0659050941467285, 2.399667739868164, 2.3983354568481445, 78.53412628173828, 46.22600173950195, 46.22600173950195, 46.22600173950195, 3.845027208328247, 22.913433074951172, 28.635160446166992, 67.00604248046875, 25.2357120513916, 24.92715072631836, 24.92715072631836, 15.928510665893555, 15.928509712219238, 15.928509712219238, 7.636465072631836, 7.636465072631836, 4.361372470855713, 4.361372470855713, 4.29454231262207, 3.845027208328247, 2.3983354568481445, 2.0659050941467285, 2.0659050941467285, 2.0659050941467285, 1.8759562969207764, 78.53412628173828, 1.6700396537780762, 1.5092449188232422, 1.5092449188232422, 1.5092449188232422, 22.913433074951172, 31.258731842041016, 10.491604804992676, 2.399667739868164, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 8.974960327148438, 40.244651794433594, 12.41508960723877, 71.96763610839844, 71.96763610839844, 24.196542739868164, 10.996064186096191, 8.974960327148438, 8.543155670166016, 7.528312683105469, 3.4157140254974365, 3.4157140254974365, 28.635160446166992, 31.258731842041016, 40.05278778076172, 18.273006439208984, 12.41508960723877, 1.5092449188232422, 1.5092449188232422, 1.5092449188232422, 1.6700396537780762, 1.8759562969207764, 2.0659050941467285, 2.0659050941467285, 2.0659050941467285, 2.399667739868164, 2.3983354568481445, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 2.830843448638916, 78.53412628173828, 40.244651794433594, 8.313684463500977, 26.27035140991211, 3.845027208328247, 22.913433074951172, 46.22600173950195, 46.22600173950195, 46.22600173950195, 10.491604804992676, 8.760205268859863, 8.760205268859863], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9136999845504761, 0.9136999845504761, 0.9136999845504761, 0.911300003528595, 0.9034000039100647, 0.8873000144958496, 0.8863000273704529, 0.8863000273704529, 0.8863000273704529, 0.8797000050544739, 0.8797000050544739, 0.8797000050544739, 0.8797000050544739, 0.8797000050544739, 0.8797000050544739, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.8460999727249146, 0.37380000948905945, 0.8460999727249146, 0.029100000858306885, 0.2770000100135803, 0.017999999225139618, 1.51419997215271, 1.51419997215271, 1.5089999437332153, 1.5058000087738037, 1.5058000087738037, 1.5058000087738037, 1.5012999773025513, 1.4768999814987183, 1.4430999755859375, 1.4430999755859375, 1.4381999969482422, 1.3538000583648682, 1.2343000173568726, 1.2343000173568726, 1.2343000173568726, 1.2343000173568726, 1.2343000173568726, 1.2343000173568726, 1.0794999599456787, -0.3093999922275543, -0.35120001435279846, -0.35120001435279846, -0.35120001435279846, -0.4523000121116638, -0.5680999755859375, -0.6651999950408936, -0.6651999950408936, -0.6651999950408936, -0.8133999705314636, -0.8138999938964844, -4.302000045776367, -3.7725000381469727, -3.7725000381469727, -3.7725000381469727, -1.2860000133514404, -3.0710999965667725, -3.2939999103546143, 1.5841000080108643, 1.566100001335144, 1.5657999515533447, 1.5657999515533447, 1.5490000247955322, 1.5490000247955322, 1.5490000247955322, 1.496899962425232, 1.496899962425232, 1.4164999723434448, 1.4164999723434448, 1.4134999513626099, 1.389799952507019, 1.2379000186920166, 1.1720999479293823, 1.1720999479293823, 1.1720999479293823, 1.1154999732971191, 1.0627000331878662, 1.0388000011444092, 0.9563999772071838, 0.9563999772071838, 0.9563999772071838, 0.814300000667572, 0.7024000287055969, 0.04039999842643738, -0.7009000182151794, -0.8682000041007996, -0.8682000041007996, -0.8682000041007996, -0.8682000041007996, -2.0160000324249268, -3.5188000202178955, -2.3452999591827393, 1.6746000051498413, 1.6746000051498413, 1.6548999547958374, 1.618399977684021, 1.6028000116348267, 1.5987000465393066, 1.5861999988555908, 1.454300045967102, 1.454300045967102, 1.1424000263214111, 1.1318999528884888, 0.8036999702453613, 0.5949000120162964, 0.0007999999797903001, -0.10830000042915344, -0.10830000042915344, -0.10830000042915344, -0.2093999981880188, -0.32510000467300415, -0.42239999771118164, -0.42239999771118164, -0.42239999771118164, -0.4537999927997589, -0.5419999957084656, -0.7376000285148621, -0.7376000285148621, -0.7376000285148621, -0.7376000285148621, -0.7376000285148621, -0.7376000285148621, -4.059199810028076, -3.3910999298095703, -1.8141000270843506, -2.9646999835968018, -1.0430999994277954, -2.8280999660491943, -3.5299999713897705, -3.5299999713897705, -3.5299999713897705, -2.047100067138672, -1.8668999671936035, -1.8668999671936035], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.584399938583374, -2.584399938583374, -2.584399938583374, -2.725399971008301, -3.091200113296509, -3.5815999507904053, -3.6050000190734863, -3.6050000190734863, -3.6050000190734863, -3.7516000270843506, -3.7516000270843506, -3.7516000270843506, -3.7516000270843506, -3.7516000270843506, -3.7516000270843506, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -4.284200191497803, -3.2676000595092773, -4.284200191497803, -2.938999891281128, -3.9230000972747803, -3.9590001106262207, -1.937000036239624, -1.937000036239624, -2.2342000007629395, -2.3861000537872314, -2.3861000537872314, -2.3861000537872314, -2.5618999004364014, -3.2035000324249268, -3.718400001525879, -3.718400001525879, -3.775599956512451, -4.474899768829346, -5.056700229644775, -5.056700229644775, -5.056700229644775, -5.056700229644775, -5.056700229644775, -5.056700229644775, -3.3466999530792236, -5.122099876403809, -7.271200180053711, -7.271200180053711, -7.271200180053711, -7.271100044250488, -7.270699977874756, -7.271299839019775, -7.271299839019775, -7.271299839019775, -7.269700050354004, -7.2708001136779785, -7.270100116729736, -7.270699977874756, -7.270699977874756, -7.270699977874756, -7.270899772644043, -7.271100044250488, -7.271100044250488, -1.542799949645996, -2.5373001098632812, -2.5499000549316406, -2.5499000549316406, -3.0144999027252197, -3.0144999027252197, -3.0144999027252197, -3.801800012588501, -3.801800012588501, -4.442399978637695, -4.442399978637695, -4.4608001708984375, -4.59499979019165, -5.218999862670898, -5.434000015258789, -5.434000015258789, -5.434000015258789, -5.587100028991699, -1.905400037765503, -5.78000020980835, -5.963699817657471, -5.963699817657471, -5.963699817657471, -3.3856000900268555, -3.1868999004364014, -4.940700054168701, -7.157199859619141, -7.159200191497803, -7.159200191497803, -7.159200191497803, -7.159200191497803, -7.153299808502197, -7.1554999351501465, -7.1579999923706055, -1.3809000253677368, -1.3809000253677368, -2.490499973297119, -3.315700054168701, -3.53439998626709, -3.587899923324585, -3.726799964904785, -4.648900032043457, -4.648900032043457, -2.834700107574463, -2.7574000358581543, -2.8376998901367188, -3.831399917602539, -4.8119001388549805, -7.028299808502197, -7.028299808502197, -7.028299808502197, -7.028200149536133, -7.027599811553955, -7.028500080108643, -7.028500080108643, -7.028500080108643, -6.910200119018555, -6.998899936676025, -7.02869987487793, -7.02869987487793, -7.02869987487793, -7.02869987487793, -7.02869987487793, -7.02869987487793, -7.027299880981445, -7.0278000831604, -7.027900218963623, -7.027900218963623, -7.027900218963623, -7.0279998779296875, -7.02810001373291, -7.02810001373291, -7.02810001373291, -7.028200149536133, -7.028299808502197, -7.028299808502197]}, \"token.table\": {\"Topic\": [1, 3, 2, 1, 3, 3, 4, 1, 1, 4, 1, 3, 1, 3, 1, 1, 2, 3, 1, 1, 4, 2, 3, 4, 1, 1, 1, 2, 1, 3, 2, 3, 2, 1, 2, 2, 2, 4, 1, 3, 2, 1, 1, 2, 4, 3, 1, 1, 3, 1, 1, 4, 1, 1, 1, 3, 2, 3, 4, 1, 3, 1, 3, 1, 4, 2, 2, 1, 2, 4, 2, 4, 4, 1, 2, 1, 3, 1, 1, 4, 3, 3, 3, 2, 4, 1, 3, 3, 3, 1, 3, 2, 1, 2, 3, 3, 1, 1], \"Freq\": [0.8852573037147522, 0.48404935002326965, 0.9172871708869934, 0.9405247569084167, 0.48404935002326965, 0.8339117169380188, 0.9865545630455017, 0.9405245184898376, 0.5742421746253967, 0.4244398772716522, 0.9405245184898376, 0.9849857687950134, 0.9951109290122986, 0.6625829935073853, 0.9405247569084167, 0.8852571249008179, 0.9907398819923401, 0.48404935002326965, 0.8852573037147522, 0.9951109290122986, 0.9364221096038818, 0.8899014592170715, 0.9171425104141235, 0.8913688659667969, 0.8852573037147522, 0.8852571249008179, 0.9708789587020874, 0.9621189832687378, 0.8852571249008179, 0.9314147233963013, 0.967461884021759, 0.9171425104141235, 0.9907398223876953, 0.8852571249008179, 0.7065032124519348, 0.7065032124519348, 0.6567063927650452, 0.3283531963825226, 0.5237102508544922, 0.43642520904541016, 0.7065032124519348, 0.8852571249008179, 0.6443771719932556, 0.1610942929983139, 0.1610942929983139, 0.9166544675827026, 0.9596045017242432, 0.9345116019248962, 0.9628055691719055, 0.9951109290122986, 0.8852571249008179, 0.9094163179397583, 0.8852571249008179, 0.8852570056915283, 0.9345117211341858, 0.9166544675827026, 0.7065032124519348, 0.41588377952575684, 0.575839102268219, 0.40746617317199707, 0.585732638835907, 0.9405247569084167, 0.9417076706886292, 0.4190652370452881, 0.5936757326126099, 0.9132205843925476, 0.9621189832687378, 0.9345117211341858, 0.9621187448501587, 0.9298232197761536, 0.9622688889503479, 0.8782936930656433, 0.9865545630455017, 0.8852571249008179, 0.7065032124519348, 0.7625144124031067, 0.19062860310077667, 0.9405247569084167, 0.8852573037147522, 0.9505490064620972, 0.9417076110839844, 0.9417076706886292, 0.6625829935073853, 0.9132205843925476, 0.8782936930656433, 0.8334487080574036, 0.9628055691719055, 0.6625829935073853, 0.7802285552024841, 0.8852571249008179, 0.5330614447593689, 0.9897089004516602, 0.8852571249008179, 0.7065032124519348, 0.5987882018089294, 0.9906595945358276, 0.8852571249008179, 0.9939208626747131], \"Term\": [\"access\", \"anything\", \"area\", \"array\", \"basketball\", \"bike\", \"bill\", \"bumper\", \"cage\", \"cage\", \"cardio\", \"cent\", \"charge\", \"climber\", \"combo\", \"con\", \"cost\", \"court\", \"crack\", \"crook\", \"davidson\", \"day\", \"desk\", \"dowd\", \"dumbbell\", \"employee\", \"equipment\", \"facility\", \"friend\", \"general\", \"gym\", \"help\", \"hospital\", \"hour\", \"idea\", \"interaction\", \"job\", \"job\", \"lifting\", \"lifting\", \"line\", \"lock\", \"locker\", \"locker\", \"locker\", \"lot\", \"machine\", \"membership\", \"month\", \"nerve\", \"nocontract\", \"nothing\", \"number\", \"option\", \"pad\", \"parking\", \"patron\", \"people\", \"people\", \"pill\", \"pill\", \"plate\", \"platform\", \"power\", \"power\", \"price\", \"product\", \"rate\", \"review\", \"room\", \"rule\", \"sauna\", \"service\", \"shower\", \"side\", \"sign\", \"sign\", \"smith\", \"space\", \"spit\", \"splotch\", \"squat\", \"stair\", \"star\", \"steam\", \"stick\", \"sticker\", \"story\", \"swam\", \"sweat\", \"thing\", \"time\", \"towel\", \"town\", \"treadmill\", \"way\", \"wear\", \"weight\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el59891213061850647167236889\", ldavis_el59891213061850647167236889_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el59891213061850647167236889\", ldavis_el59891213061850647167236889_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el59891213061850647167236889\", ldavis_el59891213061850647167236889_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.175618 -0.017836       1        1  39.477829\n",
       "3      0.365861 -0.088397       2        1  21.670103\n",
       "1     -0.168849 -0.213500       3        1  20.296135\n",
       "2     -0.021394  0.319733       4        1  18.555937, topic_info=     Category       Freq         Term      Total  loglift  logprob\n",
       "term                                                              \n",
       "0     Default  71.000000         bill  71.000000  30.0000  30.0000\n",
       "1     Default  71.000000      service  71.000000  29.0000  29.0000\n",
       "6     Default  67.000000         cent  67.000000  28.0000  28.0000\n",
       "8     Default  48.000000     hospital  48.000000  27.0000  27.0000\n",
       "7     Default  48.000000         cost  48.000000  26.0000  26.0000\n",
       "5     Default  78.000000         pill  78.000000  25.0000  25.0000\n",
       "10    Default  36.000000          gym  36.000000  24.0000  24.0000\n",
       "9     Default  31.000000     facility  31.000000  23.0000  23.0000\n",
       "12    Default  31.000000       review  31.000000  22.0000  22.0000\n",
       "11    Default  31.000000      product  31.000000  21.0000  21.0000\n",
       "51    Default  24.000000         spit  24.000000  20.0000  20.0000\n",
       "49    Default  25.000000          way  25.000000  19.0000  19.0000\n",
       "55    Default  26.000000         time  26.000000  18.0000  18.0000\n",
       "47    Default  24.000000        month  24.000000  17.0000  17.0000\n",
       "48    Default  24.000000      sticker  24.000000  16.0000  16.0000\n",
       "4     Default  46.000000        nerve  46.000000  15.0000  15.0000\n",
       "2     Default  46.000000       charge  46.000000  14.0000  14.0000\n",
       "3     Default  46.000000        crook  46.000000  13.0000  13.0000\n",
       "24    Default  40.000000       weight  40.000000  12.0000  12.0000\n",
       "50    Default  31.000000       people  31.000000  11.0000  11.0000\n",
       "15    Default  40.000000         cage  40.000000  10.0000  10.0000\n",
       "22    Default  28.000000        power  28.000000   9.0000   9.0000\n",
       "53    Default  15.000000      splotch  15.000000   8.0000   8.0000\n",
       "54    Default  15.000000        squat  15.000000   7.0000   7.0000\n",
       "52    Default  15.000000     platform  15.000000   6.0000   6.0000\n",
       "20    Default  28.000000      machine  28.000000   5.0000   5.0000\n",
       "56    Default  14.000000         area  14.000000   4.0000   4.0000\n",
       "60    Default  10.000000      nothing  10.000000   3.0000   3.0000\n",
       "57    Default  18.000000          job  18.000000   2.0000   2.0000\n",
       "19    Default  22.000000      lifting  22.000000   1.0000   1.0000\n",
       "...       ...        ...          ...        ...      ...      ...\n",
       "57     Topic4   6.146613          job  18.273006   0.5949  -3.8314\n",
       "34     Topic4   2.305640       locker  12.415090   0.0008  -4.8119\n",
       "86     Topic4   0.251310      climber   1.509245  -0.1083  -7.0283\n",
       "87     Topic4   0.251310        stair   1.509245  -0.1083  -7.0283\n",
       "88     Topic4   0.251310        story   1.509245  -0.1083  -7.0283\n",
       "85     Topic4   0.251352    treadmill   1.670040  -0.2094  -7.0282\n",
       "89     Topic4   0.251491        thing   1.875956  -0.3251  -7.0276\n",
       "80     Topic4   0.251275     anything   2.065905  -0.4224  -7.0285\n",
       "81     Topic4   0.251275   basketball   2.065905  -0.4224  -7.0285\n",
       "82     Topic4   0.251275        court   2.065905  -0.4224  -7.0285\n",
       "83     Topic4   0.282838        stick   2.399668  -0.4538  -6.9102\n",
       "84     Topic4   0.258831         bike   2.398335  -0.5420  -6.9989\n",
       "70     Topic4   0.251215  interaction   2.830843  -0.7376  -7.0287\n",
       "71     Topic4   0.251215         line   2.830843  -0.7376  -7.0287\n",
       "74     Topic4   0.251215         town   2.830843  -0.7376  -7.0287\n",
       "69     Topic4   0.251215         idea   2.830843  -0.7376  -7.0287\n",
       "72     Topic4   0.251215       patron   2.830843  -0.7376  -7.0287\n",
       "73     Topic4   0.251215         side   2.830843  -0.7376  -7.0287\n",
       "5      Topic4   0.251572         pill  78.534126  -4.0592  -7.0273\n",
       "24     Topic4   0.251453       weight  40.244652  -3.3911  -7.0278\n",
       "66     Topic4   0.251432         rule   8.313684  -1.8141  -7.0279\n",
       "55     Topic4   0.251422         time  26.270351  -2.9647  -7.0279\n",
       "79     Topic4   0.251411         swam   3.845027  -1.0431  -7.0279\n",
       "19     Topic4   0.251396      lifting  22.913433  -2.8281  -7.0280\n",
       "4      Topic4   0.251365        nerve  46.226002  -3.5300  -7.0281\n",
       "2      Topic4   0.251364       charge  46.226002  -3.5300  -7.0281\n",
       "3      Topic4   0.251364        crook  46.226002  -3.5300  -7.0281\n",
       "42     Topic4   0.251340         sign  10.491605  -2.0471  -7.0282\n",
       "61     Topic4   0.251312        price   8.760205  -1.8669  -7.0283\n",
       "62     Topic4   0.251312         star   8.760205  -1.8669  -7.0283\n",
       "\n",
       "[178 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "25        1  0.885257      access\n",
       "80        3  0.484049    anything\n",
       "56        2  0.917287        area\n",
       "13        1  0.940525       array\n",
       "81        3  0.484049  basketball\n",
       "84        3  0.833912        bike\n",
       "0         4  0.986555        bill\n",
       "14        1  0.940525      bumper\n",
       "15        1  0.574242        cage\n",
       "15        4  0.424440        cage\n",
       "16        1  0.940525      cardio\n",
       "6         3  0.984986        cent\n",
       "2         1  0.995111      charge\n",
       "86        3  0.662583     climber\n",
       "17        1  0.940525       combo\n",
       "27        1  0.885257         con\n",
       "7         2  0.990740        cost\n",
       "82        3  0.484049       court\n",
       "28        1  0.885257       crack\n",
       "3         1  0.995111       crook\n",
       "58        4  0.936422    davidson\n",
       "76        2  0.889901         day\n",
       "67        3  0.917143        desk\n",
       "59        4  0.891369        dowd\n",
       "29        1  0.885257    dumbbell\n",
       "30        1  0.885257    employee\n",
       "18        1  0.970879   equipment\n",
       "9         2  0.962119    facility\n",
       "31        1  0.885257      friend\n",
       "63        3  0.931415     general\n",
       "...     ...       ...         ...\n",
       "12        2  0.962119      review\n",
       "75        4  0.929823        room\n",
       "66        2  0.962269        rule\n",
       "77        4  0.878294       sauna\n",
       "1         4  0.986555     service\n",
       "41        1  0.885257      shower\n",
       "73        2  0.706503        side\n",
       "42        1  0.762514        sign\n",
       "42        3  0.190629        sign\n",
       "23        1  0.940525       smith\n",
       "43        1  0.885257       space\n",
       "51        4  0.950549        spit\n",
       "53        3  0.941708     splotch\n",
       "54        3  0.941708       squat\n",
       "87        3  0.662583       stair\n",
       "62        2  0.913221        star\n",
       "78        4  0.878294       steam\n",
       "83        1  0.833449       stick\n",
       "48        3  0.962806     sticker\n",
       "88        3  0.662583       story\n",
       "79        3  0.780229        swam\n",
       "44        1  0.885257       sweat\n",
       "89        3  0.533061       thing\n",
       "55        2  0.989709        time\n",
       "45        1  0.885257       towel\n",
       "74        2  0.706503        town\n",
       "85        3  0.598788   treadmill\n",
       "49        3  0.990660         way\n",
       "46        1  0.885257        wear\n",
       "24        1  0.993921      weight\n",
       "\n",
       "[98 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the topics\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model evaluation\n",
    "\n",
    "## compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus_tfidf))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "## compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_sentence[\"nouns\"], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create topic term lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_0_terms = []\n",
    "topic_1_terms = []\n",
    "topic_2_terms = []\n",
    "topic_3_terms = []\n",
    "topic_4_terms = []\n",
    "topic_5_terms = []\n",
    "topic_6_terms = []\n",
    "\n",
    "topic_terms = [topic_0_term,topic_1_term,topic_2_term,topic_3_term,topic_4_term,topic_5_term,topic_6_term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get main topic in each sentence\n",
    "for i, row in enumerate(lda_model[corpus_tfidf]):\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    ## save the dominant_topic, percage_contribution for each sentence to data_sentence_dic\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        if j == 0:  # => dominant topic\n",
    "            data_sentence_dic[\"dominant_topic\"] = topic_num\n",
    "            data_sentence_dic[\"percentage_contribution\"] = round(prop_topic,4)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Get scores of each topic for all data_sentence_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_score(topic_num):\n",
    "    ## get this topic_term list\n",
    "    topic_term_num = topic_terms[topic_num]\n",
    "    \n",
    "    ## define positive dictionary and negative dictionary\n",
    "    positive_dic = []   ## 放上positive和negative的库\n",
    "    negative_dic = []\n",
    "    \n",
    "    \n",
    "    ## define count variable\n",
    "    positive_order = 0\n",
    "    negative_order = 0\n",
    "    \n",
    "    ## define sub_adj_list for each sentence\n",
    "    sub_adj_list = []\n",
    "\n",
    "\n",
    "    for num in len(data_sentence_dic[\"review_number\"]):\n",
    "        sent = data_sentence_dic[\"sentence\"][num]\n",
    "        nouns_list = data_sentence_dic[\"nouns\"][num]\n",
    "        if any(element in sent for element in topic_term_num):\n",
    "            for term in topic_term_num:\n",
    "                if term in nouns_list:\n",
    "                    for child in list(term.children):\n",
    "                        if child.pos_ == \"ADJ\":\n",
    "                            adj = child.string.strip()\n",
    "                            sub_adj_list.append(adj)\n",
    "                                if adj in positive_dic:\n",
    "                                    positive_order+=1\n",
    "                                elif adj in negative_dic:\n",
    "                                    negative_order+=1\n",
    "          \n",
    "            data_sentence_dic.setdefault[\"adj_list\"+str(topic_num),[]].append(sub_adj_list)\n",
    "            data_sentence_dic.setdefault[\"num_positive\"+str(topic_num),[]].append(positive_order)\n",
    "            data_sentence_dic.setdefault[\"num_negative\"+str(topic_num),[]].append(negative_order)\n",
    "            data_sentence_dic.setdefault[\"score\"+str(topic_num),[]].append((positive_order-negative_order)/(positive_order+negative_order))\n",
    "        else:\n",
    "            data_sentence_dic.setdefault[\"adj_list\"+str(topic_num),[]].append([])\n",
    "            data_sentence_dic.setdefault[\"num_positive\"+str(topic_num),[]].append(0)\n",
    "            data_sentence_dic.setdefault[\"num_negative\"+str(topic_num),[]].append(0)\n",
    "            data_sentence_dic.setdefault[\"score\"+str(topic_num),[]].append(0)\n",
    "            \n",
    "    return data_sentence_dic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all topics score for each sentence\n",
    "data_sentence_dic=get_sentence_score(0)\n",
    "data_sentence_dic=get_sentence_score(1)\n",
    "data_sentence_dic=get_sentence_score(2)\n",
    "data_sentence_dic=get_sentence_score(3)\n",
    "data_sentence_dic=get_sentence_score(4)\n",
    "data_sentence_dic=get_sentence_score(5)\n",
    "data_sentence_dic=get_sentence_score(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Get all topic scores for given business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_score(business_id):\n",
    "    business_score = []\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    for t in range(6):\n",
    "        ##topic number - 1\n",
    "        sentence_score= [data_sentence_dic[\"score\"+str(t)][k] for k in range(len(data_sentence_dic[\"review_number\"])) if data_sentence_dic[\"review_number\"][k] in review_number_id]\n",
    "        denominator = 0\n",
    "        numerator = 0\n",
    "        for i in sentence_score:\n",
    "            if i !=0:\n",
    "                denominator+=1\n",
    "                numerator+=i\n",
    "        business_score.append(numerator/denominator) \n",
    "    return business_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get seven topic scorebusiness_id \n",
    "business_score(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Decide best number of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-69ae4f0a11a4>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-69ae4f0a11a4>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    data_sentence_train =\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def test_num_topics():\n",
    "    samlist = range(len(data_sentence_dic[\"review_number\"]))\n",
    "    test = random.sample(samlist, 62508)  #从list中随机获取5个元素，作为一个片断返回  \n",
    "    train = [x for x in samlist if x not in test]\n",
    "    data_sentence_train = \n",
    "    data_sentence_test = \n",
    "    corpus_tfidf_train, dictionary_train = getcorpus(data_sentence_train)\n",
    "    corpus_tfidf_test, dictionary_test = getcorpus(data_sentence_test)\n",
    "    for num_topics in [3, 5, 10, 30, 50, 100, 150, 200, 300]:\n",
    "        start_time = datetime.datetime.now()\n",
    "        lda_model = LDA(corpus_tfidf_train, num_topics=num_topics, id2word=dictionary_train,random_state=100,chunksize=1000, passes=50)\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(\"total running time = \", end_time - start_time)\n",
    "        # Compute Perplexity\n",
    "        print('\\nPerplexity with num_topics=%d : ' % num_topics, lda_model.log_perplexity(corpus_tfidf_test))  # a measure of how good the model is. lower the better.\n",
    "        # Compute Coherence Score\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=data_sentence_test[\"nouns\"], dictionary=dictionary_test, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print('\\nCoherence Score with num_topics=%d : ' % num_topics, coherence_lda)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
