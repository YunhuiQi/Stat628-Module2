{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import modules\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "import time\n",
    "wnl = WordNetLemmatizer() \n",
    "sr = stopwords.words('english')\n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "## define try size \n",
    "trysize=5000\n",
    "file_name = 'data/review_train.json'\n",
    "data_train = []\n",
    "size=0\n",
    "with open(file_name, 'r') as f:\n",
    "    try:\n",
    "        while size<=(trysize-1):\n",
    "            line = f.readline()\n",
    "            size=size+1\n",
    "            if line:\n",
    "                r = json.loads(line)\n",
    "                data_train.append(r)\n",
    "            else:\n",
    "                break\n",
    "    except:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:06<00:00, 765.08it/s]\n"
     ]
    }
   ],
   "source": [
    "## data processiong\n",
    "review_train_clean = [[1]]*trysize\n",
    "##a list  with dict element,you can get a summary of frequency of one review by [i] and frequency for a specific word by [i]['word']\n",
    "frequency_train_clean = [[1]]*trysize  \n",
    "for i in tqdm(range(trysize)):\n",
    "    #Split into words\n",
    "    x = word_tokenize(review_train[i]['text'])\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) for w in x]\n",
    "    #Remove not alphabetic\n",
    "    x = [word for word in x if word.isalpha()]\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in sr]\n",
    "    ## lemmatization\n",
    "    x = [wnl.lemmatize(w) for w in x]\n",
    "    review_train_clean[i] = x\n",
    "    frequency_train_clean[i] = collections.Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 825.3150559999999 Seconds\n"
     ]
    }
   ],
   "source": [
    "## train words embedding model\n",
    "## define dimension of need\n",
    "size = 300\n",
    "start = time.process_time()\n",
    "w2v = Word2Vec(review_train_clean, size=size, window=10, min_count=1,\n",
    "            workers=multiprocessing.cpu_count(), sg=1, iter=10, negative=20)\n",
    "end = time.process_time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "## save model as w2vmodel\n",
    "w2v.save('w2vmodel')\n",
    "## load model\n",
    "model = Word2Vec.load('w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2687.83it/s]\n"
     ]
    }
   ],
   "source": [
    "## for each review, get 1*size matrix use average of words matrix \n",
    "trysize=5000\n",
    "predictors = np.empty((trysize,size), float)\n",
    "for i in tqdm(range(trysize)):\n",
    "    predictors[i,] = pd.DataFrame(model.wv[review_train_clean[i]]).mean()\n",
    "    \n",
    "response = pd.DataFrame(data_train)['stars']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each review, we compute TF_IDF value for all word, and pick the first five,\n",
    "# then combine all first fives and pick first 100 as our predictors for logistic regression.\n",
    "\n",
    "##compute how many reviews in which this key appears\n",
    "def f_appear_num(word,dat):\n",
    "    num=0\n",
    "    for i in range(0,len(dat)):\n",
    "        d = dat[i]['text']\n",
    "        if word in d == True:\n",
    "            num = num+1\n",
    "    return num\n",
    "def tfidf_4_web(revw,dat):\n",
    "    ## input is a list of words for one review\n",
    "    text = {}\n",
    "    for key in revw:\n",
    "        text[key] = text.get(key, 0) + 1\n",
    "    tf = dict()\n",
    "    idf = dict()\n",
    "    tfidfvalue = dict()\n",
    "    for key in text:\n",
    "        num = f_appear_num(key,dat)\n",
    "        tf[key] = text[key]/len(text)\n",
    "        idf[key] = math.log(len(dat)/(1+num)) \n",
    "        tfidfvalue[key]= tf[key]*idf[key]\n",
    "    return tfidfvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:18<00:00,  9.35it/s]\n"
     ]
    }
   ],
   "source": [
    "size = 300\n",
    "trysize=5000\n",
    "predictors_weighted = np.empty((trysize,size), float)\n",
    "for i in tqdm(range(trysize)):\n",
    "    tfidf_dict = tfidf_4_web(review_train_clean[i],data_train)\n",
    "    tfidf_mul = []\n",
    "    for word in review_train_clean[i]:\n",
    "        tfidf_mul.append(tfidf_dict[word])\n",
    "    predictors_weighted[i,] = (pd.DataFrame(model.wv[review_train_clean[i]]).mul(tfidf_mul,axis=0)).mean()\n",
    "    \n",
    "response = pd.DataFrame(data_train)['stars']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  1.331465358167459\n"
     ]
    }
   ],
   "source": [
    "## logistic regression\n",
    "model_logit = LogisticRegression(solver = 'lbfgs', multi_class='multinomial',max_iter=500)\n",
    "model_logit.fit(predictors_weighted, response)\n",
    "## rmse for regression\n",
    "y_fitted = model_logit.predict(predictors_weighted)\n",
    "print(\"RMSE = \", sqrt(sum(np.square(response-y_fitted)) / len(y_fitted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
