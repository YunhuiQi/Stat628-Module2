{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook records our codes for prediction model in goal 2. The procedure might be very time consuming and memory consuming. Please run our codes cautiously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import math\n",
    "import spacy\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.matutils import sparse2full\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read  training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./Data_Module2/review_train.json') as f:\n",
    "    r_train = f.readlines()\n",
    "    r_train = list(map(json.loads,r_train))\n",
    "    \n",
    "r_train = pd.DataFrame(r_train)\n",
    "review = r_train.loc[:,'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5364626/5364626 [3:17:22<00:00, 452.98it/s]    \n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "wnl = WordNetLemmatizer()\n",
    "trysize = len(review)\n",
    "t = [[1]]*trysize\n",
    "for i in tqdm(range(trysize)):\n",
    "    #Split into words\n",
    "    x = word_tokenize(review[i])\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    ## lemmatization\n",
    "    x = [wnl.lemmatize(w) for w in x]\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) for w in x]\n",
    "    #Remove not alphabetic\n",
    "    x = [word for word in x if word.isalpha()]\n",
    "    #Change n't into not\n",
    "    x = ['not' if w=='n\\'t' else w for w in x ]\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in stop_words]\n",
    "    t[i] = x\n",
    "\n",
    "del review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_train_clean = [\" \".join(item) for item in t]\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('review_train.txt', 'w') as f:\n",
    "    for item in review_train_clean:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set data into appropriate forms, so that each observation has the same length.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "max_length = 300\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(review_train_clean)\n",
    "X=tokenizer.texts_to_sequences(review_train_clean)\n",
    "X=pad_sequences(X,maxlen=max_length)\n",
    "Y = to_categorical(r_train.loc[:,'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = to_categorical(r_train.loc[:,'stars']-1)\n",
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31292</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35344</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152538</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71871</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64913</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id                 date  stars  \\\n",
       "0        31292  2013-05-07 04:34:36    1.0   \n",
       "1        35344  2017-01-14 21:30:33    5.0   \n",
       "2       152538  2016-11-09 20:09:03    5.0   \n",
       "3        71871  2018-01-09 20:56:38    5.0   \n",
       "4        64913  2018-01-30 23:07:38    1.0   \n",
       "\n",
       "                                                text  \n",
       "0  Total bill for this horrible service? Over $8G...  \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...  \n",
       "2  I have to say that this office really has it t...  \n",
       "3  Went in for a lunch. Steak sandwich was delici...  \n",
       "4  Today was my second out of three sessions I ha...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 300, 128)          3840000   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 300, 64)           24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 150)               129000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 755       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 3,994,395\n",
      "Trainable params: 3,994,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, 128, input_length=max_length))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(5))\n",
    "model2.add(Activation(\"softmax\"))\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4291700 samples, validate on 1072926 samples\n",
      "Epoch 1/1\n",
      "4291700/4291700 [==============================] - 34979s 8ms/step - loss: 0.7049 - acc: 0.7000 - val_loss: 0.6729 - val_acc: 0.7132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8fc8d1d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y,validation_split=0.20, epochs=1,verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read test data, process and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1321274/1321274 [45:46<00:00, 481.04it/s]  \n"
     ]
    }
   ],
   "source": [
    "with open('./Data_Module2/review_test.json') as f:\n",
    "    r_test = f.readlines()\n",
    "    r_test = list(map(json.loads,r_test))\n",
    "    \n",
    "r_test = pd.DataFrame(r_test)\n",
    "review = r_test.loc[:,'text']\n",
    "ID = r_test.loc[:,'KaggleID']\n",
    "del r_test\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "wnl = WordNetLemmatizer()\n",
    "trysize = len(review)\n",
    "t = [[1]]*trysize\n",
    "for i in tqdm(range(trysize)):\n",
    "    #Split into words\n",
    "    x = word_tokenize(review[i])\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    ## lemmatization\n",
    "    x = [wnl.lemmatize(w) for w in x]\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) for w in x]\n",
    "    #Remove not alphabetic\n",
    "    x = [word for word in x if word.isalpha()]\n",
    "    #Change n't into not\n",
    "    x = ['not' if w=='n\\'t' else w for w in x ]\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in stop_words]\n",
    "    t[i] = x\n",
    "\n",
    "del review\n",
    "review_test_clean = [\" \".join(item) for item in t]\n",
    "del t\n",
    "max_features = 30000\n",
    "max_length = 300\n",
    "X_test = tokenizer.texts_to_sequences(review_test_clean)\n",
    "X_test = pad_sequences(X_test,maxlen=max_length)\n",
    "results = model.predict_classes(X_test)\n",
    "df = pd.DataFrame({'Expected':results,'ID':ID})\n",
    "df = df[['ID','Expected']]\n",
    "df.to_csv('try2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Expected':results+1,'ID':ID})\n",
    "df = df[['ID','Expected']]\n",
    "df.to_csv('try2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adjust model parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5364626/5364626 [==============================] - 42385s 8ms/step - loss: 0.6969 - acc: 0.7032\n"
     ]
    }
   ],
   "source": [
    "max_features = 30000\n",
    "max_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64, input_length=max_length))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.fit(X, Y, epochs=1,verbose=1, batch_size=32)\n",
    "results = model.predict_classes(X_test)\n",
    "df = pd.DataFrame({'Expected':results+1,'ID':ID})\n",
    "df = df[['ID','Expected']]\n",
    "df.to_csv('try3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X_test.npy',X_test)\n",
    "\n",
    "np.save('X_train.npy',X)\n",
    "\n",
    "np.save('Y_train.npy',Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
