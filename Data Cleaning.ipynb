{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to convert words into vectors**\n",
    "\n",
    "\n",
    "Clear all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31292</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35344</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152538</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71871</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64913</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>112310</td>\n",
       "      <td>2013-01-20 13:25:59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I'll be the first to admit that I was not exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>176386</td>\n",
       "      <td>2016-05-07 01:21:02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Tracy dessert had a big name in Hong Kong and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>139555</td>\n",
       "      <td>2010-10-05 19:12:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This place has gone down hill.  Clearly they h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50173</td>\n",
       "      <td>2012-02-29 21:52:43</td>\n",
       "      <td>3.0</td>\n",
       "      <td>It's a giant Best Buy with 66 registers.  I do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13172</td>\n",
       "      <td>2011-11-30 02:11:15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Like walking back in time, every Saturday morn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40775</td>\n",
       "      <td>2017-12-15 23:27:08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Walked in around 4 on a Friday afternoon, we s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>116739</td>\n",
       "      <td>2016-05-07 01:36:53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Wow. So surprised at the one and two star revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12722</td>\n",
       "      <td>2018-04-27 20:25:26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Michael from Red Carpet VIP is amazing ! I rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>185188</td>\n",
       "      <td>2012-07-16 00:37:14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I cannot believe how things have changed in 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10013</td>\n",
       "      <td>2017-04-07 21:27:49</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You can't really find anything wrong with this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14110</td>\n",
       "      <td>2015-01-03 22:47:34</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great lunch today. Staff was very helpful in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33304</td>\n",
       "      <td>2017-05-26 01:23:19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We've been a huge Slim's fan since they opened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>97870</td>\n",
       "      <td>2014-06-27 21:19:23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good selection of classes of beers and mains. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33094</td>\n",
       "      <td>2016-01-17 05:26:22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The food is always good and the prices are rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64416</td>\n",
       "      <td>2016-07-25 03:57:19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Pick any meat on the planet and the chef will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>176384</td>\n",
       "      <td>2013-12-07 02:32:45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>PlumbSmart provided superior service from begi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>93217</td>\n",
       "      <td>2014-08-10 22:07:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unfortunately, I must recommend not to conduct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18125</td>\n",
       "      <td>2013-12-28 22:28:08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>their pettuccine was fresh-made in the morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>156644</td>\n",
       "      <td>2014-04-19 15:03:17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>if i can give this place no stars i would, i o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>81486</td>\n",
       "      <td>2017-02-09 03:43:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This review is in regards to our experience wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>125478</td>\n",
       "      <td>2014-12-17 19:04:33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ended up here because Raku was closed and it r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>85734</td>\n",
       "      <td>2014-06-27 21:32:31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I tried this place because my girls are away f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>173372</td>\n",
       "      <td>2015-12-05 02:37:03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Love this place downtown but the Scottsdale lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29601</td>\n",
       "      <td>2016-03-07 01:03:08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best chinese resto. Highly recommended. 5 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9439</td>\n",
       "      <td>2015-01-18 15:30:50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Went here last weekend and was pretty disappoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364596</th>\n",
       "      <td>128662</td>\n",
       "      <td>2018-11-05 20:10:48</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The Cabeza is amazing! I had the vampiros (tos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364597</th>\n",
       "      <td>41952</td>\n",
       "      <td>2018-05-25 09:29:21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>red tiger looks like a run of the mill pub on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364598</th>\n",
       "      <td>82735</td>\n",
       "      <td>2018-11-05 23:02:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A friend had heard amazing things about Hogtow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364599</th>\n",
       "      <td>170295</td>\n",
       "      <td>2018-11-06 00:10:02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Saw Star Wars episode VIII - great movie theat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364600</th>\n",
       "      <td>120269</td>\n",
       "      <td>2015-04-07 17:57:29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Was insulted after enjoying a meal with my wif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364601</th>\n",
       "      <td>109768</td>\n",
       "      <td>2018-11-03 20:22:27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5 - 5 STARS.\\nTiny place, hidden away in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364602</th>\n",
       "      <td>146387</td>\n",
       "      <td>2014-10-15 07:54:20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I brought my daughter's Toyota Sienna there to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364603</th>\n",
       "      <td>31591</td>\n",
       "      <td>2018-11-02 11:58:05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Created my own tuna and salmon custom poke bow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364604</th>\n",
       "      <td>60175</td>\n",
       "      <td>2015-07-04 09:37:05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Our lunch with our close freind was excellent....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364605</th>\n",
       "      <td>69670</td>\n",
       "      <td>2018-11-07 09:57:30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I think this is a place for people who don't u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364606</th>\n",
       "      <td>27928</td>\n",
       "      <td>2018-11-07 23:27:41</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The food is great ... we have had it 3 days in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364607</th>\n",
       "      <td>96897</td>\n",
       "      <td>2018-01-10 23:18:28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This STORE IS Price gouging!! Do not go here f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364608</th>\n",
       "      <td>156005</td>\n",
       "      <td>2017-07-02 21:44:58</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Went here on Canada day. This place was super ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364609</th>\n",
       "      <td>5771</td>\n",
       "      <td>2018-11-09 02:00:42</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This place was so convenient, fast, and not at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364610</th>\n",
       "      <td>92635</td>\n",
       "      <td>2018-11-09 07:19:34</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Kenny's here!! First off the owner of this pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364611</th>\n",
       "      <td>152320</td>\n",
       "      <td>2017-07-13 20:56:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Let our story be a cautionary tale so you can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364612</th>\n",
       "      <td>163651</td>\n",
       "      <td>2017-06-21 20:22:20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>So happy a great coffee joint opened in the we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364613</th>\n",
       "      <td>36897</td>\n",
       "      <td>2018-07-02 18:21:54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Went to have my thyroid checked up - they said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364614</th>\n",
       "      <td>52844</td>\n",
       "      <td>2018-10-06 02:31:06</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have been wanting to try this place and it d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364615</th>\n",
       "      <td>11381</td>\n",
       "      <td>2010-06-27 02:16:17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Cuteology Cakes created a cake for my Father o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364616</th>\n",
       "      <td>86084</td>\n",
       "      <td>2018-11-10 20:50:44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great food aside, I really want to make a shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364617</th>\n",
       "      <td>150456</td>\n",
       "      <td>2018-11-11 19:11:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The service is quick, the food is very good, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364618</th>\n",
       "      <td>163946</td>\n",
       "      <td>2012-03-29 23:35:45</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for a quick stop for a pre-made slice of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364619</th>\n",
       "      <td>97518</td>\n",
       "      <td>2016-11-16 21:52:35</td>\n",
       "      <td>4.0</td>\n",
       "      <td>As soon as we got there we got amazing service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364620</th>\n",
       "      <td>117874</td>\n",
       "      <td>2013-09-21 18:14:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We've been here b4 with no major problems but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364621</th>\n",
       "      <td>118509</td>\n",
       "      <td>2018-07-03 12:17:27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have been coming here for years and this pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364622</th>\n",
       "      <td>66388</td>\n",
       "      <td>2013-05-22 15:32:08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I think this owner and the owner of Amy's Baki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364623</th>\n",
       "      <td>117152</td>\n",
       "      <td>2018-11-13 21:15:20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Off the grid Mexican in Vegas. Very tasty, qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364624</th>\n",
       "      <td>151699</td>\n",
       "      <td>2018-08-13 18:09:16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We hired Taco Naco to cater our family party a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364625</th>\n",
       "      <td>161828</td>\n",
       "      <td>2018-11-12 20:25:32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Having just come back from Hawaii a few months...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5364626 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         business_id                 date  stars  \\\n",
       "0              31292  2013-05-07 04:34:36    1.0   \n",
       "1              35344  2017-01-14 21:30:33    5.0   \n",
       "2             152538  2016-11-09 20:09:03    5.0   \n",
       "3              71871  2018-01-09 20:56:38    5.0   \n",
       "4              64913  2018-01-30 23:07:38    1.0   \n",
       "5             112310  2013-01-20 13:25:59    4.0   \n",
       "6             176386  2016-05-07 01:21:02    3.0   \n",
       "7             139555  2010-10-05 19:12:35    1.0   \n",
       "8              50173  2012-02-29 21:52:43    3.0   \n",
       "9              13172  2011-11-30 02:11:15    4.0   \n",
       "10             40775  2017-12-15 23:27:08    1.0   \n",
       "11            116739  2016-05-07 01:36:53    4.0   \n",
       "12             12722  2018-04-27 20:25:26    4.0   \n",
       "13            185188  2012-07-16 00:37:14    1.0   \n",
       "14             10013  2017-04-07 21:27:49    5.0   \n",
       "15             14110  2015-01-03 22:47:34    4.0   \n",
       "16             33304  2017-05-26 01:23:19    5.0   \n",
       "17             97870  2014-06-27 21:19:23    3.0   \n",
       "18             33094  2016-01-17 05:26:22    4.0   \n",
       "19             64416  2016-07-25 03:57:19    5.0   \n",
       "20            176384  2013-12-07 02:32:45    5.0   \n",
       "21             93217  2014-08-10 22:07:35    1.0   \n",
       "22             18125  2013-12-28 22:28:08    5.0   \n",
       "23            156644  2014-04-19 15:03:17    1.0   \n",
       "24             81486  2017-02-09 03:43:25    1.0   \n",
       "25            125478  2014-12-17 19:04:33    5.0   \n",
       "26             85734  2014-06-27 21:32:31    1.0   \n",
       "27            173372  2015-12-05 02:37:03    1.0   \n",
       "28             29601  2016-03-07 01:03:08    5.0   \n",
       "29              9439  2015-01-18 15:30:50    2.0   \n",
       "...              ...                  ...    ...   \n",
       "5364596       128662  2018-11-05 20:10:48    5.0   \n",
       "5364597        41952  2018-05-25 09:29:21    4.0   \n",
       "5364598        82735  2018-11-05 23:02:24    1.0   \n",
       "5364599       170295  2018-11-06 00:10:02    5.0   \n",
       "5364600       120269  2015-04-07 17:57:29    1.0   \n",
       "5364601       109768  2018-11-03 20:22:27    5.0   \n",
       "5364602       146387  2014-10-15 07:54:20    5.0   \n",
       "5364603        31591  2018-11-02 11:58:05    3.0   \n",
       "5364604        60175  2015-07-04 09:37:05    5.0   \n",
       "5364605        69670  2018-11-07 09:57:30    2.0   \n",
       "5364606        27928  2018-11-07 23:27:41    5.0   \n",
       "5364607        96897  2018-01-10 23:18:28    1.0   \n",
       "5364608       156005  2017-07-02 21:44:58    3.0   \n",
       "5364609         5771  2018-11-09 02:00:42    5.0   \n",
       "5364610        92635  2018-11-09 07:19:34    5.0   \n",
       "5364611       152320  2017-07-13 20:56:12    1.0   \n",
       "5364612       163651  2017-06-21 20:22:20    5.0   \n",
       "5364613        36897  2018-07-02 18:21:54    1.0   \n",
       "5364614        52844  2018-10-06 02:31:06    5.0   \n",
       "5364615        11381  2010-06-27 02:16:17    5.0   \n",
       "5364616        86084  2018-11-10 20:50:44    5.0   \n",
       "5364617       150456  2018-11-11 19:11:03    5.0   \n",
       "5364618       163946  2012-03-29 23:35:45    4.0   \n",
       "5364619        97518  2016-11-16 21:52:35    4.0   \n",
       "5364620       117874  2013-09-21 18:14:30    1.0   \n",
       "5364621       118509  2018-07-03 12:17:27    5.0   \n",
       "5364622        66388  2013-05-22 15:32:08    1.0   \n",
       "5364623       117152  2018-11-13 21:15:20    5.0   \n",
       "5364624       151699  2018-08-13 18:09:16    5.0   \n",
       "5364625       161828  2018-11-12 20:25:32    3.0   \n",
       "\n",
       "                                                      text  \n",
       "0        Total bill for this horrible service? Over $8G...  \n",
       "1        I *adore* Travis at the Hard Rock's new Kelly ...  \n",
       "2        I have to say that this office really has it t...  \n",
       "3        Went in for a lunch. Steak sandwich was delici...  \n",
       "4        Today was my second out of three sessions I ha...  \n",
       "5        I'll be the first to admit that I was not exci...  \n",
       "6        Tracy dessert had a big name in Hong Kong and ...  \n",
       "7        This place has gone down hill.  Clearly they h...  \n",
       "8        It's a giant Best Buy with 66 registers.  I do...  \n",
       "9        Like walking back in time, every Saturday morn...  \n",
       "10       Walked in around 4 on a Friday afternoon, we s...  \n",
       "11       Wow. So surprised at the one and two star revi...  \n",
       "12       Michael from Red Carpet VIP is amazing ! I rea...  \n",
       "13       I cannot believe how things have changed in 3 ...  \n",
       "14       You can't really find anything wrong with this...  \n",
       "15       Great lunch today. Staff was very helpful in a...  \n",
       "16       We've been a huge Slim's fan since they opened...  \n",
       "17       Good selection of classes of beers and mains. ...  \n",
       "18       The food is always good and the prices are rea...  \n",
       "19       Pick any meat on the planet and the chef will ...  \n",
       "20       PlumbSmart provided superior service from begi...  \n",
       "21       Unfortunately, I must recommend not to conduct...  \n",
       "22       their pettuccine was fresh-made in the morning...  \n",
       "23       if i can give this place no stars i would, i o...  \n",
       "24       This review is in regards to our experience wa...  \n",
       "25       ended up here because Raku was closed and it r...  \n",
       "26       I tried this place because my girls are away f...  \n",
       "27       Love this place downtown but the Scottsdale lo...  \n",
       "28       Best chinese resto. Highly recommended. 5 star...  \n",
       "29       Went here last weekend and was pretty disappoi...  \n",
       "...                                                    ...  \n",
       "5364596  The Cabeza is amazing! I had the vampiros (tos...  \n",
       "5364597  red tiger looks like a run of the mill pub on ...  \n",
       "5364598  A friend had heard amazing things about Hogtow...  \n",
       "5364599  Saw Star Wars episode VIII - great movie theat...  \n",
       "5364600  Was insulted after enjoying a meal with my wif...  \n",
       "5364601  4.5 - 5 STARS.\\nTiny place, hidden away in the...  \n",
       "5364602  I brought my daughter's Toyota Sienna there to...  \n",
       "5364603  Created my own tuna and salmon custom poke bow...  \n",
       "5364604  Our lunch with our close freind was excellent....  \n",
       "5364605  I think this is a place for people who don't u...  \n",
       "5364606  The food is great ... we have had it 3 days in...  \n",
       "5364607  This STORE IS Price gouging!! Do not go here f...  \n",
       "5364608  Went here on Canada day. This place was super ...  \n",
       "5364609  This place was so convenient, fast, and not at...  \n",
       "5364610  Kenny's here!! First off the owner of this pla...  \n",
       "5364611  Let our story be a cautionary tale so you can ...  \n",
       "5364612  So happy a great coffee joint opened in the we...  \n",
       "5364613  Went to have my thyroid checked up - they said...  \n",
       "5364614  I have been wanting to try this place and it d...  \n",
       "5364615  Cuteology Cakes created a cake for my Father o...  \n",
       "5364616  Great food aside, I really want to make a shou...  \n",
       "5364617  The service is quick, the food is very good, a...  \n",
       "5364618  Great for a quick stop for a pre-made slice of...  \n",
       "5364619  As soon as we got there we got amazing service...  \n",
       "5364620  We've been here b4 with no major problems but ...  \n",
       "5364621  I have been coming here for years and this pla...  \n",
       "5364622  I think this owner and the owner of Amy's Baki...  \n",
       "5364623  Off the grid Mexican in Vegas. Very tasty, qua...  \n",
       "5364624  We hired Taco Naco to cater our family party a...  \n",
       "5364625  Having just come back from Hawaii a few months...  \n",
       "\n",
       "[5364626 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open('./Data_Module2/review_train.json') as f:\n",
    "    r_train = f.readlines()\n",
    "    r_train = list(map(json.loads,r_train))\n",
    "    \n",
    "pd.DataFrame(r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_train = pd.DataFrame(r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = r_train.loc[:,'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:01<00:00, 413.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['total', 'bill', 'horrible', 'service', 'crooks', 'actually', 'nerve', 'charge', 'us', 'pills', 'checked', 'online', 'pills', 'cents', 'avoid', 'hospital', 'ers', 'costs'], ['adore', 'travis', 'hard', 'rock', 'new', 'kelly', 'cardenas', 'salon', 'always', 'fan', 'great', 'blowout', 'stranger', 'chains', 'offer', 'service', 'however', 'travis', 'taken', 'flawless', 'blowout', 'whole', 'new', 'level', 'travis', 'greets', 'perfectly', 'green', 'swoosh', 'otherwise', 'perfectly', 'styled', 'black', 'hair', 'vegasworthy', 'rockstar', 'outfit', 'next', 'comes', 'relaxing', 'incredible', 'shampoo', 'get', 'full', 'head', 'message', 'could', 'cure', 'even', 'worst', 'migraine', 'minutes', 'scented', 'shampoo', 'room', 'travis', 'freakishly', 'strong', 'fingers', 'good', 'way', 'use', 'perfect', 'amount', 'pressure', 'superb', 'starts', 'glorious', 'blowout', 'one', 'two', 'three', 'people', 'involved', 'best', 'roundbrush', 'action', 'hair', 'ever', 'seen', 'team', 'stylists', 'clearly', 'gets', 'along', 'extremely', 'well', 'evident', 'way', 'talk', 'help', 'one', 'another', 'really', 'genuine', 'corporate', 'requirement', 'much', 'fun', 'next', 'travis', 'started', 'flat', 'iron', 'way', 'flipped', 'wrist', 'get', 'volume', 'around', 'without', 'overdoing', 'making', 'look', 'like', 'texas', 'pagent', 'girl', 'admirable', 'also', 'worth', 'noting', 'nt', 'fry', 'hair', 'something', 'happen', 'less', 'skilled', 'stylists', 'end', 'blowout', 'style', 'hair', 'perfectly', 'bouncey', 'looked', 'terrific', 'thing', 'better', 'awesome', 'blowout', 'lasted', 'days', 'travis', 'see', 'every', 'single', 'time', 'vegas', 'make', 'feel', 'beauuuutiful'], ['say', 'office', 'really', 'together', 'organized', 'friendly', 'dr', 'j', 'phillipp', 'great', 'dentist', 'friendly', 'professional', 'dental', 'assistants', 'helped', 'procedure', 'amazing', 'jewel', 'bailey', 'helped', 'feel', 'comfortable', 'nt', 'dental', 'insurance', 'insurance', 'office', 'purchase', 'something', 'year', 'gave', 'dental', 'work', 'plus', 'helped', 'get', 'signed', 'care', 'credit', 'knew', 'nothing', 'visit', 'highly', 'recommend', 'office', 'nice', 'synergy', 'whole', 'office'], ['went', 'lunch', 'steak', 'sandwich', 'delicious', 'caesar', 'salad', 'absolutely', 'delicious', 'dressing', 'perfect', 'amount', 'dressing', 'distributed', 'perfectly', 'across', 'leaf', 'know', 'going', 'salad', 'perfect', 'drink', 'prices', 'pretty', 'good', 'server', 'dawn', 'friendly', 'accommodating', 'happy', 'summation', 'great', 'pub', 'experience', 'would', 'go'], ['today', 'second', 'three', 'sessions', 'paid', 'although', 'first', 'session', 'went', 'well', 'could', 'tell', 'meredith', 'particular', 'enjoyment', 'male', 'clients', 'female', 'however', 'returned', 'teeth', 'fine', 'pleased', 'results', 'went', 'today', 'whitening', 'room', 'three', 'gentlemen', 'appointment', 'started', 'well', 'although', 'person', 'service', 'industry', 'always', 'attend', 'female', 'clientele', 'first', 'couple', 'arrives', 'unbothered', 'signs', 'waited', 'turn', 'checked', 'original', 'minute', 'timer', 'ask', 'ok', 'attended', 'boyfriend', 'numerous', 'occasions', 'well', 'men', 'would', 'exit', 'room', 'without', 'even', 'asking', 'looking', 'see', 'irritation', 'half', 'way', 'another', 'woman', 'showed', 'explaining', 'deals', 'lobby', 'admits', 'timers', 'must', 'reset', 'half', 'way', 'process', 'reset', 'boyfriends', 'left', 'rest', 'gentleman', 'furthest', 'away', 'time', 'come', 'redeem', 'deal', 'get', 'set', 'gave', 'timer', 'done', 'left', 'point', 'time', 'minutes', 'reset', 'minutes', 'ago', 'according', 'sat', 'patiently', 'whole', 'time', 'major', 'pain', 'gums', 'watched', 'time', 'lamp', 'shut', 'reset', 'two', 'others', 'explained', 'deals', 'guest', 'never', 'checked', 'time', 'light', 'turned', 'released', 'stance', 'mouth', 'relaxed', 'state', 'assuming', 'getting', 'thirty', 'minute', 'session', 'instead', 'usual', 'yet', 'come', 'point', 'teeth', 'formula', 'burning', 'gum', 'neglected', 'minutes', 'began', 'burn', 'lips', 'began', 'squealing', 'slapping', 'chair', 'trying', 'get', 'attention', 'room', 'panic', 'much', 'pain', 'time', 'entered', 'room', 'already', 'chair', 'finally', 'acknowledged', 'asked', 'could', 'put', 'vitamin', 'e', 'gum', 'burn', 'pictured', 'point', 'treated', 'two', 'gums', 'burns', 'neglecting', 'irritated', 'suffer', 'wanted', 'leave', 'waited', 'boyfriend', 'kept', 'harassing', 'issue', 'saying', 'well', 'burns', 'come', 'teeth', 'whitening', 'totally', 'agree', 'justifiable', 'circumstances', 'would', 'irritate', 'could', 'easily', 'avoid', 'checked', 'even', 'second', 'time', 'could', 'let', 'know', 'never', 'check', 'physical', 'health', 'could', 'nt', 'even', 'take', 'two', 'seconds', 'reset', 'timer', 'even', 'admitted', 'accuse', 'coming', 'light', 'solid', 'two', 'minutes', 'could', 'nt', 'stand', 'pain', 'admitted', 'reset', 'every', 'minutes', 'means', 'minutes', 'bother', 'help', 'guest', 'lobby', 'proceeded', 'attack', 'well', 'simply', 'wanted', 'leave', 'way', 'treated', 'also', 'expected', 'refund', 'getting', 'complete', 'session', 'today', 'due', 'neglect', 'fact', 'wo', 'nt', 'returning', 'last', 'failed', 'even', 'screaming', 'door', 'continued', 'boyfriend', 'steps', 'never', 'life', 'appalled', 'grown', 'woman', 'behavior', 'claims', 'business', 'years', 'admit', 'wrongs', 'nt', 'make', 'guest', 'feel', 'unwelcome', 'ca', 'nt', 'job', 'properly'], ['first', 'admit', 'excited', 'going', 'la', 'tavolta', 'food', 'snob', 'group', 'friends', 'suggested', 'go', 'dinner', 'looked', 'online', 'menu', 'nothing', 'special', 'seemed', 'overpriced', 'im', 'also', 'big', 'ordering', 'pasta', 'go', 'alas', 'outnumbered', 'thank', 'goodness', 'ordered', 'sea', 'bass', 'special', 'die', 'cooked', 'perfectly', 'seasoned', 'perfectly', 'perfect', 'portion', 'say', 'enough', 'good', 'things', 'dish', 'server', 'asked', 'seemed', 'proud', 'dish', 'said', 'nt', 'chef', 'incredible', 'job', 'hubby', 'got', 'crab', 'tortellini', 'also', 'loved', 'heard', 'mmmm', 'good', 'around', 'table', 'waiter', 'super', 'nice', 'even', 'gave', 'us', 'free', 'desserts', 'last', 'people', 'restaurant', 'service', 'slow', 'place', 'packed', 'jugs', 'wine', 'large', 'group', 'good', 'conversation', 'nt', 'seem', 'bother', 'anyone', 'order', 'calamari', 'fried', 'zucchini', 'appetizers', 'leave', 'mussels', 'sea', 'bass', 'special', 'highly', 'recommend', 'chicken', 'parm', 'crab', 'tortellini', 'also', 'good', 'big', 'chicken', 'romano', 'bit', 'bland', 'house', 'salads', 'teeny', 'make', 'reservation', 'still', 'expect', 'wait', 'food', 'go', 'large', 'group', 'people', 'plan', 'loud', 'nt', 'go', 'date', 'unless', 'fighting', 'nt', 'feel', 'like', 'hearing', 'anything', 'say', 'ask', 'sit', 'side', 'room', 'available'], ['tracy', 'dessert', 'big', 'name', 'hong', 'kong', 'one', 'first', 'markham', 'place', 'many', 'years', 'came', 'chinese', 'dessert', 'must', 'say', 'selection', 'increased', 'tremendously', 'years', 'might', 'well', 'add', 'price', 'also', 'increased', 'tremendously', 'well', 'waitress', 'gave', 'us', 'tea', 'could', 'taste', 'red', 'date', 'fancy', 'simple', 'taro', 'coconut', 'tapioca', 'pearls', 'like', 'something', 'basically', 'desserts', 'crazy', 'literally', 'make', 'dessert', 'home', 'bowl', 'would', 'probably', 'cost', 'like', 'years', 'ago', 'think', 'still', 'get', 'like', 'reasonable', 'wow', 'little', 'top', 'dessert', 'though', 'must', 'say', 'tracy', 'dessert', 'little', 'expensive', 'side', 'also', 'saw', 'items', 'menu', 'like', 'fish', 'balls', 'chicken', 'wings', 'shaved', 'ice', 'friend', 'got', 'mango', 'drink', 'fresh', 'mango', 'also', 'surprised', 'many', 'people', 'come', 'tracy', 'dessert', 'work', 'came', 'sunday', 'tables', 'always', 'filled', 'think', 'amount', 'tables', 'perfect', 'one', 'really', 'waited', 'seats', 'long', 'time', 'tables', 'kept', 'filling', 'table', 'finished'], ['place', 'gone', 'hill', 'clearly', 'cut', 'back', 'staff', 'food', 'quality', 'many', 'reviews', 'written', 'menu', 'changed', 'going', 'years', 'food', 'quality', 'gone', 'hill', 'service', 'slow', 'salad', 'bad', 'gets', 'worth', 'spending', 'money', 'place', 'many', 'options'], ['giant', 'best', 'buy', 'registers', 'nt', 'get', 'big', 'deal', 'place'], ['like', 'walking', 'back', 'time', 'every', 'saturday', 'morning', 'sister', 'bowling', 'league', 'done', 'spend', 'quarters', 'playing', 'pin', 'ball', 'machines', 'mother', 'came', 'pick', 'us', 'sister', 'daring', 'play', 'machines', 'hard', 'afraid', 'tilt', 'showing', 'freezing', 'game', 'hand', 'bit', 'gentler', 'wanted', 'make', 'sure', 'got', 'quarter', 'worth', 'place', 'rows', 'rows', 'machines', 'really', 'old', 'mid', 'theme', 'even', 'ms', 'pac', 'man', 'fun', 'spend', 'afternoon', 'playing', 'machines', 'remembering', 'fun', 'early', 'teen', 'years']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "trysize = 100000\n",
    "t = [[1]]*trysize\n",
    "for i in tqdm(range(trysize)):\n",
    "    #Split into words\n",
    "    x = word_tokenize(review[i])\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) for w in x]\n",
    "    #Remove not alphabetic\n",
    "    x = [word for word in x if word.isalpha()]\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in stop_words]\n",
    "    t[i] = x\n",
    "print(t[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=91329, size=100, alpha=0.025)\n",
      "[-1.4556885  -0.33585608 -0.6782701   2.275646    1.3267819  -1.0261291\n",
      " -4.9021792   2.644819   -0.69156855 -0.6271913  -0.97369856 -2.3531938\n",
      "  0.06749574  0.4809777   0.32824266  0.37764043  0.44098464 -1.0903133\n",
      "  1.632918    0.15561041  1.3577223   0.760136   -0.9387329   1.5962226\n",
      " -0.19135228 -1.5760553  -0.526181   -1.922608   -1.2162629   0.32300267\n",
      " -1.5473568  -1.2388549  -0.9056763  -0.52151984  0.19716182  1.6705158\n",
      " -0.42407972  2.3243022   0.48484915 -0.17529786  0.73968786  0.8218942\n",
      " -2.3533404   0.81366223 -1.1841733  -1.3936126  -0.86453956  0.46341255\n",
      "  0.23869321 -1.895463   -2.3920715  -1.4740905   1.2214156  -2.3412232\n",
      " -0.51751935 -1.634826    0.14904673 -1.4511067  -1.3100302  -1.3405745\n",
      " -2.0820386   2.2555356  -0.25955355  0.32723087 -0.42610422  1.9230661\n",
      "  1.0762017  -0.88257694 -1.7148213   0.8274371   0.02922312  0.75315344\n",
      "  0.13441572 -1.1449076   0.14433318  2.6472998  -0.42584273 -0.13173626\n",
      " -2.1690707   2.090566   -0.5539377   0.13421395 -2.2772489   0.41845447\n",
      "  0.8282574   0.97822034 -2.4671066   1.0482163  -1.987955   -1.144829\n",
      "  0.603301   -0.54940516  0.47851336 -0.45272154 -1.0364332  -0.46000993\n",
      "  1.8513988   1.2767409  -0.08637756  2.7005298 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(t, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "# access vector for one word\n",
    "print(model.wv['bill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = r_train['stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence to vec (average)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 7/1000 [00:00<00:15, 64.28it/s]\u001b[A\n",
      "  1%|          | 10/1000 [00:00<00:22, 44.79it/s]\u001b[A\n",
      "  4%|▍         | 38/1000 [00:00<00:16, 59.87it/s]\u001b[A\n",
      "  8%|▊         | 81/1000 [00:00<00:11, 80.71it/s]\u001b[A\n",
      " 10%|█         | 104/1000 [00:00<00:09, 99.11it/s]\u001b[A\n",
      " 12%|█▎        | 125/1000 [00:00<00:07, 116.47it/s]\u001b[A\n",
      " 15%|█▍        | 146/1000 [00:00<00:07, 119.20it/s]\u001b[A\n",
      " 16%|█▋        | 164/1000 [00:00<00:06, 123.72it/s]\u001b[A\n",
      " 20%|█▉        | 195/1000 [00:01<00:05, 145.17it/s]\u001b[A\n",
      " 24%|██▍       | 240/1000 [00:01<00:04, 180.31it/s]\u001b[A\n",
      " 29%|██▉       | 293/1000 [00:01<00:03, 224.13it/s]\u001b[A\n",
      " 36%|███▌      | 359/1000 [00:01<00:02, 279.47it/s]\u001b[A\n",
      " 44%|████▎     | 435/1000 [00:01<00:01, 342.37it/s]\u001b[A\n",
      " 50%|████▉     | 498/1000 [00:01<00:01, 392.06it/s]\u001b[A\n",
      " 55%|█████▌    | 553/1000 [00:01<00:01, 342.21it/s]\u001b[A\n",
      " 60%|█████▉    | 599/1000 [00:01<00:01, 359.44it/s]\u001b[A\n",
      " 66%|██████▌   | 658/1000 [00:02<00:00, 392.94it/s]\u001b[A\n",
      " 72%|███████▏  | 720/1000 [00:02<00:00, 436.30it/s]\u001b[A\n",
      " 77%|███████▋  | 770/1000 [00:02<00:00, 412.72it/s]\u001b[A\n",
      " 83%|████████▎ | 826/1000 [00:02<00:00, 447.10it/s]\u001b[A\n",
      " 88%|████████▊ | 875/1000 [00:02<00:00, 440.07it/s]\u001b[A\n",
      " 93%|█████████▎| 927/1000 [00:02<00:00, 458.98it/s]\u001b[A\n",
      " 98%|█████████▊| 976/1000 [00:02<00:00, 441.93it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 360.62it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "x = np.empty((0,100), float)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    x = np.vstack((x,model.wv[t[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi class logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "mod = LogisticRegression(solver = 'lbfgs')\n",
    "mod.fit(x[:900], data[:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 41\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "y_pred = mod.predict(x[-100:])\n",
    "count_misclassified = (data[900:1000] != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = metrics.accuracy_score(data[900:1000], y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence to vec (From github for reference)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.matutils import sparse2full\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "#text2vec methods\n",
    "class text2vec():\n",
    "    def __init__(self, doc_list):\n",
    "        #Initialize\n",
    "        self.doc_list = doc_list\n",
    "        self.nlp, self.docs, self.docs_dict = self._preprocess(self.doc_list)\n",
    "    \n",
    "    # Functions to lemmatise docs\n",
    "    def _keep_token(self, t):\n",
    "        return (t.is_alpha and \n",
    "                not (t.is_space or t.is_punct or \n",
    "                     t.is_stop or t.like_num))\n",
    "    def _lemmatize_doc(self, doc):\n",
    "        return [ t.lemma_ for t in doc if self._keep_token(t)]\n",
    "\n",
    "\n",
    "    #Gensim to create a dictionary and filter out stop and infrequent words (lemmas).\n",
    "    def _get_docs_dict(self, docs):\n",
    "        docs_dict = Dictionary(docs)\n",
    "        #CAREFUL: For small corpus please carefully modify the parameters for filter_extremes, or simply comment it out.\n",
    "        docs_dict.filter_extremes(no_below=5, no_above=0.2)\n",
    "        docs_dict.compactify()\n",
    "        return docs_dict\n",
    "\n",
    "    # Preprocess docs\n",
    "    def _preprocess(self, doc_list):\n",
    "        #Load spacy model\n",
    "        nlp  = spacy.load('en')\n",
    "        #lemmatise docs\n",
    "        docs = [self._lemmatize_doc(nlp(doc)) for doc in doc_list] \n",
    "        #Get docs dictionary\n",
    "        docs_dict = self._get_docs_dict(docs)\n",
    "        return nlp, docs, docs_dict\n",
    "\n",
    "\n",
    "    # Gensim can again be used to create a bag-of-words representation of each document,\n",
    "    # build the TF-IDF model, \n",
    "    # and compute the TF-IDF vector for each document.\n",
    "    def _get_tfidf(self, docs, docs_dict):\n",
    "        docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]\n",
    "        model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "        docs_tfidf  = model_tfidf[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "        return docs_vecs\n",
    "\n",
    "\n",
    "    #Get avg w2v for one document\n",
    "    def _document_vector(self, doc, docs_dict, nlp):\n",
    "        # remove out-of-vocabulary words\n",
    "        doc_vector = [nlp(word).vector for word in doc if word in docs_dict.token2id]\n",
    "        return np.mean(doc_vector, axis=0)\n",
    "\n",
    "\n",
    "    # Get a TF-IDF weighted Glove vector summary for document list\n",
    "    # Input: a list of documents, Output: Matrix of vector for all the documents\n",
    "    def tfidf_weighted_wv(self):\n",
    "        #tf-idf\n",
    "        docs_vecs   = self._get_tfidf(self.docs, self.docs_dict)\n",
    "\n",
    "        #Load glove embedding vector for each TF-IDF term\n",
    "        tfidf_emb_vecs = np.vstack([self.nlp(self.docs_dict[i]).vector for i in range(len(self.docs_dict))])\n",
    "\n",
    "        #To get a TF-IDF weighted Glove vector summary of each document, \n",
    "        #we just need to matrix multiply docs_vecs with tfidf_emb_vecs\n",
    "        docs_emb = np.dot(docs_vecs, tfidf_emb_vecs)\n",
    "\n",
    "        return docs_emb\n",
    "\n",
    "    # Get average vector for document list\n",
    "    def avg_wv(self):\n",
    "        docs_vecs = np.vstack([self._document_vector(doc, self.docs_dict, self.nlp) for doc in self.docs])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get TF-IDF vector for document list\n",
    "    def get_tfidf(self):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_tfidf = TfidfModel(docs_corpus, id2word=self.docs_dict)\n",
    "        docs_tfidf  = model_tfidf[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_tfidf])\n",
    "        return docs_vecs\n",
    "\n",
    "\n",
    "    # Get Latent Semantic Indexing(LSI) vector for document list\n",
    "    def get_lsi(self, num_topics=300):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_lsi = models.LsiModel(docs_corpus, num_topics, id2word=self.docs_dict)\n",
    "        docs_lsi  = model_lsi[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_lsi])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get Random Projections(RP) vector for document list\n",
    "    def get_rp(self):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_rp = models.RpModel(docs_corpus, id2word=self.docs_dict)\n",
    "        docs_rp  = model_rp[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_rp])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get Latent Dirichlet Allocation(LDA) vector for document list\n",
    "    def get_lda(self, num_topics=100):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_lda = models.LdaModel(docs_corpus, num_topics, id2word=self.docs_dict)\n",
    "        docs_lda  = model_lda[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_lda])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get Hierarchical Dirichlet Process(HDP) vector for document list\n",
    "    def get_hdp(self):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_hdp = models.HdpModel(docs_corpus, id2word=self.docs_dict)\n",
    "        docs_hdp  = model_hdp[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_hdp])\n",
    "        return docs_vecs\n",
    "\n",
    "    \n",
    "    \n",
    "#Similarity Calculation methods\n",
    "class simical():\n",
    "    def __init__(self, vec1, vec2):\n",
    "        self.vec1 = vec1\n",
    "        self.vec2 = vec2\n",
    "\n",
    "    def _VectorSize(self, vec) :\n",
    "        return math.sqrt(sum(math.pow(v,2) for v in vec))\n",
    "\n",
    "    def _InnerProduct(self) :\n",
    "        return sum(v1*v2 for v1,v2 in zip(self.vec1,self.vec2))\n",
    "\n",
    "    def _Theta(self) :\n",
    "        return math.acos(self.Cosine()) + 10\n",
    "   \n",
    "    def _Magnitude_Difference(self) :\n",
    "        return abs(self._VectorSize(self.vec1) - self._VectorSize(self.vec2))\n",
    "    \n",
    "    def Euclidean(self) :\n",
    "        return math.sqrt(sum(math.pow((v1-v2),2) for v1,v2 in zip(self.vec1, self.vec2)))\n",
    "    \n",
    "    def Cosine(self) :\n",
    "        result = self._InnerProduct() / (self._VectorSize(self.vec1) * self._VectorSize(self.vec2))\n",
    "        return result\n",
    "\n",
    "    def Triangle(self) :\n",
    "        theta = math.radians(self._Theta())\n",
    "        return (self._VectorSize(self.vec1) * self._VectorSize(self.vec2) * math.sin(theta)) / 2\n",
    "\n",
    "    def Sector(self) :\n",
    "        ED = self.Euclidean()\n",
    "        MD = self._Magnitude_Difference()\n",
    "        theta = self._Theta()\n",
    "        return math.pi * math.pow((ED+MD),2) * theta/360\n",
    "\n",
    "    def TS_SS(self) :\n",
    "        return self.Triangle() * self.Sector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2v = text2vec(r_train['text'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = t2v.tfidf_weighted_wv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 1415\n",
      "Accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, data[:10000], test_size=0.3, random_state=0)\n",
    "mod = LogisticRegression(solver = 'lbfgs')\n",
    "mod.fit(X_train, Y_train)\n",
    "y_pred = mod.predict(X_test)\n",
    "count_misclassified = (Y_test != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = metrics.accuracy_score(Y_test, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  1.5084208077765744\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "print(\"RMSE = \", sqrt(sum(np.square(Y_test-y_pred)) / len(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
