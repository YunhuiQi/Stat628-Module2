{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is to generate feature summary and topic scores for all business. \n",
    "## Usage: This file will take review_train.csv as initial input, and then it will generate data_sentence.txt, read data_sentence.txt, generate data_sentence_scores.csv and business_scores.csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import modules :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import FreqDist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import multiprocessing\n",
    "import time\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## define stopwords\n",
    "sr = stopwords.words('english')\n",
    "sr2 = stopwords.words('english')\n",
    "sr.remove('not')\n",
    "p = inflect.engine()\n",
    "wnl = WordNetLemmatizer() \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "porter = nltk.PorterStemmer()\n",
    "nlp = spacy.load(\"en\")\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the reviews data for gym business as pandas dataframe and let the name be data_review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read review data\n",
    "data_review=pd.read_csv(\"../data/review_train.csv\")\n",
    "data_review[\"review_number\"] = range(len(data_review))\n",
    "idset = data_review['business_id']\n",
    "idset = list(set(idset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Main step: clean data---LDA model---creaete topic-terms list---get dominant topics---get sentence scores---get business scores and feature summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create data_sentence dictionary for sentiment analysis and save it, here we want to have 'not' seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess(dataset):\n",
    "    data_sentence = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        ## dealing with \\n and \\n\\n things\n",
    "        xx = dataset.iloc[i]['text']\n",
    "        xx = re.sub(r' \\n\\n','.',xx)\n",
    "        xx = re.sub(r'\\n','',xx)\n",
    "        xx = re.sub(r'\\.\\.','. ',xx)\n",
    "        xx = re.sub(r'(\\.)(\\S)',r'\\1 \\2',xx)\n",
    "        \n",
    "        ## for each review, tokenize it into sentences, saved in sent_set\n",
    "        sent_set = sent_tokenize(xx)\n",
    "    \n",
    "        ## for each sentence in sent_set, tokenized and cleaning into tokenized_sentence\n",
    "        for j in range(len(sent_set)):\n",
    "        \n",
    "            sent = sent_set[j]\n",
    "            if len(sent) > 3:\n",
    "                ## get \"review_number\" for this sentence\n",
    "                data_sentence.setdefault('review_number', []).append(dataset.iloc[i]['review_number'])\n",
    "        \n",
    "                ## assign this sentence into column \"sentence\" and sentence_list\n",
    "                data_sentence.setdefault('sentence', []).append(sent)\n",
    "\n",
    "        \n",
    "                ## clean this sentence into \"tokenized_sentence\"\n",
    "                x = re.sub(r'n\\'t',' not',sent)\n",
    "                ## split into words\n",
    "                x = word_tokenize(x)\n",
    "                ## remove punctuation\n",
    "                x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "                ## change numbers into words\n",
    "                x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "                ## remove not alphabetic\n",
    "                x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "                ## convert to lower case\n",
    "                x = [w.lower() for w in x]\n",
    "                ## remove stop words\n",
    "                x = [w for w in x if not w in sr]\n",
    "                ## lemmatization\n",
    "                x = [wnl.lemmatize(w) for w in x]\n",
    "            \n",
    "                ## assign cleaned sentence words to \"tokenized_sentence\" and tokenized_sentence_list\n",
    "                data_sentence.setdefault('tokenized_sentence', []).append(x)\n",
    "\n",
    "\n",
    "                \n",
    "                ## POS\n",
    "                ## change cleaned words into nlp format\n",
    "                sent_nlp = nlp(\" \".join(x))\n",
    "                ## get nouns for each sentence and saved into nouns_list\n",
    "                nolis = [token.lemma_ for token in sent_nlp if token.pos_ == \"NOUN\"]\n",
    "                ## assign nouns to \"nouns\"\n",
    "                data_sentence.setdefault('nouns', []).append(nolis)\n",
    "            \n",
    "    return data_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentence = dataprocess(data_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data into data_sentence.txt\n",
    "fw = open(\"../data/data_sentence.txt\",'w+')\n",
    "fw.write(str(data_sentence))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data_sentence.txt\n",
    "fr = open(\"../data/data_sentence.txt\",'r+')\n",
    "data_sentence_dic = eval(fr.read()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create data_sentence_formodeltraining dictionary for model training and save it, here we want to have 'not_' because not seperately is not helpful for training model, and we want to distinguish not_adjective and adjective when training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess_formodeltraining(dataset):\n",
    "    data_sentence = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        ## dealing with \\n and \\n\\n things\n",
    "        xx = dataset.iloc[i]['text']\n",
    "        xx = re.sub(r' \\n\\n','.',xx)\n",
    "        xx = re.sub(r'\\n','',xx)\n",
    "        xx = re.sub(r'\\.\\.','. ',xx)\n",
    "        xx = re.sub(r'(\\.)(\\S)',r'\\1 \\2',xx)\n",
    "        \n",
    "        ## for each review, tokenize it into sentences, saved in sent_set\n",
    "        sent_set = sent_tokenize(xx)\n",
    "    \n",
    "        ## for each sentence in sent_set, tokenized and cleaning into tokenized_sentence\n",
    "        for j in range(len(sent_set)):\n",
    "        \n",
    "            sent = sent_set[j]\n",
    "            if len(sent) > 3:\n",
    "                ## get \"review_number\" for this sentence\n",
    "                data_sentence.setdefault('review_number', []).append(dataset.iloc[i]['review_number'])\n",
    "        \n",
    "                ## assign this sentence into column \"sentence\" and sentence_list\n",
    "                data_sentence.setdefault('sentence', []).append(sent)\n",
    "\n",
    "        \n",
    "                ## clean this sentence into \"tokenized_sentence\"\n",
    "                x = re.sub(r'n\\'t',' not',sent)\n",
    "                ## change not adj into not_adj\n",
    "                x = re.sub(r'not ','not_',x)\n",
    "                ## split into words\n",
    "                x = word_tokenize(x)\n",
    "                ## remove punctuation\n",
    "                x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "                ## change numbers into words\n",
    "                x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "                ## remove not alphabetic\n",
    "                x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "                ## convert to lower case\n",
    "                x = [w.lower() for w in x]\n",
    "                ## remove stop words\n",
    "                x = [w for w in x if not w in sr]\n",
    "                ## lemmatization\n",
    "                x = [wnl.lemmatize(w) for w in x]\n",
    "            \n",
    "                ## assign cleaned sentence words to \"tokenized_sentence\" and tokenized_sentence_list\n",
    "                data_sentence.setdefault('tokenized_sentence', []).append(x)\n",
    "\n",
    "\n",
    "                \n",
    "                ## POS\n",
    "                ## change cleaned words into nlp format\n",
    "                sent_nlp = nlp(\" \".join(x))\n",
    "                ## get nouns for each sentence and saved into nouns_list\n",
    "                nolis = [token.lemma_ for token in sent_nlp if token.pos_ == \"NOUN\"]\n",
    "                ## assign nouns to \"nouns\"\n",
    "                data_sentence.setdefault('nouns', []).append(nolis)\n",
    "            \n",
    "    return data_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35033/35033 [33:33<00:00, 17.40it/s]\n"
     ]
    }
   ],
   "source": [
    "data_sentence_formodeltraining = dataprocess_formodeltraining(data_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data into data_sentence_formodeltraining.txt\n",
    "fw = open(\"../data/data_sentence_formodeltraining.txt\",'w+')\n",
    "fw.write(str(data_sentence_formodeltraining))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data_sentence_formodeltraining.txt\n",
    "fr = open(\"../data/data_sentence_formodeltraining.txt\",'r+')\n",
    "data_sentence_dic_formodeltraining = eval(fr.read()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create frequency and tfidf matrix for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcorpus(datadict):\n",
    "    \n",
    "    nouns_list = datadict[\"nouns\"]\n",
    "    \n",
    "    ## define dictionary for \"nouns\"\n",
    "    dictionary = corpora.Dictionary(nouns_list)\n",
    "\n",
    "    ## create frequency matrix\n",
    "    frequency_matrix = [dictionary.doc2bow(n) for n in nouns_list]\n",
    "                    \n",
    "    ## create tfidf matrix\n",
    "    tfidf = gensim.models.TfidfModel(frequency_matrix)\n",
    "    corpus_tfidf = tfidf[frequency_matrix]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf, dictionary = getcorpus(data_sentence_dic_formodeltraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply LDA topic model to tfidf matrix and visualization and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply LDA model on tfidf matrix                                                        \n",
    "lda_model = LDA(corpus_tfidf, id2word=dictionary, num_topics=7, random_state=100,chunksize=10000, passes=50)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "#lda_model.save(\"ldamodel/lda_7topics.model\")\n",
    "\n",
    "## load model\n",
    "lda_model=  models.LdaModel.load('ldamodel/lda_7topics.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.029*\"trainer\" + 0.027*\"massage\" + 0.022*\"location\" + 0.019*\"fee\" + 0.015*\"review\" + 0.014*\"minute\" + 0.014*\"hour\" + 0.013*\"sign\" + 0.013*\"instructor\" + 0.012*\"u\"'),\n",
       " (1,\n",
       "  '0.023*\"fitness\" + 0.020*\"thing\" + 0.018*\"coach\" + 0.018*\"anything\" + 0.017*\"guy\" + 0.014*\"music\" + 0.013*\"today\" + 0.013*\"thank\" + 0.013*\"level\" + 0.009*\"planet\"'),\n",
       " (2,\n",
       "  '0.045*\"love\" + 0.038*\"room\" + 0.034*\"service\" + 0.023*\"everything\" + 0.022*\"locker\" + 0.022*\"customer\" + 0.018*\"kid\" + 0.017*\"shower\" + 0.017*\"area\" + 0.014*\"gym\"'),\n",
       " (3,\n",
       "  '0.082*\"gym\" + 0.036*\"year\" + 0.032*\"member\" + 0.031*\"membership\" + 0.022*\"money\" + 0.016*\"anyone\" + 0.016*\"manager\" + 0.015*\"contract\" + 0.015*\"family\" + 0.014*\"business\"'),\n",
       " (4,\n",
       "  '0.049*\"staff\" + 0.047*\"equipment\" + 0.037*\"machine\" + 0.036*\"facility\" + 0.030*\"people\" + 0.024*\"weight\" + 0.023*\"lot\" + 0.020*\"star\" + 0.017*\"gym\" + 0.017*\"cardio\"'),\n",
       " (5,\n",
       "  '0.054*\"class\" + 0.050*\"place\" + 0.022*\"price\" + 0.022*\"week\" + 0.021*\"training\" + 0.017*\"fun\" + 0.015*\"group\" + 0.014*\"session\" + 0.014*\"yoga\" + 0.013*\"workout\"'),\n",
       " (6,\n",
       "  '0.033*\"time\" + 0.032*\"workout\" + 0.032*\"experience\" + 0.031*\"spa\" + 0.029*\"day\" + 0.028*\"everyone\" + 0.024*\"month\" + 0.019*\"vega\" + 0.016*\"gym\" + 0.012*\"club\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print all topics\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el67721216187790163946406106\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el67721216187790163946406106_data = {\"mdsDat\": {\"x\": [-0.18014093763854339, 0.09978477979110903, -0.321849168963094, 0.13247289727575332, 0.17003911452613193, 0.08189210886919393, 0.01780120613944874], \"y\": [-0.17119717829439887, -0.1795257898784969, 0.16898946537430345, 0.23342696745614203, 0.07428360511795495, 0.0023734858962214363, -0.12835055567172649], \"topics\": [1, 2, 3, 4, 5, 6, 7], \"cluster\": [1, 1, 1, 1, 1, 1, 1], \"Freq\": [15.290146827697754, 15.080709457397461, 14.386431694030762, 14.329256057739258, 14.19466781616211, 13.987687110900879, 12.731104850769043]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\"], \"Freq\": [3540.0, 3196.0, 2694.0, 5048.0, 3314.0, 3042.0, 6018.0, 2393.0, 11162.0, 5717.0, 2195.0, 2720.0, 3277.0, 1955.0, 1875.0, 3029.0, 3364.0, 1666.0, 1594.0, 1592.0, 1576.0, 1564.0, 3080.0, 2013.0, 1496.0, 2311.0, 1361.0, 3494.0, 1290.0, 2310.0, 3539.67041015625, 2694.02099609375, 1495.42041015625, 903.8555297851562, 839.9918823242188, 785.0762939453125, 620.8947143554688, 546.7075805664062, 528.11279296875, 493.4481506347656, 489.1583251953125, 458.6162414550781, 451.73956298828125, 426.4482727050781, 422.45660400390625, 410.56689453125, 410.5694580078125, 388.736572265625, 380.6378479003906, 356.4972229003906, 333.88006591796875, 321.443603515625, 292.8462829589844, 285.9378356933594, 284.8887023925781, 279.78326416015625, 269.008544921875, 260.43475341796875, 255.3558349609375, 240.8756866455078, 2772.305908203125, 1272.683349609375, 1714.97802734375, 3676.867431640625, 1815.190185546875, 809.578369140625, 1173.73779296875, 858.8887329101562, 2239.66748046875, 650.1389770507812, 906.17919921875, 457.5036926269531, 492.4984436035156, 1088.0106201171875, 1307.6363525390625, 740.4688110351562, 476.6992492675781, 533.7466430664062, 670.9745483398438, 610.5556640625, 518.061767578125, 519.6002807617188, 504.99005126953125, 1665.532470703125, 1229.0880126953125, 1082.281005859375, 1074.0989990234375, 1015.4403076171875, 914.6854858398438, 737.58642578125, 686.5709228515625, 638.0671997070312, 574.0059204101562, 539.660888671875, 518.1473999023438, 490.7773132324219, 481.21832275390625, 470.6062927246094, 450.9989929199219, 397.4818420410156, 390.6507263183594, 373.6315002441406, 367.1521911621094, 365.3785400390625, 361.79547119140625, 341.4090576171875, 329.776611328125, 315.1220397949219, 310.9982604980469, 303.61395263671875, 301.06414794921875, 291.97174072265625, 291.8377380371094, 1559.32470703125, 3975.186767578125, 3696.545166015625, 1620.6859130859375, 799.6705932617188, 541.9852294921875, 723.0465087890625, 421.0482177734375, 487.1610107421875, 508.291259765625, 964.81884765625, 644.2926025390625, 812.6904907226562, 616.9800415039062, 801.4583740234375, 583.8760986328125, 3195.1796875, 2392.3671875, 1593.9425048828125, 1591.704345703125, 1563.8634033203125, 1201.6932373046875, 995.7459716796875, 929.9013061523438, 835.3045043945312, 753.359375, 744.5181274414062, 699.2467041015625, 696.5404052734375, 667.6534423828125, 605.8011474609375, 588.0067138671875, 562.5330810546875, 514.3779296875, 483.059814453125, 461.39422607421875, 361.49261474609375, 359.7824401855469, 339.4848937988281, 332.512451171875, 329.58404541015625, 296.5438537597656, 290.48297119140625, 284.6278991699219, 274.0218200683594, 269.2881774902344, 2702.81884765625, 1308.2166748046875, 720.5941772460938, 846.569580078125, 605.2733764648438, 1192.405029296875, 460.54339599609375, 424.8929443359375, 396.64862060546875, 838.9552612304688, 1005.2855224609375, 602.898681640625, 550.8380737304688, 1575.2470703125, 1129.6348876953125, 1022.2289428710938, 951.3970336914062, 904.4171142578125, 884.3308715820312, 770.5432739257812, 759.1209716796875, 667.4699096679688, 587.345947265625, 536.5256958007812, 514.1605224609375, 450.6582946777344, 448.7886047363281, 346.27838134765625, 340.0978088378906, 303.2613220214844, 284.68084716796875, 283.9412536621094, 283.9618225097656, 275.9365234375, 261.4914855957031, 247.40455627441406, 245.88504028320312, 242.27870178222656, 232.48297119140625, 223.67333984375, 221.66854858398438, 212.13418579101562, 210.9170684814453, 1061.5557861328125, 2557.582763671875, 1123.41845703125, 2226.531982421875, 2219.1650390625, 590.93017578125, 5797.2353515625, 815.2083740234375, 441.9060974121094, 590.2388916015625, 513.7052001953125, 630.55419921875, 410.64801025390625, 917.5674438476562, 648.8817749023438, 484.0977783203125, 1874.5294189453125, 1360.6630859375, 859.3673706054688, 804.8280029296875, 709.7254638671875, 698.828369140625, 666.7875366210938, 599.8759765625, 585.2468872070312, 514.2987060546875, 508.816162109375, 457.1358642578125, 446.4847717285156, 434.595458984375, 424.8053894042969, 406.1580505371094, 394.4247131347656, 373.40740966796875, 368.09466552734375, 363.1564636230469, 360.5289611816406, 342.8033752441406, 342.3825988769531, 335.6510314941406, 329.9054260253906, 326.91229248046875, 316.621826171875, 311.4746398925781, 289.55291748046875, 285.94659423828125, 1042.3240966796875, 2027.6319580078125, 891.28125, 780.80126953125, 987.5137939453125, 1548.5880126953125, 682.3172607421875, 873.0728149414062, 474.7236328125, 947.210693359375, 576.3142700195312, 791.1165771484375, 535.3341674804688, 448.4930114746094, 493.725830078125, 616.9566650390625, 603.0724487304688, 566.7144775390625, 602.7279663085938, 510.3952331542969, 484.1419982910156, 2194.231201171875, 1954.6065673828125, 1290.1304931640625, 719.7750244140625, 646.06982421875, 536.8792724609375, 448.7312927246094, 439.9407958984375, 418.9465026855469, 418.2471008300781, 412.2213439941406, 411.017822265625, 399.5792236328125, 390.9099426269531, 373.22662353515625, 369.1535339355469, 365.54925537109375, 364.8208312988281, 355.4801330566406, 339.8508605957031, 338.2729187011719, 334.52191162109375, 331.4197082519531, 327.3932800292969, 299.30206298828125, 289.1339416503906, 285.62274169921875, 282.09332275390625, 268.7403564453125, 266.46533203125, 2116.693603515625, 671.2559204101562, 667.2222290039062, 1995.425048828125, 2233.958740234375, 461.8257751464844, 2274.0869140625, 1649.2506103515625, 805.696044921875, 452.48626708984375, 1084.0987548828125, 608.43310546875, 439.7894592285156, 536.7494506835938, 415.4953308105469, 453.2947692871094, 394.7223205566406, 1117.0692138671875, 1111.2437744140625, 845.523193359375, 836.65673828125, 801.4917602539062, 593.8106689453125, 557.1947021484375, 510.1091613769531, 496.27935791015625, 483.29730224609375, 442.0405578613281, 397.7620849609375, 388.7279357910156, 387.27215576171875, 381.355224609375, 363.75592041015625, 356.6109313964844, 349.5640869140625, 333.4554138183594, 332.6703796386719, 309.1138916015625, 299.8657531738281, 282.5914306640625, 271.6520690917969, 269.78656005859375, 266.58990478515625, 264.9443664550781, 255.0758056640625, 248.94473266601562, 237.66864013671875, 1084.6583251953125, 791.5908813476562, 457.211181640625, 1259.1248779296875, 1415.2706298828125, 390.3751220703125, 359.7384033203125, 443.2281494140625, 564.522705078125, 346.57501220703125, 420.4378356933594, 437.456298828125, 408.4197998046875, 376.5531921386719], \"Term\": [\"equipment\", \"love\", \"facility\", \"staff\", \"room\", \"machine\", \"class\", \"service\", \"gym\", \"place\", \"experience\", \"spa\", \"year\", \"everyone\", \"massage\", \"member\", \"membership\", \"price\", \"everything\", \"locker\", \"money\", \"customer\", \"trainer\", \"lot\", \"star\", \"weight\", \"fee\", \"month\", \"vega\", \"week\", \"equipment\", \"facility\", \"star\", \"space\", \"reason\", \"parking\", \"visit\", \"wall\", \"stuff\", \"community\", \"court\", \"amount\", \"ton\", \"quality\", \"lvac\", \"basketball\", \"groupon\", \"kind\", \"one\", \"tub\", \"state\", \"teacher\", \"vibe\", \"salt\", \"climbing\", \"bag\", \"parent\", \"ball\", \"swim\", \"lesson\", \"machine\", \"cardio\", \"lot\", \"staff\", \"weight\", \"studio\", \"pool\", \"part\", \"people\", \"floor\", \"use\", \"tv\", \"variety\", \"work\", \"gym\", \"area\", \"management\", \"life\", \"class\", \"room\", \"location\", \"time\", \"place\", \"price\", \"fun\", \"group\", \"session\", \"yoga\", \"shape\", \"town\", \"schedule\", \"environment\", \"program\", \"spin\", \"energy\", \"offer\", \"weekend\", \"ranch\", \"bike\", \"camp\", \"age\", \"student\", \"look\", \"evening\", \"zumba\", \"summer\", \"school\", \"selection\", \"half\", \"boxing\", \"wish\", \"technique\", \"front\", \"training\", \"class\", \"place\", \"week\", \"friend\", \"pay\", \"something\", \"strength\", \"check\", \"employee\", \"workout\", \"instructor\", \"time\", \"work\", \"gym\", \"fitness\", \"love\", \"service\", \"everything\", \"locker\", \"customer\", \"shower\", \"nothing\", \"sauna\", \"woman\", \"steam\", \"man\", \"girl\", \"treatment\", \"child\", \"guest\", \"lady\", \"recommend\", \"bathroom\", \"daughter\", \"canyon\", \"muscle\", \"mind\", \"train\", \"section\", \"policy\", \"situation\", \"jacuzzi\", \"notice\", \"lounge\", \"product\", \"room\", \"kid\", \"change\", \"desk\", \"care\", \"area\", \"need\", \"hair\", \"challenge\", \"place\", \"gym\", \"spa\", \"staff\", \"money\", \"anyone\", \"family\", \"business\", \"problem\", \"job\", \"deal\", \"home\", \"eos\", \"rate\", \"lifetime\", \"box\", \"company\", \"wife\", \"attitude\", \"practice\", \"heart\", \"joke\", \"cancel\", \"route\", \"value\", \"case\", \"pricing\", \"discount\", \"therapist\", \"sunday\", \"thought\", \"childcare\", \"maintenance\", \"advantage\", \"contract\", \"year\", \"manager\", \"member\", \"membership\", \"call\", \"gym\", \"person\", \"number\", \"treadmill\", \"phone\", \"crossfit\", \"side\", \"month\", \"fitness\", \"time\", \"massage\", \"fee\", \"u\", \"card\", \"charge\", \"atmosphere\", \"result\", \"other\", \"client\", \"husband\", \"try\", \"get\", \"cancellation\", \"line\", \"fan\", \"spot\", \"credit\", \"min\", \"party\", \"package\", \"trial\", \"feel\", \"chair\", \"payment\", \"bed\", \"information\", \"tanning\", \"paper\", \"refund\", \"signing\", \"review\", \"trainer\", \"sign\", \"help\", \"minute\", \"location\", \"towel\", \"instructor\", \"account\", \"hour\", \"owner\", \"way\", \"water\", \"question\", \"goal\", \"month\", \"membership\", \"time\", \"gym\", \"fitness\", \"staff\", \"experience\", \"everyone\", \"vega\", \"pas\", \"night\", \"door\", \"gold\", \"tour\", \"face\", \"hotel\", \"course\", \"access\", \"sweat\", \"join\", \"walk\", \"rest\", \"move\", \"idea\", \"world\", \"working\", \"goodlife\", \"hope\", \"process\", \"tip\", \"feeling\", \"cleanliness\", \"guess\", \"city\", \"saturday\", \"yelp\", \"spa\", \"morning\", \"couple\", \"day\", \"workout\", \"fact\", \"time\", \"month\", \"club\", \"plan\", \"gym\", \"hour\", \"body\", \"class\", \"someone\", \"work\", \"option\", \"coach\", \"anything\", \"music\", \"today\", \"thank\", \"planet\", \"complaint\", \"cost\", \"house\", \"salon\", \"rock\", \"want\", \"rack\", \"choice\", \"matter\", \"pressure\", \"word\", \"station\", \"notch\", \"injury\", \"answer\", \"bench\", \"condition\", \"game\", \"wave\", \"athlete\", \"opinion\", \"leg\", \"scottsdale\", \"ability\", \"guy\", \"level\", \"hand\", \"thing\", \"fitness\", \"appointment\", \"health\", \"exercise\", \"gym\", \"amenity\", \"trainer\", \"workout\", \"time\", \"class\"], \"Total\": [3540.0, 3196.0, 2694.0, 5048.0, 3314.0, 3042.0, 6018.0, 2393.0, 11162.0, 5717.0, 2195.0, 2720.0, 3277.0, 1955.0, 1875.0, 3029.0, 3364.0, 1666.0, 1594.0, 1592.0, 1576.0, 1564.0, 3080.0, 2013.0, 1496.0, 2311.0, 1361.0, 3494.0, 1290.0, 2310.0, 3540.521484375, 2694.87255859375, 1496.271240234375, 904.7064819335938, 840.843017578125, 785.9267578125, 621.7462768554688, 547.5589599609375, 528.9640502929688, 494.2989807128906, 490.0086975097656, 459.46759033203125, 452.5904541015625, 427.2995910644531, 423.3074645996094, 411.4173583984375, 411.420654296875, 389.5880126953125, 381.48846435546875, 357.3486328125, 334.7312927246094, 322.29425048828125, 293.6968078613281, 286.789794921875, 285.73992919921875, 280.6344299316406, 269.85986328125, 261.28558349609375, 256.2065124511719, 241.72628784179688, 3042.6513671875, 1446.370849609375, 2013.6458740234375, 5048.6259765625, 2311.6201171875, 956.6875, 1525.4454345703125, 1063.29931640625, 3736.509765625, 811.6304931640625, 1293.7196044921875, 526.6906127929688, 618.8323974609375, 3174.609375, 11162.9638671875, 2511.015625, 625.0209350585938, 1023.5505981445312, 6018.62646484375, 3314.09375, 3271.017333984375, 5482.85546875, 5717.640625, 1666.3760986328125, 1229.9307861328125, 1083.12451171875, 1074.9420166015625, 1016.283447265625, 915.5285034179688, 738.4295043945312, 687.4136962890625, 638.9102172851562, 574.8490600585938, 540.5037231445312, 518.990234375, 491.62066650390625, 482.0615539550781, 471.4549560546875, 451.84246826171875, 398.3244323730469, 391.49383544921875, 374.47467041015625, 367.9955749511719, 366.2218017578125, 362.63812255859375, 342.2520751953125, 330.6198425292969, 315.9654846191406, 311.8417053222656, 304.4571533203125, 301.9071960449219, 292.81494140625, 292.6815490722656, 1930.37109375, 6018.62646484375, 5717.640625, 2310.0673828125, 1286.9769287109375, 726.7745971679688, 1164.1951904296875, 507.69091796875, 704.5177001953125, 845.484375, 4596.8046875, 1732.6666259765625, 5482.85546875, 3174.609375, 11162.9638671875, 3847.9931640625, 3196.034423828125, 2393.22216796875, 1594.7974853515625, 1592.55859375, 1564.7181396484375, 1202.5477294921875, 996.6008911132812, 930.755859375, 836.1589965820312, 754.213623046875, 745.3727416992188, 700.1016845703125, 697.395751953125, 668.5081787109375, 606.65673828125, 588.8614501953125, 563.38720703125, 515.232421875, 483.91455078125, 462.2537841796875, 362.3476867675781, 360.6373596191406, 340.3398132324219, 333.36737060546875, 330.43914794921875, 297.39892578125, 291.3373718261719, 285.4832458496094, 274.8765563964844, 270.14288330078125, 3314.09375, 1505.885009765625, 833.7163696289062, 1161.0308837890625, 816.906494140625, 2511.015625, 616.867919921875, 548.8223266601562, 487.094970703125, 5717.640625, 11162.9638671875, 2720.30859375, 5048.6259765625, 1576.0933837890625, 1130.4813232421875, 1023.0750732421875, 952.2432250976562, 905.2634887695312, 885.1771240234375, 771.3893432617188, 759.9672241210938, 668.3162231445312, 588.1917724609375, 537.3721313476562, 515.0068359375, 451.5042724609375, 449.63482666015625, 347.12432861328125, 340.94403076171875, 304.107177734375, 285.526611328125, 284.78704833984375, 284.8082580566406, 276.7824401855469, 262.3374938964844, 248.25042724609375, 246.73129272460938, 243.12583923339844, 233.3295135498047, 224.51937866210938, 222.51454162597656, 212.98025512695312, 211.763427734375, 1218.8277587890625, 3277.310546875, 1352.2572021484375, 3029.053466796875, 3364.871337890625, 709.8965454101562, 11162.9638671875, 1108.4873046875, 524.6102294921875, 787.1449584960938, 693.7728881835938, 975.6277465820312, 592.8051147460938, 3494.039306640625, 3847.9931640625, 5482.85546875, 1875.3779296875, 1361.5108642578125, 860.2155151367188, 805.6754150390625, 710.5730590820312, 699.6760864257812, 667.6351318359375, 600.7238159179688, 586.0947875976562, 515.1465454101562, 509.66357421875, 457.9835205078125, 447.3323059082031, 435.4434509277344, 425.6531066894531, 407.0059509277344, 395.27203369140625, 374.2548522949219, 368.9424743652344, 364.0042419433594, 361.3763732910156, 343.6510009765625, 343.2310485839844, 336.4983215332031, 330.7528076171875, 327.7598876953125, 317.4693908691406, 312.322265625, 290.4003601074219, 286.7938537597656, 1346.3369140625, 3080.680908203125, 1135.6878662109375, 1016.6267700195312, 1582.94189453125, 3271.017333984375, 1103.7430419921875, 1732.6666259765625, 610.5950927734375, 2433.884033203125, 977.5451049804688, 2046.840087890625, 876.474853515625, 614.5689697265625, 868.0250244140625, 3494.039306640625, 3364.871337890625, 5482.85546875, 11162.9638671875, 3847.9931640625, 5048.6259765625, 2195.0791015625, 1955.454345703125, 1290.978271484375, 720.62255859375, 646.9175415039062, 537.7271728515625, 449.5788879394531, 440.78863525390625, 419.7945861816406, 419.0948791503906, 413.0696716308594, 411.8660888671875, 400.4267272949219, 391.7575378417969, 374.0744323730469, 370.00146484375, 366.39697265625, 365.6685485839844, 356.3280944824219, 340.6982421875, 339.12017822265625, 335.3694152832031, 332.2677307128906, 328.2416076660156, 300.14923095703125, 289.98187255859375, 286.4703369140625, 282.94097900390625, 269.5874938964844, 267.3127746582031, 2720.30859375, 798.4047241210938, 809.1068725585938, 3598.178466796875, 4596.8046875, 554.0640869140625, 5482.85546875, 3494.039306640625, 1312.1014404296875, 594.3914184570312, 11162.9638671875, 2433.884033203125, 1155.113037109375, 6018.62646484375, 1090.180908203125, 3174.609375, 808.5508422851562, 1117.900634765625, 1112.0755615234375, 846.3546752929688, 837.488525390625, 802.3228759765625, 594.6419677734375, 558.026123046875, 510.94085693359375, 497.1110534667969, 484.1295166015625, 442.87261962890625, 398.59368896484375, 389.5591735839844, 388.1037292480469, 382.1866760253906, 364.5874938964844, 357.442626953125, 350.39593505859375, 334.2868347167969, 333.5015563964844, 309.9452819824219, 300.6973876953125, 283.4228210449219, 272.48382568359375, 270.6217041015625, 267.4210510253906, 265.77569580078125, 255.90713500976562, 249.77618408203125, 238.5001983642578, 1394.0810546875, 978.7053833007812, 536.3045654296875, 2555.98095703125, 3847.9931640625, 512.2242431640625, 472.63623046875, 889.036865234375, 11162.9638671875, 676.7672119140625, 3080.680908203125, 4596.8046875, 5482.85546875, 6018.62646484375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.8776999711990356, 1.8775999546051025, 1.8774000406265259, 1.8769999742507935, 1.8768999576568604, 1.8768999576568604, 1.8766000270843506, 1.8763999938964844, 1.8763999938964844, 1.8761999607086182, 1.8761999607086182, 1.876099944114685, 1.876099944114685, 1.8760000467300415, 1.8759000301361084, 1.8759000301361084, 1.8759000301361084, 1.8758000135421753, 1.8756999969482422, 1.875599980354309, 1.8753999471664429, 1.8753000497817993, 1.875100016593933, 1.875, 1.875, 1.874899983406067, 1.8747999668121338, 1.8746999502182007, 1.8746000528335571, 1.874400019645691, 1.7848999500274658, 1.75, 1.7173999547958374, 1.5608999729156494, 1.636199951171875, 1.7109999656677246, 1.6159000396728516, 1.6644999980926514, 1.3660999536514282, 1.6561000347137451, 1.5219000577926636, 1.7371000051498413, 1.6496000289916992, 0.8070999979972839, -0.266400009393692, 0.6567999720573425, 1.607100009918213, 1.2267999649047852, -0.3158999979496002, 0.18639999628067017, 0.03519999980926514, -0.47839999198913574, -0.548799991607666, 1.8911999464035034, 1.8911000490188599, 1.8910000324249268, 1.8910000324249268, 1.8909000158309937, 1.8907999992370605, 1.8905999660491943, 1.8904999494552612, 1.8904000520706177, 1.8903000354766846, 1.8902000188827515, 1.8901000022888184, 1.8899999856948853, 1.8899999856948853, 1.8899999856948853, 1.8898999691009521, 1.8896000385284424, 1.8896000385284424, 1.8895000219345093, 1.8895000219345093, 1.8894000053405762, 1.8894000053405762, 1.889299988746643, 1.88919997215271, 1.8890999555587769, 1.8890000581741333, 1.8890000581741333, 1.8890000581741333, 1.8889000415802002, 1.8889000415802002, 1.6783000230789185, 1.4769999980926514, 1.4556000232696533, 1.5372999906539917, 1.4158999919891357, 1.5983999967575073, 1.4154000282287598, 1.7045999765396118, 1.5227999687194824, 1.3828999996185303, 0.33059999346733093, 0.9024999737739563, -0.01730000041425228, 0.25369998812675476, -0.7422000169754028, 0.006099999882280827, 1.938599944114685, 1.9385000467300415, 1.9383000135421753, 1.9383000135421753, 1.9383000135421753, 1.9381999969482422, 1.937999963760376, 1.937999963760376, 1.9378999471664429, 1.9378000497817993, 1.9377000331878662, 1.9377000331878662, 1.9377000331878662, 1.937600016593933, 1.9375, 1.937399983406067, 1.937399983406067, 1.9371999502182007, 1.9371000528335571, 1.937000036239624, 1.9364999532699585, 1.9364999532699585, 1.936400055885315, 1.9363000392913818, 1.9363000392913818, 1.9359999895095825, 1.9358999729156494, 1.9358999729156494, 1.9357999563217163, 1.9357000589370728, 1.7350000143051147, 1.798200011253357, 1.7930999994277954, 1.6230000257492065, 1.6390000581741333, 1.194200038909912, 1.6466000080108643, 1.6828999519348145, 1.7335000038146973, 0.019700000062584877, -0.4684000015258789, 0.43209999799728394, -0.27649998664855957, 1.9422999620437622, 1.9421000480651855, 1.9420000314712524, 1.9420000314712524, 1.9419000148773193, 1.9419000148773193, 1.9417999982833862, 1.9417999982833862, 1.94159996509552, 1.9414000511169434, 1.9413000345230103, 1.9412000179290771, 1.940999984741211, 1.940999984741211, 1.9404000043869019, 1.9404000043869019, 1.9400999546051025, 1.9399000406265259, 1.9399000406265259, 1.9399000406265259, 1.9398000240325928, 1.9395999908447266, 1.9394999742507935, 1.9393999576568604, 1.9393999576568604, 1.9392000436782837, 1.9391000270843506, 1.9391000270843506, 1.9388999938964844, 1.9388999938964844, 1.8047000169754028, 1.6949000358581543, 1.7575000524520874, 1.63510000705719, 1.5266000032424927, 1.7594000101089478, 1.287600040435791, 1.635599970817566, 1.771299958229065, 1.6549999713897705, 1.6424000263214111, 1.5063999891281128, 1.575700044631958, 0.6057999730110168, 0.16279999911785126, -0.48420000076293945, 1.9519000053405762, 1.95169997215271, 1.951300024986267, 1.951300024986267, 1.9510999917984009, 1.9510999917984009, 1.9509999752044678, 1.9508999586105347, 1.9508999586105347, 1.950700044631958, 1.950600028038025, 1.9505000114440918, 1.9503999948501587, 1.9503999948501587, 1.9502999782562256, 1.9501999616622925, 1.9501999616622925, 1.9500000476837158, 1.9500000476837158, 1.9500000476837158, 1.9500000476837158, 1.9498000144958496, 1.9498000144958496, 1.9498000144958496, 1.9496999979019165, 1.9496999979019165, 1.9495999813079834, 1.9495999813079834, 1.9493999481201172, 1.9493000507354736, 1.6964000463485718, 1.534000039100647, 1.7100000381469727, 1.6884000301361084, 1.4804999828338623, 1.2044999599456787, 1.4713000059127808, 1.2668999433517456, 1.700600028038025, 1.0085999965667725, 1.4239000082015991, 1.0017000436782837, 1.4593000411987305, 1.6373000144958496, 1.388100028038025, 0.218299999833107, 0.23319999873638153, -0.3172000050544739, -0.9666000008583069, -0.06780000030994415, -0.3921999931335449, 1.966599941253662, 1.966599941253662, 1.9663000106811523, 1.9658000469207764, 1.9657000303268433, 1.965399980545044, 1.9651000499725342, 1.9651000499725342, 1.965000033378601, 1.965000033378601, 1.964900016784668, 1.964900016784668, 1.964900016784668, 1.9648000001907349, 1.9646999835968018, 1.9646999835968018, 1.9646999835968018, 1.9646999835968018, 1.9645999670028687, 1.9644999504089355, 1.9644999504089355, 1.9644999504089355, 1.964400053024292, 1.964400053024292, 1.9642000198364258, 1.9641000032424927, 1.9639999866485596, 1.9639999866485596, 1.9637999534606934, 1.9637999534606934, 1.7160999774932861, 1.7934999465942383, 1.7741999626159668, 1.3774000406265259, 1.2453999519348145, 1.7848999500274658, 1.086899995803833, 1.2163000106811523, 1.4793000221252441, 1.694200038909912, -0.36489999294281006, 0.5806000232696533, 1.0012999773025513, -0.45010000467300415, 1.0024000406265259, 0.020600000396370888, 1.249899983406067, 2.0604000091552734, 2.0604000091552734, 2.0601000785827637, 2.0601000785827637, 2.0601000785827637, 2.0597000122070312, 2.0596001148223877, 2.059499979019165, 2.0594000816345215, 2.0594000816345215, 2.0592000484466553, 2.059000015258789, 2.059000015258789, 2.059000015258789, 2.0589001178741455, 2.058799982070923, 2.058799982070923, 2.0587000846862793, 2.0585999488830566, 2.0585999488830566, 2.0583999156951904, 2.0583999156951904, 2.058199882507324, 2.0580999851226807, 2.058000087738037, 2.058000087738037, 2.058000087738037, 2.0578999519348145, 2.057800054550171, 2.0576000213623047, 1.8101999759674072, 1.8488999605178833, 1.9016000032424927, 1.3530999422073364, 1.0608999729156494, 1.7894999980926514, 1.7882000207901, 1.3651000261306763, -0.92330002784729, 1.3918999433517456, 0.06949999928474426, -0.29100000858306885, -0.5360000133514404, -0.7103999853134155], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.0560998916625977, -3.3290998935699463, -3.9177000522613525, -4.421199798583984, -4.494500160217285, -4.562099933624268, -4.7967000007629395, -4.923999786376953, -4.958600044250488, -5.026500225067139, -5.035200119018555, -5.099699974060059, -5.114799976348877, -5.172399997711182, -5.18179988861084, -5.210299968719482, -5.210299968719482, -5.264999866485596, -5.285999774932861, -5.351500034332275, -5.417099952697754, -5.455100059509277, -5.5482001304626465, -5.5721001625061035, -5.575799942016602, -5.593900203704834, -5.6331000328063965, -5.665500164031982, -5.685200214385986, -5.743599891662598, -3.3004000186920166, -4.078999996185303, -3.7806999683380127, -3.0181000232696533, -3.723900079727173, -4.531400203704834, -4.159900188446045, -4.4721999168396, -3.5137999057769775, -4.750699996948242, -4.418600082397461, -5.102099895477295, -5.02839994430542, -4.235799789428711, -4.0518999099731445, -4.62060022354126, -5.060999870300293, -4.947999954223633, -4.719099998474121, -4.813499927520752, -4.977799892425537, -4.974800109863281, -5.003300189971924, -3.7962000370025635, -4.100100040435791, -4.22730016708374, -4.234799861907959, -4.290999889373779, -4.395500183105469, -4.6107001304626465, -4.682400226593018, -4.7555999755859375, -4.861400127410889, -4.923099994659424, -4.963799953460693, -5.018099784851074, -5.037799835205078, -5.060100078582764, -5.10260009765625, -5.228899955749512, -5.246300220489502, -5.290800094604492, -5.308300018310547, -5.3130998611450195, -5.322999954223633, -5.38100004196167, -5.4156999588012695, -5.461100101470947, -5.474299907684326, -5.498300075531006, -5.506800174713135, -5.537399768829346, -5.537899971008301, -3.8620998859405518, -2.926300048828125, -2.9988999366760254, -3.823499917984009, -4.529900074005127, -4.918799877166748, -4.6305999755859375, -5.171299934387207, -5.0254998207092285, -4.982999801635742, -4.342100143432617, -4.7459001541137695, -4.513700008392334, -4.7891998291015625, -4.527599811553955, -4.844399929046631, -3.097599983215332, -3.386899948120117, -3.7929999828338623, -3.7943999767303467, -3.812000036239624, -4.075500011444092, -4.263500213623047, -4.331900119781494, -4.439199924468994, -4.542399883270264, -4.554200172424316, -4.6168999671936035, -4.620800018310547, -4.6631999015808105, -4.76039981842041, -4.790200233459473, -4.834499835968018, -4.923999786376953, -4.986800193786621, -5.032700061798096, -5.276700019836426, -5.281499862670898, -5.3394999504089355, -5.360300064086914, -5.369100093841553, -5.474800109863281, -5.4953999519348145, -5.5157999992370605, -5.553699970245361, -5.571199893951416, -3.264899969100952, -3.990499973297119, -4.586900234222412, -4.42579984664917, -4.761300086975098, -4.083199977874756, -5.0345001220703125, -5.115099906921387, -5.183899879455566, -4.434800148010254, -4.253900051116943, -4.765200138092041, -4.855500221252441, -3.800800085067749, -4.133299827575684, -4.2332000732421875, -4.304999828338623, -4.3557000160217285, -4.3780999183654785, -4.515900135040283, -4.530799865722656, -4.6595001220703125, -4.787399768829346, -4.877900123596191, -4.920400142669678, -5.052299976348877, -5.056399822235107, -5.315700054168701, -5.333700180053711, -5.448400020599365, -5.511600017547607, -5.514200210571289, -5.514100074768066, -5.542799949645996, -5.59660005569458, -5.651899814605713, -5.658100128173828, -5.672900199890137, -5.714200019836426, -5.752799987792969, -5.7617998123168945, -5.805799961090088, -5.811500072479248, -4.195499897003174, -3.316200017929077, -4.138800144195557, -3.4547998905181885, -3.4581000804901123, -4.781300067901611, -2.49780011177063, -4.459499835968018, -5.071899890899658, -4.782400131225586, -4.921299934387207, -4.716400146484375, -5.145199775695801, -4.34119987487793, -4.687699794769287, -4.9807000160217285, -3.6173999309539795, -3.9377999305725098, -4.397299766540527, -4.462900161743164, -4.588699817657471, -4.604100227355957, -4.651100158691406, -4.756800174713135, -4.781499862670898, -4.910699844360352, -4.92140007019043, -5.028600215911865, -5.05210018157959, -5.079100131988525, -5.101900100708008, -5.1468000411987305, -5.17609977722168, -5.230899810791016, -5.245200157165527, -5.258699893951416, -5.265999794006348, -5.316400051116943, -5.317599773406982, -5.337500095367432, -5.354700088500977, -5.363800048828125, -5.3958001136779785, -5.412199974060059, -5.485199928283691, -5.497700214385986, -4.2042999267578125, -3.5388998985290527, -4.360899925231934, -4.493199825286865, -4.258299827575684, -3.8083999156951904, -4.627999782562256, -4.381499767303467, -4.990799903869629, -4.300000190734863, -4.796899795532227, -4.480100154876709, -4.87060022354126, -5.047599792480469, -4.951600074768066, -4.728700160980225, -4.751500129699707, -4.813700199127197, -4.752099990844727, -4.918300151824951, -4.971199989318848, -3.4453001022338867, -3.5608999729156494, -3.976300001144409, -4.559899806976318, -4.667900085449219, -4.853099822998047, -5.032400131225586, -5.052199840545654, -5.101099967956543, -5.102799892425537, -5.117300033569336, -5.120200157165527, -5.148399829864502, -5.170400142669678, -5.216700077056885, -5.22760009765625, -5.237400054931641, -5.2393999099731445, -5.265399932861328, -5.310299873352051, -5.315000057220459, -5.326099872589111, -5.3354997634887695, -5.347700119018555, -5.437399864196777, -5.4720001220703125, -5.4842000007629395, -5.496600151062012, -5.545100212097168, -5.553599834442139, -3.4811999797821045, -4.629700183868408, -4.635700225830078, -3.5401999950408936, -3.427299976348877, -5.003699779510498, -3.4094998836517334, -3.730799913406372, -4.4471001625061035, -5.024099826812744, -4.150300025939941, -4.728000164031982, -5.052499771118164, -4.853300094604492, -5.109399795532227, -5.022299766540527, -5.160699844360352, -4.026299953460693, -4.031499862670898, -4.304800033569336, -4.315299987792969, -4.3582000732421875, -4.658199787139893, -4.721799850463867, -4.810100078582764, -4.837600231170654, -4.864099979400635, -4.9532999992370605, -5.058899879455566, -5.0817999839782715, -5.085599899291992, -5.10099983215332, -5.148200035095215, -5.168099880218506, -5.188000202178955, -5.235199928283691, -5.237599849700928, -5.310999870300293, -5.341400146484375, -5.400700092315674, -5.440199851989746, -5.4471001625061035, -5.459000110626221, -5.465199947357178, -5.503200054168701, -5.527500152587891, -5.573800086975098, -4.055699825286865, -4.370699882507324, -4.919600009918213, -3.9065001010894775, -3.789599895477295, -5.077600002288818, -5.159299850463867, -4.9506001472473145, -4.708700180053711, -5.196599960327148, -5.003399848937988, -4.963699817657471, -5.032400131225586, -5.113699913024902]}, \"token.table\": {\"Topic\": [7, 6, 4, 5, 4, 2, 1, 3, 7, 1, 7, 4, 7, 5, 7, 1, 3, 4, 6, 7, 7, 5, 4, 1, 1, 1, 3, 5, 7, 2, 1, 2, 3, 6, 7, 4, 2, 4, 4, 5, 2, 4, 5, 3, 5, 1, 2, 3, 4, 7, 4, 5, 3, 7, 3, 6, 5, 2, 5, 6, 3, 4, 7, 6, 1, 2, 5, 6, 7, 6, 5, 1, 3, 4, 6, 7, 1, 4, 7, 7, 4, 5, 7, 5, 6, 6, 1, 5, 2, 4, 7, 3, 3, 2, 3, 4, 5, 6, 7, 4, 3, 4, 5, 4, 6, 2, 3, 4, 2, 2, 4, 1, 2, 6, 3, 1, 2, 7, 6, 6, 1, 6, 7, 4, 5, 5, 5, 6, 1, 2, 4, 5, 6, 7, 1, 3, 2, 4, 5, 6, 2, 2, 7, 5, 3, 2, 5, 7, 6, 6, 2, 1, 6, 3, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 3, 7, 2, 5, 7, 6, 7, 4, 5, 7, 4, 6, 6, 1, 2, 4, 5, 6, 7, 7, 5, 6, 5, 7, 2, 5, 7, 3, 4, 6, 4, 1, 3, 1, 3, 7, 1, 2, 7, 1, 2, 6, 7, 4, 5, 1, 2, 4, 5, 6, 7, 3, 2, 1, 2, 3, 3, 1, 1, 7, 4, 3, 1, 4, 4, 5, 5, 7, 4, 5, 6, 7, 2, 4, 5, 6, 5, 3, 2, 5, 6, 7, 4, 2, 4, 5, 6, 2, 6, 6, 3, 7, 2, 3, 5, 6, 7, 3, 3, 4, 5, 2, 1, 7, 1, 2, 5, 6, 5, 4, 5, 7, 5, 5, 1, 1, 1, 4, 5, 6, 2, 5, 5, 1, 2, 4, 5, 6, 7, 4, 5, 7, 4, 5, 6, 1, 2, 3, 6, 7, 2, 6, 7, 3, 1, 3, 4, 7, 2, 4, 4, 6, 3, 2, 1, 5, 7, 7, 2, 4, 1, 3, 5, 6, 5, 5, 6, 7, 1, 3, 4, 7, 1, 6, 3, 2, 2, 7, 3, 2, 3, 2, 2, 3, 1, 4, 4, 5, 5, 3, 2, 3, 4, 5, 6, 7, 2, 4, 6, 7, 3, 6, 1, 2, 5, 1, 3, 5, 7, 1, 1, 7, 3, 2, 7, 2, 1, 6, 1, 2, 4, 6, 1, 5, 1, 2, 7, 4, 1, 2, 3, 4, 6, 7, 4, 1, 2, 3, 4, 5, 6, 7, 6, 7, 1, 6, 3, 5, 2, 3, 2, 4, 5, 7, 2, 4, 7, 2, 4, 7, 3, 5, 5, 1, 1, 7, 5, 1, 3, 5, 6, 4, 1, 2, 6, 1, 1, 6, 1, 7, 1, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 2, 5, 6, 2, 1, 2, 7, 4, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 6, 1, 2, 3, 5, 6, 7, 6, 2, 4, 6, 7, 6, 2, 2], \"Freq\": [0.9979027509689331, 0.9978971481323242, 0.22109578549861908, 0.7779296040534973, 0.99639493227005, 0.9987385869026184, 0.24823898077011108, 0.23937329649925232, 0.512731671333313, 0.998982310295105, 0.9969501495361328, 0.9995742440223694, 0.9990328550338745, 0.23622466623783112, 0.7613852620124817, 0.2947014570236206, 0.47470831871032715, 0.07327712327241898, 0.09358762949705124, 0.0637192353606224, 0.9984254837036133, 0.9990336894989014, 0.9967610239982605, 0.997739315032959, 0.995079755783081, 0.9989855885505676, 0.9976080060005188, 0.9977239370346069, 0.9976807832717896, 0.998135507106781, 0.1142745316028595, 0.19218897819519043, 0.15409746766090393, 0.380915105342865, 0.15842604637145996, 0.998045027256012, 0.9984984397888184, 0.9986944198608398, 0.8325156569480896, 0.16622141003608704, 0.9966750144958496, 0.9972363710403442, 0.9970216751098633, 0.9972876906394958, 0.9991616606712341, 0.880133867263794, 0.11960971355438232, 0.7405988574028015, 0.19463671743869781, 0.06365477293729782, 0.9949016571044922, 0.9964133501052856, 0.8150361180305481, 0.1847689002752304, 0.8648024797439575, 0.13433825969696045, 0.999193549156189, 0.6912530660629272, 0.16890987753868103, 0.1391022503376007, 0.9992398023605347, 0.997687578201294, 0.9971560835838318, 0.9966742992401123, 0.11148723214864731, 0.6604496836662292, 0.07626324892044067, 0.08922301232814789, 0.06263887882232666, 0.9966140389442444, 0.998132050037384, 0.9974104762077332, 0.17910200357437134, 0.20577676594257355, 0.6142817735671997, 0.999194324016571, 0.9973720908164978, 0.9988831281661987, 0.9981611371040344, 0.9985081553459167, 0.8713290095329285, 0.1288122981786728, 0.9981585741043091, 0.17426621913909912, 0.8243657350540161, 0.9974104166030884, 0.9979414939880371, 0.9967818856239319, 0.2019212692975998, 0.6467630863189697, 0.15169720351696014, 0.9995410442352295, 0.9981101155281067, 0.11728156358003616, 0.059752456843853, 0.07476004958152771, 0.12895414233207703, 0.5544472336769104, 0.06447707116603851, 0.999495267868042, 0.7295240759849548, 0.12058249115943909, 0.14986681938171387, 0.9970360994338989, 0.9986476898193359, 0.6008390188217163, 0.2057991921901703, 0.19278889894485474, 0.998091995716095, 0.9985753893852234, 0.9980305433273315, 0.99985271692276, 0.9966637492179871, 0.9997676610946655, 0.9994999170303345, 0.2114647924900055, 0.28907686471939087, 0.49829205870628357, 0.9995083808898926, 0.9981071949005127, 0.9996762275695801, 0.8338385820388794, 0.16604577004909515, 0.9989491701126099, 0.9984656572341919, 0.9996247887611389, 0.9981056451797485, 0.9961711168289185, 0.08004172146320343, 0.15176741778850555, 0.16865934431552887, 0.1325366199016571, 0.09901265054941177, 0.36772415041923523, 0.8008570671081543, 0.19836613535881042, 0.6216117739677429, 0.15229488909244537, 0.11189011484384537, 0.11499817669391632, 0.997671365737915, 0.9992431998252869, 0.9982243776321411, 0.9978525042533875, 0.9984263777732849, 0.16819791495800018, 0.5691080093383789, 0.2626652419567108, 0.9987123608589172, 0.9966968297958374, 0.9989618062973022, 0.998977541923523, 0.9983581900596619, 0.9989174604415894, 0.08823016285896301, 0.13270390033721924, 0.7782904505729675, 0.11717317998409271, 0.07175513356924057, 0.09002985060214996, 0.5193065404891968, 0.05401791259646416, 0.09710682928562164, 0.05061379820108414, 0.774385392665863, 0.224116250872612, 0.9973008632659912, 0.1454397439956665, 0.852127730846405, 0.23696871101856232, 0.7616851329803467, 0.9963592290878296, 0.7682268619537354, 0.2311566174030304, 0.9987272620201111, 0.9988985061645508, 0.9973875284194946, 0.11380986124277115, 0.1376400887966156, 0.043140921741724014, 0.38909003138542175, 0.2498064786195755, 0.0665602758526802, 0.9977650046348572, 0.9977743029594421, 0.9981716871261597, 0.9976815581321716, 0.9984961152076721, 0.37168142199516296, 0.5038476586341858, 0.12408618628978729, 0.9954095482826233, 0.9986701607704163, 0.9980663061141968, 0.9981556534767151, 0.13082008063793182, 0.8685922026634216, 0.9984906911849976, 0.9985370635986328, 0.996455192565918, 0.9969953894615173, 0.1900469809770584, 0.8092322945594788, 0.5217133164405823, 0.20614515244960785, 0.14654868841171265, 0.12505488097667694, 0.9993075132369995, 0.9989815950393677, 0.15836051106452942, 0.060531627386808395, 0.11800610274076462, 0.47355297207832336, 0.1115860790014267, 0.0779573991894722, 0.9996492266654968, 0.9972946047782898, 0.8516889810562134, 0.14799027144908905, 0.9968110918998718, 0.9996763467788696, 0.9969112873077393, 0.9110475182533264, 0.08873839676380157, 0.9953974485397339, 0.9994999170303345, 0.7631744146347046, 0.23679207265377045, 0.8304632902145386, 0.1686069816350937, 0.9997984766960144, 0.9968950152397156, 0.7352131605148315, 0.10564356297254562, 0.08088335394859314, 0.07824226468801498, 0.08885926753282547, 0.6594606041908264, 0.17920447885990143, 0.07221672683954239, 0.99664705991745, 0.9982326626777649, 0.09349679946899414, 0.6241543292999268, 0.14845775067806244, 0.13329611718654633, 0.9993062615394592, 0.08872252702713013, 0.2627331614494324, 0.17658644914627075, 0.4719465970993042, 0.15781469643115997, 0.8404259085655212, 0.9989165663719177, 0.9962806701660156, 0.9995809197425842, 0.13292959332466125, 0.7473236918449402, 0.11833975464105606, 0.9985816478729248, 0.9961504936218262, 0.9993970394134521, 0.9983072876930237, 0.8425302505493164, 0.15630652010440826, 0.9987375140190125, 0.9987195730209351, 0.9970813989639282, 0.2609606981277466, 0.13604587316513062, 0.1137838140130043, 0.4885283410549164, 0.9987950921058655, 0.22403058409690857, 0.5892311334609985, 0.18515770137310028, 0.9972411394119263, 0.9957663416862488, 0.9968136548995972, 0.9988207817077637, 0.8078628182411194, 0.19185566902160645, 0.9974454641342163, 0.9991360902786255, 0.7457607984542847, 0.2531734108924866, 0.9985191226005554, 0.5994899272918701, 0.09741711616516113, 0.08350038528442383, 0.08376801013946533, 0.06209002807736397, 0.07386572659015656, 0.7352362275123596, 0.1587749421596527, 0.10464711487293243, 0.7408764362335205, 0.175850048661232, 0.08215945214033127, 0.08832314610481262, 0.6465953588485718, 0.14673884212970734, 0.06506180018186569, 0.05334367975592613, 0.23721742630004883, 0.7604416608810425, 0.9989204406738281, 0.9986709952354431, 0.7696112990379333, 0.2300967276096344, 0.997231125831604, 0.9983885884284973, 0.9997742772102356, 0.9949630498886108, 0.9986042976379395, 0.9961845874786377, 0.9957693219184875, 0.9985229969024658, 0.9969586133956909, 0.7289661765098572, 0.2684808373451233, 0.998564600944519, 0.9990350008010864, 0.9979738593101501, 0.9989973902702332, 0.999312698841095, 0.9986213445663452, 0.9972933530807495, 0.9990487098693848, 0.7739518880844116, 0.22505511343479156, 0.9980296492576599, 0.18436412513256073, 0.8156076073646545, 0.9971621036529541, 0.9976668953895569, 0.9972460865974426, 0.9978207945823669, 0.9991878867149353, 0.9993981719017029, 0.9981251955032349, 0.9968924522399902, 0.9988980293273926, 0.9969443082809448, 0.9994893074035645, 0.9991236329078674, 0.9994227290153503, 0.9995445013046265, 0.30532801151275635, 0.6933138370513916, 0.21484775841236115, 0.7845465540885925, 0.9972319602966309, 0.9986585974693298, 0.09264517575502396, 0.09539701044559479, 0.11374258995056152, 0.18253850936889648, 0.3806707561016083, 0.1348400115966797, 0.6210298538208008, 0.10135757178068161, 0.2087278813123703, 0.06871700286865234, 0.22166602313518524, 0.778220534324646, 0.9992191195487976, 0.999068021774292, 0.9975284337997437, 0.7283169627189636, 0.1091386079788208, 0.09586767107248306, 0.06655275821685791, 0.9991503953933716, 0.9978153109550476, 0.9988700151443481, 0.9983908534049988, 0.8292446732521057, 0.16939440369606018, 0.9987324476242065, 0.846671462059021, 0.1526099145412445, 0.9981774687767029, 0.9963416457176208, 0.994301974773407, 0.9989343285560608, 0.9952908754348755, 0.9985214471817017, 0.9959842562675476, 0.9972168803215027, 0.9983512163162231, 0.9953693151473999, 0.14123736321926117, 0.08646386861801147, 0.11463309079408646, 0.07629165053367615, 0.08842006325721741, 0.4925701916217804, 0.9976866841316223, 0.09484109282493591, 0.14828039705753326, 0.07605525851249695, 0.08827517181634903, 0.10341326892375946, 0.4147473871707916, 0.07441377639770508, 0.9962173700332642, 0.9994166493415833, 0.9986953735351562, 0.9982108473777771, 0.3814293444156647, 0.6178974509239197, 0.9994183778762817, 0.9960632920265198, 0.14542239904403687, 0.059727054089307785, 0.6582960486412048, 0.13633349537849426, 0.8076167106628418, 0.0844397246837616, 0.10723327100276947, 0.15244968235492706, 0.7495442628860474, 0.09655146300792694, 0.9994325041770935, 0.9989585280418396, 0.9986979961395264, 0.9962260127067566, 0.8695806860923767, 0.12910805642604828, 0.9985869526863098, 0.7003062963485718, 0.11362585425376892, 0.09352876991033554, 0.09198283404111862, 0.9971730709075928, 0.7950456142425537, 0.20360924303531647, 0.9992422461509705, 0.9976274371147156, 0.9987996816635132, 0.9971277713775635, 0.9989791512489319, 0.9985105395317078, 0.09013378620147705, 0.2989247143268585, 0.6103997230529785, 0.9977026581764221, 0.08305484801530838, 0.13826189935207367, 0.09038322418928146, 0.09380313009023666, 0.38644933700561523, 0.1133454442024231, 0.09478024393320084, 0.701711118221283, 0.1341952234506607, 0.16406448185443878, 0.9977979063987732, 0.7851636409759521, 0.1310769021511078, 0.08349122852087021, 0.9985881447792053, 0.9969950914382935, 0.9986138939857483, 0.9987617135047913, 0.34271931648254395, 0.19435462355613708, 0.07024486362934113, 0.06583487242460251, 0.09449981898069382, 0.14269472658634186, 0.08945982158184052, 0.997950553894043, 0.10137476772069931, 0.20992843806743622, 0.0345892459154129, 0.07309424877166748, 0.4859897494316101, 0.09506603330373764, 0.99627286195755, 0.09550514072179794, 0.7805180549621582, 0.07414615899324417, 0.0497359037399292, 0.9950889945030212, 0.998737096786499, 0.9982403516769409], \"Term\": [\"ability\", \"access\", \"account\", \"account\", \"advantage\", \"age\", \"amenity\", \"amenity\", \"amenity\", \"amount\", \"answer\", \"anyone\", \"anything\", \"appointment\", \"appointment\", \"area\", \"area\", \"area\", \"area\", \"area\", \"athlete\", \"atmosphere\", \"attitude\", \"bag\", \"ball\", \"basketball\", \"bathroom\", \"bed\", \"bench\", \"bike\", \"body\", \"body\", \"body\", \"body\", \"body\", \"box\", \"boxing\", \"business\", \"call\", \"call\", \"camp\", \"cancel\", \"cancellation\", \"canyon\", \"card\", \"cardio\", \"cardio\", \"care\", \"care\", \"care\", \"case\", \"chair\", \"challenge\", \"challenge\", \"change\", \"change\", \"charge\", \"check\", \"check\", \"check\", \"child\", \"childcare\", \"choice\", \"city\", \"class\", \"class\", \"class\", \"class\", \"class\", \"cleanliness\", \"client\", \"climbing\", \"club\", \"club\", \"club\", \"coach\", \"community\", \"company\", \"complaint\", \"condition\", \"contract\", \"contract\", \"cost\", \"couple\", \"couple\", \"course\", \"court\", \"credit\", \"crossfit\", \"crossfit\", \"crossfit\", \"customer\", \"daughter\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deal\", \"desk\", \"desk\", \"desk\", \"discount\", \"door\", \"employee\", \"employee\", \"employee\", \"energy\", \"environment\", \"eos\", \"equipment\", \"evening\", \"everyone\", \"everything\", \"exercise\", \"exercise\", \"exercise\", \"experience\", \"face\", \"facility\", \"fact\", \"fact\", \"family\", \"fan\", \"fee\", \"feel\", \"feeling\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"floor\", \"floor\", \"friend\", \"friend\", \"friend\", \"friend\", \"front\", \"fun\", \"game\", \"get\", \"girl\", \"goal\", \"goal\", \"goal\", \"gold\", \"goodlife\", \"group\", \"groupon\", \"guess\", \"guest\", \"guy\", \"guy\", \"guy\", \"gym\", \"gym\", \"gym\", \"gym\", \"gym\", \"gym\", \"gym\", \"hair\", \"hair\", \"half\", \"hand\", \"hand\", \"health\", \"health\", \"heart\", \"help\", \"help\", \"home\", \"hope\", \"hotel\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"house\", \"husband\", \"idea\", \"information\", \"injury\", \"instructor\", \"instructor\", \"instructor\", \"jacuzzi\", \"job\", \"join\", \"joke\", \"kid\", \"kid\", \"kind\", \"lady\", \"leg\", \"lesson\", \"level\", \"level\", \"life\", \"life\", \"life\", \"life\", \"lifetime\", \"line\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"locker\", \"look\", \"lot\", \"lot\", \"lounge\", \"love\", \"lvac\", \"machine\", \"machine\", \"maintenance\", \"man\", \"management\", \"management\", \"manager\", \"manager\", \"massage\", \"matter\", \"member\", \"member\", \"member\", \"member\", \"membership\", \"membership\", \"membership\", \"membership\", \"min\", \"mind\", \"minute\", \"minute\", \"minute\", \"minute\", \"money\", \"month\", \"month\", \"month\", \"month\", \"morning\", \"morning\", \"move\", \"muscle\", \"music\", \"need\", \"need\", \"need\", \"night\", \"notch\", \"nothing\", \"notice\", \"number\", \"number\", \"offer\", \"one\", \"opinion\", \"option\", \"option\", \"option\", \"option\", \"other\", \"owner\", \"owner\", \"owner\", \"package\", \"paper\", \"parent\", \"parking\", \"part\", \"part\", \"party\", \"pas\", \"pay\", \"pay\", \"payment\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"person\", \"person\", \"person\", \"phone\", \"phone\", \"phone\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plan\", \"plan\", \"planet\", \"policy\", \"pool\", \"pool\", \"practice\", \"pressure\", \"price\", \"pricing\", \"problem\", \"process\", \"product\", \"program\", \"quality\", \"question\", \"question\", \"rack\", \"ranch\", \"rate\", \"reason\", \"recommend\", \"refund\", \"rest\", \"result\", \"review\", \"review\", \"rock\", \"room\", \"room\", \"route\", \"salon\", \"salt\", \"saturday\", \"sauna\", \"schedule\", \"school\", \"scottsdale\", \"section\", \"selection\", \"service\", \"session\", \"shape\", \"shower\", \"side\", \"side\", \"sign\", \"sign\", \"signing\", \"situation\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"something\", \"something\", \"something\", \"something\", \"spa\", \"spa\", \"space\", \"spin\", \"spot\", \"staff\", \"staff\", \"staff\", \"staff\", \"star\", \"state\", \"station\", \"steam\", \"strength\", \"strength\", \"student\", \"studio\", \"studio\", \"stuff\", \"summer\", \"sunday\", \"sweat\", \"swim\", \"tanning\", \"teacher\", \"technique\", \"thank\", \"therapist\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thought\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tip\", \"today\", \"ton\", \"tour\", \"towel\", \"towel\", \"town\", \"train\", \"trainer\", \"trainer\", \"trainer\", \"trainer\", \"training\", \"training\", \"training\", \"treadmill\", \"treadmill\", \"treadmill\", \"treatment\", \"trial\", \"try\", \"tub\", \"tv\", \"tv\", \"u\", \"use\", \"use\", \"use\", \"use\", \"value\", \"variety\", \"variety\", \"vega\", \"vibe\", \"visit\", \"walk\", \"wall\", \"want\", \"water\", \"water\", \"water\", \"wave\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"week\", \"week\", \"week\", \"weekend\", \"weight\", \"weight\", \"weight\", \"wife\", \"wish\", \"woman\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"working\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"world\", \"year\", \"year\", \"year\", \"year\", \"yelp\", \"yoga\", \"zumba\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 6, 3, 4, 1, 7, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el67721216187790163946406106\", ldavis_el67721216187790163946406106_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el67721216187790163946406106\", ldavis_el67721216187790163946406106_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el67721216187790163946406106\", ldavis_el67721216187790163946406106_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4     -0.180141 -0.171197       1        1  15.290147\n",
       "5      0.099785 -0.179526       2        1  15.080709\n",
       "2     -0.321849  0.168989       3        1  14.386432\n",
       "3      0.132473  0.233427       4        1  14.329256\n",
       "0      0.170039  0.074284       5        1  14.194668\n",
       "6      0.081892  0.002373       6        1  13.987687\n",
       "1      0.017801 -0.128351       7        1  12.731105, topic_info=     Category          Freq         Term         Total  loglift  logprob\n",
       "term                                                                    \n",
       "18    Default   3540.000000    equipment   3540.000000  30.0000  30.0000\n",
       "182   Default   3196.000000         love   3196.000000  29.0000  29.0000\n",
       "9     Default   2694.000000     facility   2694.000000  28.0000  28.0000\n",
       "111   Default   5048.000000        staff   5048.000000  27.0000  27.0000\n",
       "72    Default   3314.000000         room   3314.000000  26.0000  26.0000\n",
       "20    Default   3042.000000      machine   3042.000000  25.0000  25.0000\n",
       "193   Default   6018.000000        class   6018.000000  24.0000  24.0000\n",
       "1     Default   2393.000000      service   2393.000000  23.0000  23.0000\n",
       "10    Default  11162.000000          gym  11162.000000  22.0000  22.0000\n",
       "173   Default   5717.000000        place   5717.000000  21.0000  21.0000\n",
       "107   Default   2195.000000   experience   2195.000000  20.0000  20.0000\n",
       "736   Default   2720.000000          spa   2720.000000  19.0000  19.0000\n",
       "157   Default   3277.000000         year   3277.000000  18.0000  18.0000\n",
       "218   Default   1955.000000     everyone   1955.000000  17.0000  17.0000\n",
       "981   Default   1875.000000      massage   1875.000000  16.0000  16.0000\n",
       "135   Default   3029.000000       member   3029.000000  15.0000  15.0000\n",
       "28    Default   3364.000000   membership   3364.000000  14.0000  14.0000\n",
       "57    Default   1666.000000        price   1666.000000  13.0000  13.0000\n",
       "172   Default   1594.000000   everything   1594.000000  12.0000  12.0000\n",
       "71    Default   1592.000000       locker   1592.000000  11.0000  11.0000\n",
       "420   Default   1576.000000        money   1576.000000  10.0000  10.0000\n",
       "513   Default   1564.000000     customer   1564.000000   9.0000   9.0000\n",
       "269   Default   3080.000000      trainer   3080.000000   8.0000   8.0000\n",
       "60    Default   2013.000000          lot   2013.000000   7.0000   7.0000\n",
       "58    Default   1496.000000         star   1496.000000   6.0000   6.0000\n",
       "24    Default   2311.000000       weight   2311.000000   5.0000   5.0000\n",
       "743   Default   1361.000000          fee   1361.000000   4.0000   4.0000\n",
       "43    Default   3494.000000        month   3494.000000   3.0000   3.0000\n",
       "244   Default   1290.000000         vega   1290.000000   2.0000   2.0000\n",
       "180   Default   2310.000000         week   2310.000000   1.0000   1.0000\n",
       "...       ...           ...          ...           ...      ...      ...\n",
       "139    Topic7    381.355225       matter    382.186676   2.0589  -5.1010\n",
       "1038   Topic7    363.755920     pressure    364.587494   2.0588  -5.1482\n",
       "614    Topic7    356.610931         word    357.442627   2.0588  -5.1681\n",
       "571    Topic7    349.564087      station    350.395935   2.0587  -5.1880\n",
       "634    Topic7    333.455414        notch    334.286835   2.0586  -5.2352\n",
       "888    Topic7    332.670380       injury    333.501556   2.0586  -5.2376\n",
       "689    Topic7    309.113892       answer    309.945282   2.0584  -5.3110\n",
       "32     Topic7    299.865753        bench    300.697388   2.0584  -5.3414\n",
       "537    Topic7    282.591431    condition    283.422821   2.0582  -5.4007\n",
       "1097   Topic7    271.652069         game    272.483826   2.0581  -5.4402\n",
       "1522   Topic7    269.786560         wave    270.621704   2.0580  -5.4471\n",
       "1064   Topic7    266.589905      athlete    267.421051   2.0580  -5.4590\n",
       "1343   Topic7    264.944366      opinion    265.775696   2.0580  -5.4652\n",
       "327    Topic7    255.075806          leg    255.907135   2.0579  -5.5032\n",
       "722    Topic7    248.944733   scottsdale    249.776184   2.0578  -5.5275\n",
       "1009   Topic7    237.668640      ability    238.500198   2.0576  -5.5738\n",
       "307    Topic7   1084.658325          guy   1394.081055   1.8102  -4.0557\n",
       "131    Topic7    791.590881        level    978.705383   1.8489  -4.3707\n",
       "431    Topic7    457.211182         hand    536.304565   1.9016  -4.9196\n",
       "86     Topic7   1259.124878        thing   2555.980957   1.3531  -3.9065\n",
       "108    Topic7   1415.270630      fitness   3847.993164   1.0609  -3.7896\n",
       "591    Topic7    390.375122  appointment    512.224243   1.7895  -5.0776\n",
       "119    Topic7    359.738403       health    472.636230   1.7882  -5.1593\n",
       "120    Topic7    443.228149     exercise    889.036865   1.3651  -4.9506\n",
       "10     Topic7    564.522705          gym  11162.963867  -0.9233  -4.7087\n",
       "1356   Topic7    346.575012      amenity    676.767212   1.3919  -5.1966\n",
       "269    Topic7    420.437836      trainer   3080.680908   0.0695  -5.0034\n",
       "167    Topic7    437.456299      workout   4596.804688  -0.2910  -4.9637\n",
       "51     Topic7    408.419800         time   5482.855469  -0.5360  -5.0324\n",
       "193    Topic7    376.553192        class   6018.626465  -0.7104  -5.1137\n",
       "\n",
       "[360 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "1009      7  0.997903      ability\n",
       "25        6  0.997897       access\n",
       "739       4  0.221096      account\n",
       "739       5  0.777930      account\n",
       "1053      4  0.996395    advantage\n",
       "191       2  0.998739          age\n",
       "1356      1  0.248239      amenity\n",
       "1356      3  0.239373      amenity\n",
       "1356      7  0.512732      amenity\n",
       "449       1  0.998982       amount\n",
       "689       7  0.996950       answer\n",
       "146       4  0.999574       anyone\n",
       "77        7  0.999033     anything\n",
       "591       5  0.236225  appointment\n",
       "591       7  0.761385  appointment\n",
       "52        1  0.294701         area\n",
       "52        3  0.474708         area\n",
       "52        4  0.073277         area\n",
       "52        6  0.093588         area\n",
       "52        7  0.063719         area\n",
       "1064      7  0.998425      athlete\n",
       "138       5  0.999034   atmosphere\n",
       "746       4  0.996761     attitude\n",
       "1431      1  0.997739          bag\n",
       "290       1  0.995080         ball\n",
       "78        1  0.998986   basketball\n",
       "649       3  0.997608     bathroom\n",
       "728       5  0.997724          bed\n",
       "32        7  0.997681        bench\n",
       "81        2  0.998136         bike\n",
       "...     ...       ...          ...\n",
       "259       2  0.997798      weekend\n",
       "24        1  0.785164       weight\n",
       "24        2  0.131077       weight\n",
       "24        7  0.083491       weight\n",
       "115       4  0.998588         wife\n",
       "247       2  0.996995         wish\n",
       "632       3  0.998614        woman\n",
       "614       7  0.998762         word\n",
       "333       1  0.342719         work\n",
       "333       2  0.194355         work\n",
       "333       3  0.070245         work\n",
       "333       4  0.065835         work\n",
       "333       5  0.094500         work\n",
       "333       6  0.142695         work\n",
       "333       7  0.089460         work\n",
       "360       6  0.997951      working\n",
       "167       1  0.101375      workout\n",
       "167       2  0.209928      workout\n",
       "167       3  0.034589      workout\n",
       "167       5  0.073094      workout\n",
       "167       6  0.485990      workout\n",
       "167       7  0.095066      workout\n",
       "960       6  0.996273        world\n",
       "157       2  0.095505         year\n",
       "157       4  0.780518         year\n",
       "157       6  0.074146         year\n",
       "157       7  0.049736         year\n",
       "1105      6  0.995089         yelp\n",
       "314       2  0.998737         yoga\n",
       "1542      2  0.998240        zumba\n",
       "\n",
       "[488 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 6, 3, 4, 1, 7, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## visualize the topics\n",
    "plot_lda = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, dictionary)\n",
    "plot_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model evaluation\n",
    "\n",
    "## compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus_tfidf))  # a measure of how good the model is. lower the better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Decide final topic term lists for all seven topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several assumptions:\n",
    "\n",
    " 1. If a term shows in several topics, this term is talked a lot by customers, but is not helpful in distinguish topics. So we want to consider unique words more when deciding the subjects of topics.\n",
    " \n",
    " 2. The subject of each topic is  determined by most of the terms in this topic but not all of them\n",
    " \n",
    " 3. The subjects of topics should not be too far away from our background knowledge which includes environment, facilities, service, instructors,courses.\n",
    " \n",
    "So when we decide the subject and terms of each topic, we have following principles:\n",
    "\n",
    " 1. The number of terms in each topic are not necessarily be the same, also they are not necessarily be the most relevant 10 terms. So what we have done is to limit terms in each topic to top 30 most relevant terms of this topic and then follow the principles we set to pick final terms of each topic.\n",
    " \n",
    " 2. The more topics a term shows, the less informative it is in deciding the subject.\n",
    " \n",
    " 3. Once we have decided the subject of one topic using 3 or 4 terms, we can select the rest terms using subject information instead of quantative measures. Thsi is called subject-first-rule\n",
    " \n",
    " 4. Use the probability of each topic, we determine the subject of topic one by one. Once we have decided subjects of previous topics, when we decide that of next topic, we do not consider the terms belonging to previous subject any more.\n",
    " \n",
    " 5. Three main measures: subject, estimated term frequency(ETF), ratio between estimated term frequency and total frequency(RATIO)\n",
    " \n",
    "Here are the detailed reasons for each topic:\n",
    "\n",
    " 1. Topic 1: \n",
    " \n",
    "    1.1 Although 'staff' has highest ETF in this topic, it also shows in topic 3,5,7, so we ignore it when deciding subject of this topic. \n",
    "    \n",
    "    1.2 Notice that the following three terms 'equipment', 'machine', and 'facility' have high ETF as well as RATIO, we want to assigh Subject-Facilities to this topic.\n",
    "    \n",
    "    1.3 Since we have decide the subject of this topic, we pick the rest 7 terms according to mainly Subject-Facilities, and then RATIO, last ETF\n",
    "    \n",
    "    1.4 The final terms are ['equipment','machine','facility','weight','cardio','pool','studio','floor','wall','stuff']\n",
    "    \n",
    " 2. Topic 2:\n",
    " \n",
    "    2.1 We first decide the subject of this topic. Since 'class' and 'place' shows in five topics, we cannot use them to decide the subject. Then, considering both ETF and RATIO, we found 'training','fun','group','session','yoga', they are the ones with high ETF or high RATIO. So we decide the this topic is Subject-Courses\n",
    "    \n",
    "    2.2 After we determine the Subject-Courses, we want to re-evaluate the terms 'class' and 'place', we retain 'class' in our final term list.\n",
    "    \n",
    "    2.3 According to Subject first rule, considering ETF and RATIO, our final terms are ['class','training','fun','group','session','yoga','schdule','program']\n",
    "    \n",
    " 3. Topic 3:\n",
    " \n",
    "    3.1 We first decided the subject of this topic. Although term 'love' has highest ETF and RATIO, it is not useful for determining the subject. So we look at next term. Although 'room' has the second ETF, but its RATIO is not 100%, and it appears in two topics. We also ignore 'room'. Then We found 'service','everything','locker','customer','kid','shower',these are all the terms wrt Service and Accessories. So the assigned subject of topic 3 is Subject-Service and Accessories.\n",
    "    \n",
    "    3.2 After we determine the Subject-Service and Accessories, we pick terms mainly according to this subject.\n",
    "    \n",
    "    3.3 The final terms of topic 3 is ['service','everything','locker','customer','kid','shower','sauna','desk','change','child','girl','treatment','staff','bathroom']\n",
    "    \n",
    " 4. Topic 4:\n",
    " \n",
    "     4.1 We first decide the subject of this topic: gym is definitely the one with hightest frequency, but it is non-informative since it appears in several topics. Then, although 'year,'member','membership' appear in several topics, we still use it for two reasons. On one hand, the RATIOs of them in this topic are the high. On other hand, looking at other terms in this topic, the majority of them are describing membership. As a result,  the subject of this topic is Subject-Membership and Price.\n",
    "    \n",
    "     4.2 After we determine the Subject-Membership and Price, we pick terms mainly according to this subject.\n",
    "     \n",
    "     4.3 THe final terms of topic 3 is ['year,'member','membership','money','contract','family','deal','rate']\n",
    "\n",
    " 5. Topic 5:\n",
    "    \n",
    "     5.1 When we want to determine the subject of this topic. using principle 4, we want to delete the terms belonging to previous subject. And after doing so, we assign Subject-Trainer to this topic. Please notice there are some  terms describing time after we doing so.  However, the RATIOs of these terms are not the highest ones in this topic.  So we consider Subject-Time to other topics.\n",
    "     \n",
    "     5.2 The terms of this topic are ['trainer','instructor','help']\n",
    "     \n",
    " 6. Topic 6:\n",
    "     \n",
    "     6.1 Use same rules as topic 5, the subject is Subject-Time\n",
    "     \n",
    "     6.2 The terms of this topic are ['time','day','month','morning','night','hour','door']\n",
    "     \n",
    " 7. Topic 7:\n",
    "     \n",
    "     7.1 Use same rules as topic 5, the subject is Subject-Environment\n",
    "     \n",
    "     7.2 The terms of this topic are ['anything','music','planet','house']\n",
    "     \n",
    "     \n",
    "We should admit that the LDA topic model may not be the best one, because we did not do Grid Search for all the parameters due to limit of memory. So there may be some misclassified terms crossing topics. As a result, after assign subject and pick the terms list for each topic. We research all the terms and the relevant topic's subject, we add some unused term to  the final term list.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_0_terms = ['equipment','machine','facility','weight','cardio','pool','studio','floor','wall','stuff']\n",
    "topic_1_terms = ['class','training','fun','group','session','yoga','schdule','program']\n",
    "topic_2_terms = ['service','everything','locker','customer','kid','shower','sauna','desk','change','child','girl','treatment','staff','bathroom']\n",
    "topic_3_terms = ['year','member','membership','money','contract','family','deal','rate']\n",
    "topic_4_terms = ['trainer','instructor','help','coach']\n",
    "topic_5_terms = ['time','day','month','morning','night','hour','door']\n",
    "topic_6_terms = ['anything','music','planet','house']\n",
    "\n",
    "topic_terms = [topic_0_terms,topic_1_terms,topic_2_terms,topic_3_terms,topic_4_terms,topic_5_terms,topic_6_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Enlarge the topic term lists using synsets manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_0_terms_en = ['equipment','machine','facility','weight','cardio','pool','studio','floor','wall','stuff','tools','treadmill','barbell','dumbbell','elliptical','ball','band','spinning','stepper','back','adductor','cycling','step','cardio','machine','cycle','climber','beach','bike']\n",
    "topic_1_terms_en = ['class','training','fun','group','session','yoga','schdule','program','course','strength','aerobics','workout','boxing','crunch','squat','raw','entrench','press','fitball','abs','karate','zumba','speed','trial','programming','level','crossfit']\n",
    "topic_2_terms_en = ['service','everything','locker','customer','kid','shower','sauna','desk','change','child','girl','treatment','staff','bathroom','dryer']\n",
    "topic_3_terms_en = ['year','member','membership','money','contract','family','deal','rate','bill','month','season','flexibility']\n",
    "topic_4_terms_en = ['trainer','instructor','help','coach','teacher','advisor','mentor','helper','tutor']\n",
    "topic_5_terms_en = ['time','day','month','morning','night','hour','door','minute','24hr','hour']\n",
    "topic_6_terms_en = ['anything','music','planet','house','environment','capability','layout','design','smell']\n",
    "\n",
    "topic_terms_en = [topic_0_terms_en,topic_1_terms_en,topic_2_terms_en,topic_3_terms_en,topic_4_terms_en ,topic_5_terms_en,topic_6_terms_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Create dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use terms detection to find the dominant topic, here, we follow a rule that if a sentence mention more than one topic, and the numbers of espective \n",
    "## terms are the same, then we assign topic with smaller index to this sentence.\n",
    "## the proof is that the smaller index the topic have, the important this topic is.\n",
    "## of course, this will cause some inaccuracy.\n",
    "for num in tqdm(range(len(data_sentence_dic[\"review_number\"]))):\n",
    "    sent = data_sentence_dic[\"sentence\"][num]\n",
    "    nouns_list = data_sentence_dic[\"nouns\"][num]\n",
    "    count_topic=[]\n",
    "    for t in range(7):\n",
    "        topic_term_num = topic_terms_en[t]\n",
    "        count = 0\n",
    "        if any(element in sent for element in topic_term_num):\n",
    "            for term in topic_term_num:\n",
    "                if term in nouns_list:\n",
    "                    count+=1\n",
    "        count_topic.append(count)\n",
    "        \n",
    "    if sum(count_topic)==0:\n",
    "        data_sentence_dic.setdefault(\"dominant_topic\",[]).append(7)\n",
    "    else:\n",
    "        data_sentence_dic.setdefault(\"dominant_topic\",[]).append(count_topic.index(max(count_topic)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Read sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import another positive and negative words dictionary \n",
    "positive_dic = set(pd.read_csv('../data/dictionary/positive-words.txt',names = ['word'])['word'])\n",
    "negative_dic = set(pd.read_csv('../data/dictionary/negative-words.txt', encoding = \"ISO-8859-1\",names = ['word'])['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = positive_dic\n",
    "neg_dict = negative_dic\n",
    "deny_dict = [\"not\",\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "             \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "             \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "             \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "             \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "             \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "             \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "             \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "most_dict = ['absolute','absolutely','alarmingly','amazingly','astonishingly','awfullybitterly','completely',\n",
    "             'deep-rooted','deep-seated','deeply','definitely','disastrously','downright','entirely','exceedingly',\n",
    "             'excessively','extreme','extremely','fully','greatest','greatly','heinous','hundred-percent','immensely',\n",
    "             'immoderate','incomparably','ingrained','matchlessly','monstrous','most','outstanding','outstandingly',\n",
    "             'right-down','sharply','sheer','superb','terribly','totally','towering','unusually','utmost','utterly','most']\n",
    "very_dict = ['awfully','badly','better','considerably','deep','disastrously','especially','extraordinarily''extremely','greatly',\n",
    "             'how','however','indeed','much','particularly','really','terribly','unusually','very']\n",
    "more_dict = ['comparatively','further','increasingly','more','plus','relatively','so','such']\n",
    "\n",
    "ish_dict = ['bit','fairly','passably','pretty','quite','rather','slightly','some','somewhat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Get sentence score for each sentence use sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_score(data_sentence_dic_sent):\n",
    "    data_sentence_dic_sent['sentiment_score_topic']=[0]*len(data_sentence_dic_sent['review_number'])\n",
    "    for t in range(7):\n",
    "        ## for each topic, compute the scores of sentences under this topic  \n",
    "        index_topic_t = [i for i in range(len(data_sentence_dic_sent['dominant_topic'])) if data_sentence_dic_sent['dominant_topic'][i] == t]\n",
    "        ## save word score in each sentence into count_sentence\n",
    "        count_sentence = []\n",
    "        ## save count_sentence into count_topic\n",
    "        count_topic = []\n",
    "        for index in tqdm(index_topic_t):\n",
    "            ##for each sentence, first get tokenized_sentence\n",
    "            tokenized_sent = data_sentence_dic_sent['tokenized_sentence'][index] \n",
    "            word_loc = 0 ## record the location of adjective\n",
    "            adj_loc = 0 ## record the location of word\n",
    "            poscount_origin = 0 ## original score of positive word \n",
    "            poscount_inverse = 0 ## if there are deny words before this positive word, poscount_origin will be replaced by poscount_inverse\n",
    "            poscount_final = 0 ## final score for positive words\n",
    "            negcount_origin = 0 ## original score of negative words\n",
    "            negcount_inverse = 0 ## inverse score of negative words\n",
    "            negcount_final = 0 ##final score for negative words\n",
    "            ## loop all the words in this sentence's tokenized_sentence\n",
    "            for word in tokenized_sent:\n",
    "                ##judge if it is positive word\n",
    "                if word in pos_dict: \n",
    "                    poscount_origin +=1\n",
    "                    ## count the number of deny word\n",
    "                    deny_num = 0\n",
    "                    ## loop all the words before this specific word to find degree adverb\n",
    "                    for w in tokenized_sent[adj_loc:word_loc]: \n",
    "                        if w in most_dict: ## judge if the adverb is most degree, then the score will be  four times\n",
    "                            poscount_origin *= 4.0\n",
    "                        elif w in very_dict:  ##judge if the adverb is very degree, then the score will be three times\n",
    "                            poscount_origin *= 3.0\n",
    "                        elif w in more_dict: ## judge if the adverb is more degree, then the score will be two times\n",
    "                            poscount_origin *= 2.0\n",
    "                        elif w in ish_dict:  ## judge if the adverb is a bit degree, then the score will be half\n",
    "                            poscount_origin *= 0.5\n",
    "                        elif w in deny_dict: ## if it is deny word, deny_num will add one\n",
    "                            deny_num+= 1\n",
    "                    if deny_num % 2 != 0 : ## if number of deny word is odd, score will inverse. ow, score will stay the same\n",
    "                        poscount_origin *= -1.0\n",
    "                        poscount_inverse += poscount_origin\n",
    "                        poscount_origin = 0\n",
    "                        poscount_final = poscount_origin + poscount_inverse + poscount_final\n",
    "                        poscount_inverse = 0\n",
    "                    else:\n",
    "                        poscount_final = poscount_origin + poscount_inverse + poscount_final\n",
    "                        poscount_origin = 0\n",
    "                    adj_loc = word_loc + 1\n",
    "                elif word in neg_dict: \n",
    "                    negcount_origin += 1\n",
    "                    num_deny = 0\n",
    "                    for w in tokenized_sent[adj_loc:word_loc]:\n",
    "                        if w in most_dict:\n",
    "                            negcount_origin *= 4.0\n",
    "                        elif w in very_dict:\n",
    "                            negcount_origin *= 3.0\n",
    "                        elif w in more_dict:\n",
    "                            negcount_origin *= 2.0\n",
    "                        elif w in ish_dict:\n",
    "                            negcount_origin *= 0.5\n",
    "                        elif w in deny_dict:\n",
    "                            num_deny += 1\n",
    "                    if num_deny % 2 != 0 :\n",
    "                        negcount_origin *= -1.0\n",
    "                        negcount_inverse += negcount_origin\n",
    "                        negcount_origin = 0\n",
    "                        negcount_final = negcount_origin + negcount_inverse + negcount_final\n",
    "                        negcount_inverse = 0\n",
    "                    else:\n",
    "                        negcount_final = negcount_origin + negcount_inverse + negcount_final\n",
    "                        negcount_origin = 0\n",
    "                    adj_loc = word_loc + 1\n",
    "                else:\n",
    "                    poscount_final=0\n",
    "                    negcount_final=0\n",
    "\n",
    "                word_loc += 1\n",
    " \n",
    "            ## in case poscount_final or negcount_final is 0\n",
    "                pos_count = 0\n",
    "                neg_count = 0\n",
    "                if poscount_final <0 and negcount_final > 0:\n",
    "                    neg_count += negcount_final - poscount_final\n",
    "                    pos_count = 0\n",
    "                elif negcount_final <0 and poscount_final > 0:\n",
    "                    pos_count = poscount_final - negcount_final\n",
    "                    neg_count = 0\n",
    "                elif poscount_final <0 and negcount_final < 0:\n",
    "                    neg_count = -pos_count\n",
    "                    pos_count = -neg_count\n",
    "                else:\n",
    "                    pos_count = poscount_final\n",
    "                    neg_count = negcount_final\n",
    "                    \n",
    "                \n",
    "                count_sentence.append([pos_count,neg_count]) ## a list with sublist describing every word's pos or neg count\n",
    "                \n",
    "            count_topic.append(count_sentence)\n",
    "            count_sentence=[]\n",
    "        \n",
    "        for index in tqdm(range(len(index_topic_t))):\n",
    "            sent_index = index_topic_t[index]\n",
    "            sent_s = count_topic[index]\n",
    "            if len(sent_s) !=0:\n",
    "                score_array =  np.array(sent_s)\n",
    "                pos_score = np.mean(score_array[:,0])## average count of positive adjective\n",
    "                pos_score = float('%.lf' % pos_score)\n",
    "                neg_score = np.mean(score_array[:, 1])## average count of negative adjective\n",
    "                neg_score = float('%.1f' % neg_score)\n",
    "                final_score = pos_score - neg_score\n",
    " \n",
    "                data_sentence_dic_sent['sentiment_score_topic'][sent_index] = final_score\n",
    "\n",
    "            else:\n",
    "                ## if tokenized_sentence is empty list\n",
    "                data_sentence_dic_sent['sentiment_score_topic'][sent_index] = 0\n",
    "    \n",
    "    return data_sentence_dic_sent  ## return to data_sentence_dic with sentence sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=get_sentence_score(data_sentence_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Evaluate business use sentences scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_summary(business_id,):\n",
    "    business_score = []\n",
    "    business_summary = [] ## index:  (0,1,2 topic1 total pos and neg，345，topic2 total pos and neg ………………)\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    for t in range(8):\n",
    "        sentence_score= [data_sentence_dic[\"sentiment_score_topic\"][k] for k in range(len(data_sentence_dic[\"review_number\"])) if data_sentence_dic[\"review_number\"][k] in review_number_id and data_sentence_dic['dominant_topic'][k]==t]\n",
    "        topic_pos_num = 0\n",
    "        topic_neg_num = 0\n",
    "        topic_neu_num = 0\n",
    "\n",
    "        if len(sentence_score)!=0:\n",
    "            for s in sentence_score:\n",
    "                if s>0:\n",
    "                    topic_pos_num+=1\n",
    "                elif s < 0:\n",
    "                    topic_neg_num+=1\n",
    "                else:\n",
    "                    topic_neu_num+=1\n",
    "            business_score.append(sum(sentence_score)/len(sentence_score))\n",
    "\n",
    "        else:\n",
    "            business_score.append(0)\n",
    "            \n",
    "            \n",
    "        business_summary.append(topic_pos_num+topic_neg_num+topic_neu_num)\n",
    "        business_summary.append(topic_pos_num)\n",
    "        business_summary.append(topic_neg_num)\n",
    "        business_summary.append(topic_neu_num)\n",
    "\n",
    "    #summary = (\n",
    "    #    'For your gym, there are '+str(len(review_number_id))+' reviews.\\n Among these reviews,'+'Topic_facilities has been mentioned '+str(business_summary[0])+' times,'+str(business_summary[1])+' are positive,'+str(business_summary[2])+' are negative,'+str(business_summary[3])+' are neutral.\\n'\n",
    "    #    +'Topic_Courses has been mentioned '+str(business_summary[4])+' times,'+str(business_summary[5])+' are positive,'+str(business_summary[6])+' are negative,'+str(business_summary[7])+' are neutral.\\n  '\n",
    "    #    +'Topic_Service and Accessories has been mentioned '+str(business_summary[8])+' times,'+str(business_summary[9])+' are positive,'+str(business_summary[10])+' are negative,'+str(business_summary[11])+' are neutral.\\n'\n",
    "    #    +'Topic_Membership and Price has been mentioned '+str(business_summary[12])+' times,'+str(business_summary[13])+' are positive,'+str(business_summary[14])+' are negative,'+str(business_summary[15])+' are neutral.\\n'\n",
    "    #    +'Topic_Trainers has been mentioned '+str(business_summary[16])+' times,'+str(business_summary[17])+' are positive,'+str(business_summary[18])+' are negative,'+str(business_summary[19])+' are neutral.\\n'\n",
    "    #    +'Topic_Time has been mentioned '+str(business_summary[20])+' times,'+str(business_summary[21])+' are positive,'+str(business_summary[22])+' are negative,'+str(business_summary[23])+' are neutral.\\n'\n",
    "    #    +'Topic_Environment has been mentioned '+str(business_summary[24])+' times,'+str(business_summary[25])+' are positive,'+str(business_summary[26])+' are negative,'+str(business_summary[27])+' are neutral.\\n'\n",
    "    #    +'Other topics have been mentioned '+str(business_summary[28])+' times,'+str(business_summary[29])+' are positive,'+str(business_summary[30])+' are negative,'+str(business_summary[31])+' are neutral.\\n'\n",
    "    #    +'Your gym\\'s topic scores on seven topics are '+str(business_score)\n",
    "    #        )\n",
    "    \n",
    "    return business_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate eight topic scores for each business\n",
    "\n",
    "def get_business_score(business_id):\n",
    "    business_score = []\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    for t in range(8):\n",
    "        sentence_score= [data_sentence_dic[\"sentiment_score_topic\"][k] for k in range(len(data_sentence_dic[\"review_number\"])) if data_sentence_dic[\"review_number\"][k] in review_number_id and data_sentence_dic['dominant_topic'][k]==t]\n",
    "        topic_pos_num = 0\n",
    "        topic_neg_num = 0\n",
    "        topic_neu_num = 0\n",
    "\n",
    "        if len(sentence_score)!=0:\n",
    "            for s in sentence_score:\n",
    "                if s>0:\n",
    "                    topic_pos_num+=1\n",
    "                elif s < 0:\n",
    "                    topic_neg_num+=1\n",
    "                else:\n",
    "                    topic_neu_num+=1\n",
    "            business_score.append(sum(sentence_score)/len(sentence_score))\n",
    "\n",
    "        else:\n",
    "            business_score.append(0)\n",
    "\n",
    "    return business_score\n",
    "\n",
    "\n",
    "## create weighted_stars \n",
    "\n",
    "def get_review_weight(rev_num):\n",
    "    nouns_list_rev = [data_sentence_dic['nouns'][i] for i in range(len(data_sentence_dic['review_number'])) if data_sentence_dic['review_number'][i] == rev_num]\n",
    "    nouns_list_rev = [item for sublist in nouns_list_rev for item in sublist]\n",
    "    topic_terms_en_rev = [item for sublist in topic_terms_en for item in sublist]\n",
    "    count_rev = 0\n",
    "    for word in nouns_list_rev:\n",
    "        if word in topic_terms_en_rev:\n",
    "            count_rev+=1\n",
    "    return count_rev\n",
    "\n",
    "def get_business_weighted_stars(business_id):\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in review_number_id:\n",
    "        weight = get_review_weight(i)\n",
    "        star = data_review.loc[data_review['business_id']==business_id]['stars'][i]\n",
    "        numerator+=weight*star\n",
    "        denominator+=weight\n",
    "        \n",
    "    return numerator/denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_scores={}\n",
    "for d in tqdm(idset):\n",
    "    scores = get_business_score(d)\n",
    "    summary = get_business_summary(d)\n",
    "    business_scores.setdefault('business_id',[]).append(d)\n",
    "    business_scores.setdefault('topic_1_Facilities',[]).append(scores[0])\n",
    "    business_scores.setdefault('topic_2_Courses',[]).append(scores[1])\n",
    "    business_scores.setdefault('topic_3_Service_and_Accessories',[]).append(scores[2])\n",
    "    business_scores.setdefault('topic_4_Membership_and_Price',[]).append(scores[3])\n",
    "    business_scores.setdefault('topic_5_Trainer',[]).append(scores[4])\n",
    "    business_scores.setdefault('topic_6_Time',[]).append(scores[5])\n",
    "    business_scores.setdefault('topic_7_Environment',[]).append(scores[6])\n",
    "    business_scores.setdefault('topic_8_Other',[]).append(scores[7])\n",
    "    business_scores.setdefault('weighted_stars',[]).append(get_business_weighted_stars(d))\n",
    "    for i in range(32):\n",
    "        business_scores.setdefault('summary_'+str(i),[]).append(summary[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create business_scores.csv\n",
    "pd.DataFrame(business_scores).to_csv(path_or_buf='../data/business_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
