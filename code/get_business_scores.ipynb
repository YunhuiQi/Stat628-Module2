{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is to generate feature summary and topic scores for all business. \n",
    "## Usage: This file will take review_train.csv as initial input, and then it will generate data_sentence.csv, read data_sentence.csv, generate data_sentence_scores.csv and business_scores.csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import modules :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import FreqDist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import multiprocessing\n",
    "import time\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## define stopwords\n",
    "sr = stopwords.words('english')\n",
    "sr.remove('not')\n",
    "p = inflect.engine()\n",
    "wnl = WordNetLemmatizer() \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "porter = nltk.PorterStemmer()\n",
    "nlp = spacy.load(\"en\")\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the reviews data for gym business as pandas dataframe and let the name be data_review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read review data\n",
    "data_review=pd.read_csv(\"review_train.csv\")\n",
    "data_review[\"review_number\"] = range(len(data_review))\n",
    "idset = data_review['business_id']\n",
    "idset = list(set(idset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Main step: clean data---LDA model---creaete topic-terms list---get dominant topics---get sentence scores---get business scores and feature summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create data_sentence dictionary and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess(dataset):\n",
    "    data_sentence = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        ## dealing with \\n and \\n\\n things\n",
    "        xx = dataset.iloc[i]['text']\n",
    "        xx = re.sub(r' \\n\\n','.',xx)\n",
    "        xx = re.sub(r'\\n','',xx)\n",
    "        xx = re.sub(r'\\.\\.','. ',xx)\n",
    "        xx = re.sub(r'(\\.)(\\S)',r'\\1 \\2',xx)\n",
    "        \n",
    "        ## for each review, tokenize it into sentences, saved in sent_set\n",
    "        sent_set = sent_tokenize(xx)\n",
    "    \n",
    "        ## for each sentence in sent_set, tokenized and cleaning into tokenized_sentence\n",
    "        for j in range(len(sent_set)):\n",
    "        \n",
    "            sent = sent_set[j]\n",
    "            if len(sent) > 3:\n",
    "                ## get \"review_number\" for this sentence\n",
    "                data_sentence.setdefault('review_number', []).append(dataset.iloc[i]['review_number'])\n",
    "        \n",
    "                ## assign this sentence into column \"sentence\" and sentence_list\n",
    "                data_sentence.setdefault('sentence', []).append(sent)\n",
    "\n",
    "        \n",
    "                ## clean this sentence into \"tokenized_sentence\"\n",
    "                x = re.sub(r'n\\'t',' not',sent)\n",
    "                ## split into words\n",
    "                x = word_tokenize(x)\n",
    "                ## remove punctuation\n",
    "                x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "                ## change numbers into words\n",
    "                x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "                ## remove not alphabetic\n",
    "                x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "                ## convert to lower case\n",
    "                x = [w.lower() for w in x]\n",
    "                ## remove stop words\n",
    "                x = [w for w in x if not w in sr]\n",
    "                ## lemmatization\n",
    "                x = [wnl.lemmatize(w) for w in x]\n",
    "            \n",
    "                ## assign cleaned sentence words to \"tokenized_sentence\" and tokenized_sentence_list\n",
    "                data_sentence.setdefault('tokenized_sentence', []).append(x)\n",
    "\n",
    "\n",
    "                \n",
    "                ## POS\n",
    "                ## change cleaned words into nlp format\n",
    "                sent_nlp = nlp(\" \".join(x))\n",
    "                ## get nouns for each sentence and saved into nouns_list\n",
    "                nolis = [token.lemma_ for token in sent_nlp if token.pos_ == \"NOUN\"]\n",
    "                ## assign nouns to \"nouns\"\n",
    "                data_sentence.setdefault('nouns', []).append(nolis)\n",
    "            \n",
    "    return data_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentence = dataprocess(data_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data into data_sentence.txt\n",
    "fw = open(\"data_sentence.txt\",'w+')\n",
    "fw.write(str(data_sentence))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data_sentence.txt\n",
    "fr = open(\"data_sentence.txt\",'r+')\n",
    "data_sentence_dic = eval(fr.read()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create frequency and tfidf matrix for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcorpus(datadict):\n",
    "    \n",
    "    nouns_list = datadict[\"nouns\"]\n",
    "    \n",
    "    ## define dictionary for \"nouns\"\n",
    "    dictionary = corpora.Dictionary(nouns_list)\n",
    "\n",
    "    ## create frequency matrix\n",
    "    frequency_matrix = [dictionary.doc2bow(n) for n in nouns_list]\n",
    "                    \n",
    "    ## create tfidf matrix\n",
    "    tfidf = gensim.models.TfidfModel(frequency_matrix)\n",
    "    corpus_tfidf = tfidf[frequency_matrix]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf, dictionary = getcorpus(data_sentence_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply LDA topic model to tfidf matrix and visualization and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply LDA model on tfidf matrix                                                        \n",
    "lda_model = LDA(corpus_tfidf, id2word=dictionary, num_topics=7, random_state=100,chunksize=10000, passes=50)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "lda_model.save(\"ldamodel/lda_7topics.model\")\n",
    "\n",
    "## load model\n",
    "lda_model=  models.LdaModel.load('ldamodel/lda_7topics.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print all topics\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the topics\n",
    "plot_lda = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, dictionary)\n",
    "plot_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model evaluation\n",
    "\n",
    "## compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus_tfidf))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "## compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_sentence_dic[\"nouns\"], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Decide final topic term lists for all seven topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_0_terms = ['equipment','machine','facility','weight','cardio','pool','studio','floor','wall','stuff']\n",
    "topic_1_terms = ['class','training','fun','group','session','yoga','schdule','program']\n",
    "topic_2_terms = ['service','everything','locker','customer','kid','shower','sauna','desk','change','child','girl','treatment','staff','bathroom']\n",
    "topic_3_terms = ['year','member','membership','money','contract','family','deal','rate']\n",
    "topic_4_terms = ['trainer','instructor','help','coach']\n",
    "topic_5_terms = ['time','day','month','morning','night','hour','door']\n",
    "topic_6_terms = ['anything','music','planet','house']\n",
    "\n",
    "topic_terms = [topic_0_terms,topic_1_terms,topic_2_terms,topic_3_terms,topic_4_terms,topic_5_terms,topic_6_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Enlarge the topic term lists using synsets manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_0_terms_en = ['equipment','machine','facility','weight','cardio','pool','studio','floor','wall','stuff','tools','treadmill','barbell','dumbbell','elliptical','ball','band','spinning','stepper','back','adductor','cycling','step','cardio','machine','cycle','climber','beach','bike']\n",
    "topic_1_terms_en = ['class','training','fun','group','session','yoga','schdule','program','course','strength','aerobics','workout','boxing','crunch','squat','raw','entrench','press','fitball','abs','karate','zumba','speed','trial','programming','level','crossfit']\n",
    "topic_2_terms_en = ['service','everything','locker','customer','kid','shower','sauna','desk','change','child','girl','treatment','staff','bathroom','dryer']\n",
    "topic_3_terms_en = ['year','member','membership','money','contract','family','deal','rate','bill','month','season','flexibility']\n",
    "topic_4_terms_en = ['trainer','instructor','help','coach','teacher','advisor','mentor','helper','tutor']\n",
    "topic_5_terms_en = ['time','day','month','morning','night','hour','door','minute','24hr','hour']\n",
    "topic_6_terms_en = ['anything','music','planet','house','environment','capability','layout','design','smell']\n",
    "\n",
    "topic_terms_en = [topic_0_terms_en,topic_1_terms_en,topic_2_terms_en,topic_3_terms_en,topic_4_terms_en ,topic_5_terms_en,topic_6_terms_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Create dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use terms detection to find the dominant topic, here, we follow a rule that if a sentence mention more than one topic, and the numbers of espective \n",
    "## terms are the same, then we assign topic with smaller index to this sentence.\n",
    "## the proof is that the smaller index the topic have, the important this topic is.\n",
    "## of course, this will cause some inaccuracy.\n",
    "for num in tqdm(range(len(data_sentence_dic[\"review_number\"]))):\n",
    "    sent = data_sentence_dic[\"sentence\"][num]\n",
    "    nouns_list = data_sentence_dic[\"nouns\"][num]\n",
    "    count_topic=[]\n",
    "    for t in range(7):\n",
    "        topic_term_num = topic_terms_en[t]\n",
    "        count = 0\n",
    "        if any(element in sent for element in topic_term_num):\n",
    "            for term in topic_term_num:\n",
    "                if term in nouns_list:\n",
    "                    count+=1\n",
    "        count_topic.append(count)\n",
    "        \n",
    "    if sum(count_topic)==0:\n",
    "        data_sentence_dic.setdefault(\"dominant_topic\",[]).append(7)\n",
    "    else:\n",
    "        data_sentence_dic.setdefault(\"dominant_topic\",[]).append(count_topic.index(max(count_topic)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Read sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import another positive and negative words dictionary \n",
    "positive_dic = set(pd.read_csv('positive-words.txt',names = ['word'])['word'])\n",
    "negative_dic = set(pd.read_csv('negative-words.txt', encoding = \"ISO-8859-1\",names = ['word'])['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = positive_dic\n",
    "neg_dict = negative_dic\n",
    "deny_dict = [\"not\",\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "             \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "             \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "             \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "             \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "             \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "             \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "             \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "most_dict = ['absolute','absolutely','alarmingly','amazingly','astonishingly','awfullybitterly','completely',\n",
    "             'deep-rooted','deep-seated','deeply','definitely','disastrously','downright','entirely','exceedingly',\n",
    "             'excessively','extreme','extremely','fully','greatest','greatly','heinous','hundred-percent','immensely',\n",
    "             'immoderate','incomparably','ingrained','matchlessly','monstrous','most','outstanding','outstandingly',\n",
    "             'right-down','sharply','sheer','superb','terribly','totally','towering','unusually','utmost','utterly','most']\n",
    "very_dict = ['awfully','badly','better','considerably','deep','disastrously','especially','extraordinarily''extremely','greatly',\n",
    "             'how','however','indeed','much','particularly','really','terribly','unusually','very']\n",
    "more_dict = ['comparatively','further','increasingly','more','plus','relatively','so','such']\n",
    "\n",
    "ish_dict = ['bit','fairly','passably','pretty','quite','rather','slightly','some','somewhat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Get sentence score for each sentence use sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_score(data_sentence_dic_sent):\n",
    "    data_sentence_dic_sent['sentiment_score_topic']=[0]*len(data_sentence_dic_sent['review_number'])\n",
    "    for t in range(7):\n",
    "        ## for each topic, compute the scores of sentences under this topic  \n",
    "        index_topic_t = [i for i in range(len(data_sentence_dic_sent['dominant_topic'])) if data_sentence_dic_sent['dominant_topic'][i] == t]\n",
    "        ## save word score in each sentence into count_sentence\n",
    "        count_sentence = []\n",
    "        ## save count_sentence into count_topic\n",
    "        count_topic = []\n",
    "        for index in tqdm(index_topic_t):\n",
    "            ##for each sentence, first get tokenized_sentence\n",
    "            tokenized_sent = data_sentence_dic_sent['tokenized_sentence'][index] \n",
    "            word_loc = 0 ## record the location of adjective\n",
    "            adj_loc = 0 ## record the location of word\n",
    "            poscount_origin = 0 ## original score of positive word \n",
    "            poscount_inverse = 0 ## if there are deny words before this positive word, poscount_origin will be replaced by poscount_inverse\n",
    "            poscount_final = 0 ## final score for positive words\n",
    "            negcount_origin = 0 ## original score of negative words\n",
    "            negcount_inverse = 0 ## inverse score of negative words\n",
    "            negcount_final = 0 ##final score for negative words\n",
    "            ## loop all the words in this sentence's tokenized_sentence\n",
    "            for word in tokenized_sent:\n",
    "                ##judge if it is positive word\n",
    "                if word in pos_dict: \n",
    "                    poscount_origin +=1\n",
    "                    ## count the number of deny word\n",
    "                    deny_num = 0\n",
    "                    ## loop all the words before this specific word to find degree adverb\n",
    "                    for w in tokenized_sent[adj_loc:word_loc]: \n",
    "                        if w in most_dict: ## judge if the adverb is most degree, then the score will be  four times\n",
    "                            poscount_origin *= 4.0\n",
    "                        elif w in very_dict:  ##judge if the adverb is very degree, then the score will be three times\n",
    "                            poscount_origin *= 3.0\n",
    "                        elif w in more_dict: ## judge if the adverb is more degree, then the score will be two times\n",
    "                            poscount_origin *= 2.0\n",
    "                        elif w in ish_dict:  ## judge if the adverb is a bit degree, then the score will be half\n",
    "                            poscount_origin *= 0.5\n",
    "                        elif w in deny_dict: ## if it is deny word, deny_num will add one\n",
    "                            deny_num+= 1\n",
    "                    if deny_num % 2 != 0 : ## if number of deny word is odd, score will inverse. ow, score will stay the same\n",
    "                        poscount_origin *= -1.0\n",
    "                        poscount_inverse += poscount_origin\n",
    "                        poscount_origin = 0\n",
    "                        poscount_final = poscount_origin + poscount_inverse + poscount_final\n",
    "                        poscount_inverse = 0\n",
    "                    else:\n",
    "                        poscount_final = poscount_origin + poscount_inverse + poscount_final\n",
    "                        poscount_origin = 0\n",
    "                    adj_loc = word_loc + 1\n",
    "                elif word in neg_dict: \n",
    "                    negcount_origin += 1\n",
    "                    num_deny = 0\n",
    "                    for w in tokenized_sent[adj_loc:word_loc]:\n",
    "                        if w in most_dict:\n",
    "                            negcount_origin *= 4.0\n",
    "                        elif w in very_dict:\n",
    "                            negcount_origin *= 3.0\n",
    "                        elif w in more_dict:\n",
    "                            negcount_origin *= 2.0\n",
    "                        elif w in ish_dict:\n",
    "                            negcount_origin *= 0.5\n",
    "                        elif w in deny_dict:\n",
    "                            num_deny += 1\n",
    "                    if num_deny % 2 != 0 :\n",
    "                        negcount_origin *= -1.0\n",
    "                        negcount_inverse += negcount_origin\n",
    "                        negcount_origin = 0\n",
    "                        negcount_final = negcount_origin + negcount_inverse + negcount_final\n",
    "                        negcount_inverse = 0\n",
    "                    else:\n",
    "                        negcount_final = negcount_origin + negcount_inverse + negcount_final\n",
    "                        negcount_origin = 0\n",
    "                    adj_loc = word_loc + 1\n",
    "                else:\n",
    "                    poscount_final=0\n",
    "                    negcount_final=0\n",
    "\n",
    "                word_loc += 1\n",
    " \n",
    "            ## in case poscount_final or negcount_final is 0\n",
    "                pos_count = 0\n",
    "                neg_count = 0\n",
    "                if poscount_final <0 and negcount_final > 0:\n",
    "                    neg_count += negcount_final - poscount_final\n",
    "                    pos_count = 0\n",
    "                elif negcount_final <0 and poscount_final > 0:\n",
    "                    pos_count = poscount_final - negcount_final\n",
    "                    neg_count = 0\n",
    "                elif poscount_final <0 and negcount_final < 0:\n",
    "                    neg_count = -pos_count\n",
    "                    pos_count = -neg_count\n",
    "                else:\n",
    "                    pos_count = poscount_final\n",
    "                    neg_count = negcount_final\n",
    "                    \n",
    "                \n",
    "                count_sentence.append([pos_count,neg_count]) ## a list with sublist describing every word's pos or neg count\n",
    "                \n",
    "            count_topic.append(count_sentence)\n",
    "            count_sentence=[]\n",
    "        \n",
    "        for index in tqdm(range(len(index_topic_t))):\n",
    "            sent_index = index_topic_t[index]\n",
    "            sent_s = count_topic[index]\n",
    "            if len(sent_s) !=0:\n",
    "                score_array =  np.array(sent_s)\n",
    "                pos_score = np.mean(score_array[:,0])## average count of positive adjective\n",
    "                pos_score = float('%.lf' % pos_score)\n",
    "                neg_score = np.mean(score_array[:, 1])## average count of negative adjective\n",
    "                neg_score = float('%.1f' % neg_score)\n",
    "                final_score = pos_score - neg_score\n",
    " \n",
    "                data_sentence_dic_sent['sentiment_score_topic'][sent_index] = final_score\n",
    "\n",
    "            else:\n",
    "                ## if tokenized_sentence is empty list\n",
    "                data_sentence_dic_sent['sentiment_score_topic'][sent_index] = 0\n",
    "    \n",
    "    return data_sentence_dic_sent  ## return to data_sentence_dic with sentence sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=get_sentence_score(data_sentence_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Evaluate business use sentences scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_summary(business_id,):\n",
    "    business_score = []\n",
    "    business_summary = [] ## index:  (0,1,2 topic1 total pos and neg，345，topic2 total pos and neg ………………)\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    for t in range(8):\n",
    "        sentence_score= [data_sentence_dic[\"sentiment_score_topic\"][k] for k in range(len(data_sentence_dic[\"review_number\"])) if data_sentence_dic[\"review_number\"][k] in review_number_id and data_sentence_dic['dominant_topic'][k]==t]\n",
    "        topic_pos_num = 0\n",
    "        topic_neg_num = 0\n",
    "        topic_neu_num = 0\n",
    "\n",
    "        if len(sentence_score)!=0:\n",
    "            for s in sentence_score:\n",
    "                if s>0:\n",
    "                    topic_pos_num+=1\n",
    "                elif s < 0:\n",
    "                    topic_neg_num+=1\n",
    "                else:\n",
    "                    topic_neu_num+=1\n",
    "            business_score.append(sum(sentence_score)/len(sentence_score))\n",
    "\n",
    "        else:\n",
    "            business_score.append(0)\n",
    "            \n",
    "            \n",
    "        business_summary.append(topic_pos_num+topic_neg_num+topic_neu_num)\n",
    "        business_summary.append(topic_pos_num)\n",
    "        business_summary.append(topic_neg_num)\n",
    "        business_summary.append(topic_neu_num)\n",
    "\n",
    "    #summary = (\n",
    "    #    'For your gym, there are '+str(len(review_number_id))+' reviews.\\n Among these reviews,'+'Topic_facilities has been mentioned '+str(business_summary[0])+' times,'+str(business_summary[1])+' are positive,'+str(business_summary[2])+' are negative,'+str(business_summary[3])+' are neutral.\\n'\n",
    "    #    +'Topic_Courses has been mentioned '+str(business_summary[4])+' times,'+str(business_summary[5])+' are positive,'+str(business_summary[6])+' are negative,'+str(business_summary[7])+' are neutral.\\n  '\n",
    "    #    +'Topic_Service and Accessories has been mentioned '+str(business_summary[8])+' times,'+str(business_summary[9])+' are positive,'+str(business_summary[10])+' are negative,'+str(business_summary[11])+' are neutral.\\n'\n",
    "    #    +'Topic_Membership and Price has been mentioned '+str(business_summary[12])+' times,'+str(business_summary[13])+' are positive,'+str(business_summary[14])+' are negative,'+str(business_summary[15])+' are neutral.\\n'\n",
    "    #    +'Topic_Trainers has been mentioned '+str(business_summary[16])+' times,'+str(business_summary[17])+' are positive,'+str(business_summary[18])+' are negative,'+str(business_summary[19])+' are neutral.\\n'\n",
    "    #    +'Topic_Time has been mentioned '+str(business_summary[20])+' times,'+str(business_summary[21])+' are positive,'+str(business_summary[22])+' are negative,'+str(business_summary[23])+' are neutral.\\n'\n",
    "    #    +'Topic_Environment has been mentioned '+str(business_summary[24])+' times,'+str(business_summary[25])+' are positive,'+str(business_summary[26])+' are negative,'+str(business_summary[27])+' are neutral.\\n'\n",
    "    #    +'Other topics have been mentioned '+str(business_summary[28])+' times,'+str(business_summary[29])+' are positive,'+str(business_summary[30])+' are negative,'+str(business_summary[31])+' are neutral.\\n'\n",
    "    #    +'Your gym\\'s topic scores on seven topics are '+str(business_score)\n",
    "    #        )\n",
    "    \n",
    "    return business_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate eight topic scores for each business\n",
    "\n",
    "def get_business_score(business_id):\n",
    "    business_score = []\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    for t in range(8):\n",
    "        sentence_score= [data_sentence_dic[\"sentiment_score_topic\"][k] for k in range(len(data_sentence_dic[\"review_number\"])) if data_sentence_dic[\"review_number\"][k] in review_number_id and data_sentence_dic['dominant_topic'][k]==t]\n",
    "        topic_pos_num = 0\n",
    "        topic_neg_num = 0\n",
    "        topic_neu_num = 0\n",
    "\n",
    "        if len(sentence_score)!=0:\n",
    "            for s in sentence_score:\n",
    "                if s>0:\n",
    "                    topic_pos_num+=1\n",
    "                elif s < 0:\n",
    "                    topic_neg_num+=1\n",
    "                else:\n",
    "                    topic_neu_num+=1\n",
    "            business_score.append(sum(sentence_score)/len(sentence_score))\n",
    "\n",
    "        else:\n",
    "            business_score.append(0)\n",
    "\n",
    "    return business_score\n",
    "\n",
    "\n",
    "## create weighted_stars \n",
    "\n",
    "def get_review_weight(rev_num):\n",
    "    nouns_list_rev = [data_sentence_dic['nouns'][i] for i in range(len(data_sentence_dic['review_number'])) if data_sentence_dic['review_number'][i] == rev_num]\n",
    "    nouns_list_rev = [item for sublist in nouns_list_rev for item in sublist]\n",
    "    topic_terms_en_rev = [item for sublist in topic_terms_en for item in sublist]\n",
    "    count_rev = 0\n",
    "    for word in nouns_list_rev:\n",
    "        if word in topic_terms_en_rev:\n",
    "            count_rev+=1\n",
    "    return count_rev\n",
    "\n",
    "def get_business_weighted_stars(business_id):\n",
    "    review_number_id = list(data_review.loc[data_review['business_id']==business_id][\"review_number\"])\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in review_number_id:\n",
    "        weight = get_review_weight(i)\n",
    "        star = data_review.loc[data_review['business_id']==business_id]['stars'][i]\n",
    "        numerator+=weight*star\n",
    "        denominator+=weight\n",
    "        \n",
    "    return numerator/denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_scores={}\n",
    "for d in tqdm(idset):\n",
    "    scores = get_business_score(d)\n",
    "    summary = get_business_summary(d)\n",
    "    business_scores.setdefault('business_id',[]).append(d)\n",
    "    business_scores.setdefault('topic_1_Facilities',[]).append(scores[0])\n",
    "    business_scores.setdefault('topic_2_Courses',[]).append(scores[1])\n",
    "    business_scores.setdefault('topic_3_Service_and_Accessories',[]).append(scores[2])\n",
    "    business_scores.setdefault('topic_4_Membership_and_Price',[]).append(scores[3])\n",
    "    business_scores.setdefault('topic_5_Trainer',[]).append(scores[4])\n",
    "    business_scores.setdefault('topic_6_Time',[]).append(scores[5])\n",
    "    business_scores.setdefault('topic_7_Environment',[]).append(scores[6])\n",
    "    business_scores.setdefault('topic_8_Other',[]).append(scores[7])\n",
    "    business_scores.setdefault('weighted_stars',[]).append(get_business_weighted_stars(d))\n",
    "    for i in range(32):\n",
    "        business_scores.setdefault('summary_'+str(i),[]).append(summary[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create business_scores.csv\n",
    "pd.DataFrame(business_scores).to_csv(path_or_buf='business_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
